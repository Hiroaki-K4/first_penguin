http://arxiv.org/abs/2007.12173v1,Bridging the Imitation Gap by Adaptive Insubordination,http://arxiv.org/pdf/2007.12173v1,"Why do agents often obtain better reinforcement learning policies whenimitating a worse expert? We show that privileged information used by theexpert is marginalized in the learned agent policy, resulting in an ""imitationgap."" Prior work bridges this gap via a progression from imitation learning toreinforcement learning. While often successful, gradual progression fails fortasks that require frequent switches between exploration and memorizationskills. To better address these tasks and alleviate the imitation gap wepropose 'Adaptive Insubordination' (ADVISOR), which dynamically reweightsimitation and reward-based reinforcement learning losses during training,enabling switching between imitation and exploration. On a suite of challengingtasks, we show that ADVISOR outperforms pure imitation, pure reinforcementlearning, as well as sequential combinations of these approaches.",より悪い専門家を模倣するとき、なぜエージェントはしばしばより良い強化学習方針を得るのですか？専門家が使用する特権情報は、学習されたエージェントポリシーでは無視され、「模倣ギャップ」が生じることを示しています。以前の研究は、模倣学習から強化学習への進歩を通じてこのギャップを埋めています。多くの場合成功しますが、探索スキルと記憶スキルを頻繁に切り替える必要があるタスクでは、段階的な進行は失敗します。これらのタスクにより適切に対処し、模倣のギャップを緩和するために、トレーニング中に動的に再重み付けシミュレーションと報酬ベースの強化学習損失を模倣する「適応型不従順」（ADVISOR）を提案し、模倣と探索の切り替えを可能にします。一連のやりがいのあるタスクで、ADVISORが純粋な模倣、純粋な強化学習、およびこれらのアプローチの逐次的な組み合わせよりも優れていることを示します。
http://arxiv.org/abs/2007.12163v1,Smooth-AP: Smoothing the Path Towards Large-Scale Image Retrieval,http://arxiv.org/pdf/2007.12163v1,"Optimising a ranking-based metric, such as Average Precision (AP), isnotoriously challenging due to the fact that it is non-differentiable, andhence cannot be optimised directly using gradient-descent methods. To this end,we introduce an objective that optimises instead a smoothed approximation ofAP, coined Smooth-AP. Smooth-AP is a plug-and-play objective function thatallows for end-to-end training of deep networks with a simple and elegantimplementation. We also present an analysis for why directly optimising theranking based metric of AP offers benefits over other deep metric learninglosses. We apply Smooth-AP to standard retrieval benchmarks: Stanford Onlineproducts and VehicleID, and also evaluate on larger-scale datasets: INaturalistfor fine-grained category retrieval, and VGGFace2 and IJB-C for face retrieval.In all cases, we improve the performance over the state-of-the-art, especiallyfor larger-scale datasets, thus demonstrating the effectiveness and scalabilityof Smooth-AP to real-world scenarios.",平均精度（AP）などのランキングベースのメトリックの最適化は、微分不可能であり、勾配降下法を使用して直接最適化することができないため、非常に困難です。この目的のために、代わりに平滑化されたAPの近似であるSmooth-APを最適化する目的を紹介します。 Smooth-APはプラグアンドプレイの目的関数であり、シンプルでエレガントな実装でディープネットワークのエンドツーエンドのトレーニングを可能にします。また、APのランキングベースのメトリックを直接最適化すると、他のディープメトリックの学習損失よりもメリットがある理由の分析も示します。 Smooth-APを標準の検索ベンチマーク（Stanford OnlineproductsとVehicleID）に適用し、さらに大規模なデータセット（細かいカテゴリ検索にはINaturalist、顔検索にはVGGFace2とIJB-C）も評価します。特に大規模なデータセット向けの最先端技術により、Smooth-APの有効性とスケーラビリティを実際のシナリオに示します。
http://arxiv.org/abs/2007.12148v1,"Enhanced Transfer Learning for Autonomous Driving with Systematic
  Accident Simulation",http://arxiv.org/pdf/2007.12148v1,"Simulation data can be utilized to extend real-world driving data in order tocover edge cases, such as vehicle accidents. The importance of handling edgecases can be observed in the high societal costs in handling car accidents, aswell as potential dangers to human drivers. In order to cover a wide anddiverse range of all edge cases, we systemically parameterize and simulate themost common accident scenarios. By applying this data to autonomous drivingmodels, we show that transfer learning on simulated data sets provide bettergeneralization and collision avoidance, as compared to random initializationmethods. Our results illustrate that information from a model trained onsimulated data can be inferred to a model trained on real-world data,indicating the potential influence of simulation data in real world models andadvancements in handling of anomalous driving scenarios.",シミュレーションデータは、自動車事故などのエッジケースをカバーするために、実際の運転データを拡張するために利用できます。エッジケースを処理することの重要性は、自動車事故を処理するための社会的コストが高いこと、および人間のドライバーに対する潜在的な危険性において観察できます。すべてのエッジケースの幅広い多様な範囲をカバーするために、最も一般的な事故シナリオを体系的にパラメーター化してシミュレーションします。このデータを自動運転モデル​​に適用することにより、シミュレートされたデータセットの転移学習が、ランダムな初期化方法と比較して、より一般化され、衝突を回避できることを示しています。私たちの結果は、シミュレートされたデータでトレーニングされたモデルからの情報が、実世界のデータでトレーニングされたモデルに推測できることを示しており、実際のモデルのシミュレーションデータの潜在的な影響と異常な運転シナリオの処理の進歩を示しています。
http://arxiv.org/abs/2007.12147v1,"CurveLane-NAS: Unifying Lane-Sensitive Architecture Search and Adaptive
  Point Blending",http://arxiv.org/pdf/2007.12147v1,"We address the curve lane detection problem which poses more realisticchallenges than conventional lane detection for better facilitating modernassisted/autonomous driving systems. Current hand-designed lane detectionmethods are not robust enough to capture the curve lanes especially the remoteparts due to the lack of modeling both long-range contextual information anddetailed curve trajectory. In this paper, we propose a novel lane-sensitivearchitecture search framework named CurveLane-NAS to automatically capture bothlong-ranged coherent and accurate short-range curve information while unifyingboth architecture search and post-processing on curve lane predictions viapoint blending. It consists of three search modules: a) a feature fusion searchmodule to find a better fusion of the local and global context for multi-levelhierarchy features; b) an elastic backbone search module to explore anefficient feature extractor with good semantics and latency; c) an adaptivepoint blending module to search a multi-level post-processing refinementstrategy to combine multi-scale head prediction. The unified framework ensureslane-sensitive predictions by the mutual guidance between NAS and adaptivepoint blending. Furthermore, we also steer forward to release a morechallenging benchmark named CurveLanes for addressing the most difficult curvelanes. It consists of 150K images with 680K labels.The new dataset can bedownloaded at github.com/xbjxh/CurveLanes (already anonymized for thissubmission). Experiments on the new CurveLanes show that the SOTA lanedetection methods suffer substantial performance drop while our model can stillreach an 80+% F1-score. Extensive experiments on traditional lane benchmarkssuch as CULane also demonstrate the superiority of our CurveLane-NAS, e.g.achieving a new SOTA 74.8% F1-score on CULane.","従来の車線検出よりも現実的な課題を提起するカーブ車線検出問題に対処し、最新の支援/自動運転システムをより容易にします。現在手作業で設計されたレーン検出方法は、長距離のコンテキスト情報と詳細な曲線軌道の両方をモデル化していないため、曲線レーン、特にリモートパーツをキャプチャするには十分に堅牢ではありません。このホワイトペーパーでは、CurveLane-NASという名前の新しいレーンセンシティブアーキテクチャ検索フレームワークを提案し、アーキテクチャの検索とポイントブレンディング経由のカーブレーン予測の両方の処理を統合しながら、長距離のコヒーレントで正確な短距離カーブ情報を自動的にキャプチャしますこれは、3つの検索モジュールで構成されています。a）マルチレベル階層機能のローカルコンテキストとグローバルコンテキストのより良い融合を見つける機能融合検索モジュール。 b）弾性バックボーン検索モジュール。優れたセマンティクスとレイテンシを備えた効率的な特徴抽出器を探索します。 c）マルチレベルのヘッド予測を組み合わせるためにマルチレベルの後処理改良戦略を検索する適応ポイントブレンディングモジュール。統合されたフレームワークは、NASとアダプティブポイントブレンディング間の相互ガイダンスによって車線に敏感な予測を保証します。さらに、最も困難なカーブレーンに対処するために、CurveLanesというより挑戦的なベンチマークをリリースすることも進めています。 150,000枚の画像と680,000枚のラベルで構成されます。新しいデータセットはgithub.com/xbjxh/CurveLanesからダウンロードできます（この送信についてはすでに匿名化されています）。新しいCurveLanesでの実験により、SOTAレーン検出方法ではパフォーマンスが大幅に低下する一方で、モデルでは80％以上のF1スコアに到達できることが示されています。 CULaneなどの従来のレーンベンチマークでの広範な実験も、Curaneでの新しいSOTA 74.8％F1スコアの達成など、CurveLane-NASの優位性を実証しています。"
http://arxiv.org/abs/2007.12146v1,Spatially Aware Multimodal Transformers for TextVQA,http://arxiv.org/pdf/2007.12146v1,"Textual cues are essential for everyday tasks like buying groceries and usingpublic transport. To develop this assistive technology, we study the TextVQAtask, i.e., reasoning about text in images to answer a question. Existingapproaches are limited in their use of spatial relations and rely onfully-connected transformer-like architectures to implicitly learn the spatialstructure of a scene. In contrast, we propose a novel spatially awareself-attention layer such that each visual entity only looks at neighboringentities defined by a spatial graph. Further, each head in our multi-headself-attention layer focuses on a different subset of relations. Our approachhas two advantages: (1) each head considers local context instead of dispersingthe attention amongst all visual entities; (2) we avoid learning redundantfeatures. We show that our model improves the absolute accuracy of currentstate-of-the-art methods on TextVQA by 2.2% overall over an improved baseline,and 4.62% on questions that involve spatial reasoning and can be answeredcorrectly using OCR tokens. Similarly on ST-VQA, we improve the absoluteaccuracy by 4.2%. We further show that spatially aware self-attention improvesvisual grounding.",食料品の購入や公共交通機関の使用などの日常的な作業には、テキストによる手がかりが不可欠です。この支援技術を開発するために、私たちはTextVQAタスクを研究します。つまり、質問に答えるために画像内のテキストについて推論します。既存のアプローチは、空間関係の使用に制限があり、シーンの空間構造を暗黙的に学習するために、完全に接続されたトランスフォーマーのようなアーキテクチャに依存しています。対照的に、私たちは、各視覚エンティティが空間グラフによって定義された隣接エンティティのみを見るように、新しい空間認識自己注意レイヤーを提案します。さらに、マルチヘッドセルフアテンションレイヤーの各ヘッドは、関係の異なるサブセットに焦点を合わせています。私たちのアプローチには2つの利点があります。（1）各頭は、すべての視覚エンティティに注意を分散させるのではなく、ローカルコンテキストを考慮します。 （2）冗長な機能の学習を避けます。私たちのモデルは、TextVQAの最新のメソッドの絶対精度を、改善されたベースラインよりも全体で2.2％、空間的推論を含み、OCRトークンを使用して正しく回答できる質問では4.62％改善することを示しています。同様に、ST-VQAでは、絶対精度が4.2％向上します。さらに、空間認識の自己注意が視覚的接地を改善することを示します。
http://arxiv.org/abs/2007.12142v1,"PIPAL: a Large-Scale Image Quality Assessment Dataset for Perceptual
  Image Restoration",http://arxiv.org/pdf/2007.12142v1,"Image quality assessment (IQA) is the key factor for the fast development ofimage restoration (IR) algorithms. The most recent IR methods based onGenerative Adversarial Networks (GANs) have achieved significant improvement invisual performance, but also presented great challenges for quantitativeevaluation. Notably, we observe an increasing inconsistency between perceptualquality and the evaluation results. Then we raise two questions: (1) Canexisting IQA methods objectively evaluate recent IR algorithms? (2) When focuson beating current benchmarks, are we getting better IR algorithms? To answerthese questions and promote the development of IQA methods, we contribute alarge-scale IQA dataset, called Perceptual Image Processing Algorithms (PIPAL)dataset. Especially, this dataset includes the results of GAN-based methods,which are missing in previous datasets. We collect more than 1.13 million humanjudgments to assign subjective scores for PIPAL images using the more reliable""Elo system"". Based on PIPAL, we present new benchmarks for both IQA andsuper-resolution methods. Our results indicate that existing IQA methods cannotfairly evaluate GAN-based IR algorithms. While using appropriate evaluationmethods is important, IQA methods should also be updated along with thedevelopment of IR algorithms. At last, we improve the performance of IQAnetworks on GAN-based distortions by introducing anti-aliasing pooling.Experiments show the effectiveness of the proposed method.",画像品質評価（IQA）は、画像復元（IR）アルゴリズムを迅速に開発するための重要な要素です。 Generative Adversarial Networks（GAN）に基づく最新のIRメソッドは、視覚的パフォーマンスの大幅な改善を達成しましたが、定量的評価にも大きな課題を提示しました。特に、知覚の質と評価結果の間に一貫性の欠如が見られます。次に、2つの質問を提起します。（1）既存のIQAメソッドは、最近のIRアルゴリズムを客観的に評価しますか？ （2）現在のベンチマークを上回ることに焦点を当てると、より良いIRアルゴリズムが得られますか？これらの質問に答えてIQAメソッドの開発を促進するために、知覚画像処理アルゴリズム（PIPAL）データセットと呼ばれる大規模なIQAデータセットを提供します。特に、このデータセットには、以前のデータセットにはないGANベースのメソッドの結果が含まれています。より信頼性の高い「Eloシステム」を使用してPIPAL画像に主観的なスコアを割り当てるために、113万人を超える人の判断を収集します。 PIPALに基づいて、IQAおよび超解像法の両方の新しいベンチマークを提示します。私たちの結果は、既存のIQAメソッドはGANベースのIRアルゴリズムを公平に評価できないことを示しています。適切な評価方法を使用することが重要ですが、IQA方法もIRアルゴリズムの開発とともに更新する必要があります。最後に、アンチエイリアシングプーリングを導入することにより、GANベースの歪みでのIQAnetworksのパフォーマンスを向上させます。実験では、提案された方法の有効性を示しています。
http://arxiv.org/abs/2007.12140v1,"HITNet: Hierarchical Iterative Tile Refinement Network for Real-time
  Stereo Matching",http://arxiv.org/pdf/2007.12140v1,"This paper presents HITNet, a novel neural network architecture for real-timestereo matching. Contrary to many recent neural network approaches that operateon a full cost volume and rely on 3D convolutions, our approach does notexplicitly build a volume and instead relies on a fast multi-resolutioninitialization step, differentiable 2D geometric propagation and warpingmechanisms to infer disparity hypotheses. To achieve a high level of accuracy,our network not only geometrically reasons about disparities but also infersslanted plane hypotheses allowing to more accurately perform geometric warpingand upsampling operations. Our architecture is inherently multi-resolutionallowing the propagation of information at different levels. Multipleexperiments prove the effectiveness of the proposed approach at a fraction ofthe computation required by recent state-of-the-art methods. At time ofwriting, HITNet ranks 1st-3rd on all the metrics published on the ETH3D websitefor two view stereo and ranks 1st on the popular KITTI 2012 and 2015 benchmarksamong the published methods faster than 100ms.",このペーパーでは、リアルタイムステレオマッチング用の新しいニューラルネットワークアーキテクチャであるHITNetについて説明します。フルコストのボリュームで動作し、3D畳み込みに依存する最近の多くのニューラルネットワークアプローチとは対照的に、私たちのアプローチは、ボリュームを明示的に構築せず、代わりに、高速マルチ解像度初期化ステップ、微分可能な2D幾何学的伝播およびワーピングメカニズムに依存して、視差仮説を推定します。高レベルの精度を達成するために、私たちのネットワークは、視差に関する幾何学的な理由だけでなく、幾何学的ワーピングおよびアップサンプリング操作をより正確に実行することを可能にする推測された平面仮説も推論します。私たちのアーキテクチャは本質的にマルチ解像度であり、さまざまなレベルでの情報の伝播を可能にします。複数の実験により、最近の最先端の方法で必要とされる計算のほんの一部で、提案されたアプローチの有効性が証明されています。執筆時点では、HITNetは2ビューステレオのETH3D Webサイトで公開されているすべてのメトリックで1位から3位にランクされ、公開されているメソッドの中で人気の高いKITTI 2012および2015ベンチマークで100ミリ秒よりも速くランク付けされています
http://arxiv.org/abs/2007.12131v1,"BSL-1K: Scaling up co-articulated sign language recognition using
  mouthing cues",http://arxiv.org/pdf/2007.12131v1,"Recent progress in fine-grained gesture and action classification, andmachine translation, point to the possibility of automated sign languagerecognition becoming a reality. A key stumbling block in making progresstowards this goal is a lack of appropriate training data, stemming from thehigh complexity of sign annotation and a limited supply of qualifiedannotators. In this work, we introduce a new scalable approach to datacollection for sign recognition in continuous videos. We make use ofweakly-aligned subtitles for broadcast footage together with a keyword spottingmethod to automatically localise sign-instances for a vocabulary of 1,000 signsin 1,000 hours of video. We make the following contributions: (1) We show howto use mouthing cues from signers to obtain high-quality annotations from videodata - the result is the BSL-1K dataset, a collection of British Sign Language(BSL) signs of unprecedented scale; (2) We show that we can use BSL-1K to trainstrong sign recognition models for co-articulated signs in BSL and that thesemodels additionally form excellent pretraining for other sign languages andbenchmarks - we exceed the state of the art on both the MSASL and WLASLbenchmarks. Finally, (3) we propose new large-scale evaluation sets for thetasks of sign recognition and sign spotting and provide baselines which we hopewill serve to stimulate research in this area.","きめの細かいジェスチャーとアクションの分類、および機械翻訳の最近の進歩は、自動化された手話認識が現実になる可能性を指摘しています。この目標に向けて前進する上での主な障害は、適切なトレーニングデータが不足していることです。これは、標識注釈が非常に複雑であり、資格のある注釈者が限られているためです。この作業では、連続ビデオでの標識認識のためのデータ収集への新しいスケーラブルなアプローチを紹介します。私たちは、放送映像用の弱く配置された字幕とキーワードスポッティングメソッドを使用して、1,000時間のビデオで1,000サインの語彙のサインインスタンスを自動的にローカライズします。私たちは以下の貢献をします：（1）ビデオデータから高品質の注釈を取得するために、署名者からの口伝えの手掛かりを使用する方法を示します-結果は、前例のないスケールのイギリス手話（BSL）標識のコレクションであるBSL-1Kデータセットです。 （2）私たちは、BSL-1Kを使用してBSLで連結された標識の強力な標識認識モデルをトレーニングできること、およびこれらのモデルがさらに他の手話言語およびベンチマークの優れた事前トレーニングを形成することを示します-MSASLおよびWLASLベンチマークの両方で最先端の技術を超えます。最後に、（3）標識認識と標識スポッティングのタスクに新しい大規模評価セットを提案し、この領域の研究を刺激するために役立つと期待されるベースラインを提供します。"
http://arxiv.org/abs/2007.12130v1,Sound2Sight: Generating Visual Dynamics from Sound and Context,http://arxiv.org/pdf/2007.12130v1,"Learning associations across modalities is critical for robust multimodalreasoning, especially when a modality may be missing during inference. In thispaper, we study this problem in the context of audio-conditioned visualsynthesis -- a task that is important, for example, in occlusion reasoning.Specifically, our goal is to generate future video frames and their motiondynamics conditioned on audio and a few past frames. To tackle this problem, wepresent Sound2Sight, a deep variational framework, that is trained to learn aper frame stochastic prior conditioned on a joint embedding of audio and pastframes. This embedding is learned via a multi-head attention-based audio-visualtransformer encoder. The learned prior is then sampled to further condition avideo forecasting module to generate future frames. The stochastic prior allowsthe model to sample multiple plausible futures that are consistent with theprovided audio and the past context. Moreover, to improve the quality andcoherence of the generated frames, we propose a multimodal discriminator thatdifferentiates between a synthesized and a real audio-visual clip. Weempirically evaluate our approach, vis-\'a-vis closely-related prior methods,on two new datasets viz. (i) Multimodal Stochastic Moving MNIST with a SurpriseObstacle, (ii) Youtube Paintings; as well as on the existing Audio-Set Drumsdataset. Our extensive experiments demonstrate that Sound2Sight significantlyoutperforms the state of the art in the generated video quality, while alsoproducing diverse video content.",モダリティ全体で関連付けを学習することは、特に推論中にモダリティが欠落している可能性がある場合に、堅牢なマルチモーダル推論にとって重要です。このペーパーでは、オーディオ条件付き視覚合成のコンテキストでこの問題を研究します-たとえば、閉塞推論で重要なタスクです。具体的には、私たちの目標は、将来のビデオフレームとオーディオおよびいくつかの過去の条件付きのモーションダイナミクスを生成することですフレーム。この問題に取り組むために、深い変分フレームワークであるSound2Sightを紹介します。これは、オーディオと過去のフレームの同時埋め込みを条件として、フレームごとの確率的事前確率を学習するようにトレーニングされています。この埋め込みは、マルチヘッド注意ベースのオーディオビジュアルトランスフォーマーエンコーダーによって学習されます。次に、学習した事前分布をサンプリングして、ビデオ予測モジュールをさらに調整し、将来のフレームを生成します。確率的事前分布により、モデルは、提供されたオーディオおよび過去のコンテキストと一致する複数のもっともらしい未来をサンプリングできます。さらに、生成されたフレームの品質とコヒーレンスを改善するために、合成クリップと実際のオーディオビジュアルクリップを区別するマルチモーダル弁別器を提案します。 2つの新しいデータセット、つまり密接に関連する以前の方法と比較して、経験的に私たちのアプローチを評価します。 （i）SurpriseObstacleを使用したマルチモーダル確率移動MNIST、（ii）Youtube絵画。既存のAudio-Set Drumsdatasetも同様です。当社の広範な実験により、Sound2Sightは、生成されたビデオ品質において最先端の技術を大幅に上回るだけでなく、多様なビデオコンテンツを生成することを実証しています。
http://arxiv.org/abs/2007.12107v1,"Few-Shot Object Detection and Viewpoint Estimation for Objects in the
  Wild",http://arxiv.org/pdf/2007.12107v1,"Detecting objects and estimating their viewpoint in images are key tasks of3D scene understanding. Recent approaches have achieved excellent results onvery large benchmarks for object detection and viewpoint estimation. However,performances are still lagging behind for novel object categories with fewsamples. In this paper, we tackle the problems of few-shot object detection andfew-shot viewpoint estimation. We propose a meta-learning framework that can beapplied to both tasks, possibly including 3D data. Our models improve theresults on objects of novel classes by leveraging on rich feature informationoriginating from base classes with many samples. A simple joint featureembedding module is proposed to make the most of this feature sharing. Despiteits simplicity, our method outperforms state-of-the-art methods by a largemargin on a range of datasets, including PASCAL VOC and MS COCO for few-shotobject detection, and Pascal3D+ and ObjectNet3D for few-shot viewpointestimation. And for the first time, we tackle the combination of both few-shottasks, on Object- Net3D, showing promising results. Our code and data areavailable at http://imagine.enpc.fr/~xiaoy/FSDetView/.",画像内のオブジェクトを検出して視点を推定することは、3Dシーンを理解するための重要なタスクです。最近のアプローチは、オブジェクト検出と視点推定の非常に大きなベンチマークで優れた結果を達成しています。ただし、サンプル数が少ない新規オブジェクトカテゴリのパフォーマンスはまだ遅れています。この論文では、少数ショットの物体検出と少数ショットの視点推定の問題に取り組みます。 3Dデータを含め、両方のタスクに適用できるメタ学習フレームワークを提案します。私たちのモデルは、多くのサンプルを持つ基本クラスから発生する豊富な機能情報を活用することにより、新規クラスのオブジェクトの結果を改善します。この機能の共有を最大限に活用するために、単純なジョイント機能埋め込みモジュールが提案されています。シンプルさにもかかわらず、このメソッドは、少数ショットオブジェクト検出用のPASCAL VOCとMS COCO、少数ショットオブジェクトの視点推定用のPascal3D +とObjectNet3Dを含む、幅広いデータセットで最新のメソッドよりも優れています。そして初めて、Object-Net3Dで両方の少数のショットタスクの組み合わせに取り組み、有望な結果を示しました。コードとデータは、http：//imagine.enpc.fr/~xiaoy/FSDetView/で入手できます。
http://arxiv.org/abs/2007.12104v1,"Leveraging Bottom-Up and Top-Down Attention for Few-Shot Object
  Detection",http://arxiv.org/pdf/2007.12104v1,"Few-shot object detection aims at detecting objects with few annotatedexamples, which remains a challenging research problem yet to be explored.Recent studies have shown the effectiveness of self-learned top-down attentionmechanisms in object detection and other vision tasks. The top-down attention,however, is less effective at improving the performance of few-shot detectors.Due to the insufficient training data, object detectors cannot effectivelygenerate attention maps for few-shot examples. To improve the performance andinterpretability of few-shot object detectors, we propose an attentive few-shotobject detection network (AttFDNet) that takes the advantages of both top-downand bottom-up attention. Being task-agnostic, the bottom-up attention serves asa prior that helps detect and localize naturally salient objects. We furtheraddress specific challenges in few-shot object detection by introducing twonovel loss terms and a hybrid few-shot learning strategy. Experimental resultsand visualization demonstrate the complementary nature of the two types ofattention and their roles in few-shot object detection. Codes are available athttps://github.com/chenxy99/AttFDNet.",少数ショットのオブジェクト検出は、注釈が付けられた例がほとんどないオブジェクトを検出することを目的としています。ただし、トップダウンアテンションは、少数ショット検出器のパフォーマンスの向上にはあまり効果がありません。トレーニングデータが不十分なため、オブジェクト検出器は少数ショットの例の効果マップを効果的に生成できません。少数ショットオブジェクト検出器のパフォーマンスと解釈可能性を向上させるために、トップダウンとボトムアップの両方の利点を活用する注意深い少数ショットオブジェクト検出ネットワーク（AttFDNet）を提案します。タスクにとらわれないボトムアップアテンションは、自然に顕著なオブジェクトを検出してローカライズするのに役立つ先行機能として機能します。さらに、2つの新しい損失項とハイブリッド数ショット学習戦略を導入することにより、数ショットオブジェクト検出における特定の課題に取り組みます。実験結果と視覚化は、2つのタイプの注意の補完的な性質と、少数ショットオブジェクト検出におけるそれらの役割を示しています。コードはhttps://github.com/chenxy99/AttFDNetで入手できます。
http://arxiv.org/abs/2007.12099v1,PP-YOLO: An Effective and Efficient Implementation of Object Detector,http://arxiv.org/pdf/2007.12099v1,"Object detection is one of the most important areas in computer vision, whichplays a key role in various practical scenarios. Due to limitation of hardware,it is often necessary to sacrifice accuracy to ensure the infer speed of thedetector in practice. Therefore, the balance between effectiveness andefficiency of object detector must be considered. The goal of this paper is toimplement an object detector with relatively balanced effectiveness andefficiency that can be directly applied in actual application scenarios, ratherthan propose a novel detection model. Considering that YOLOv3 has been widelyused in practice, we develop a new object detector based on YOLOv3. We mainlytry to combine various existing tricks that almost not increase the number ofmodel parameters and FLOPs, to achieve the goal of improving the accuracy ofdetector as much as possible while ensuring that the speed is almost unchanged.Since all experiments in this paper are conducted based on PaddlePaddle, wecall it PP-YOLO. By combining multiple tricks, PP-YOLO can achieve a betterbalance between effectiveness (45.% mAP) and efficiency (72.9 FPS), surpassingthe existing state-of-the-art detectors such as EfficientDet and YOLOv4.Sourcecode is at https://github.com/PaddlePaddle/PaddleDetection.",物体検出は、コンピュータビジョンの最も重要な領域の1つであり、さまざまな実際的なシナリオで重要な役割を果たします。ハードウェアの制限により、実際には検出器の推定速度を保証するために精度を犠牲にする必要があることがよくあります。したがって、オブジェクト検出器の有効性と効率のバランスを考慮する必要があります。このホワイトペーパーの目的は、新しい検出モデルを提案するのではなく、実際のアプリケーションシナリオに直接適用できる比較的バランスのとれた有効性と効率を備えたオブジェクト検出器を実装することです。 YOLOv3が実際に広く使用されていることを考慮して、YOLOv3に基づく新しいオブジェクト検出器を開発します。主に、モデルパラメーターとFLOPの数をほとんど増加させないさまざまな既存のトリックを組み合わせて、速度がほとんど変わらないようにしながら、検出器の精度を可能な限り向上させるという目標を達成します。この論文のすべての実験は、パドルパドル、PP-YOLOと呼びます。複数のトリックを組み合わせることにより、PP-YOLOは、有効性（45.％mAP）と効率（72.9 FPS）のバランスを改善し、EfficientDetやYOLOv4などの既存の最先端の検出器を上回ることができます。ソースコードはhttps：//にあります。 github.com/PaddlePaddle/PaddleDetection。
http://arxiv.org/abs/2007.12088v1,"Pixel-Pair Occlusion Relationship Map(P2ORM): Formulation, Inference &
  Application",http://arxiv.org/pdf/2007.12088v1,"We formalize concepts around geometric occlusion in 2D images (i.e., ignoringsemantics), and propose a novel unified formulation of both occlusionboundaries and occlusion orientations via a pixel-pair occlusion relation. Theformer provides a way to generate large-scale accurate occlusion datasetswhile, based on the latter, we propose a novel method for task-independentpixel-level occlusion relationship estimation from single images. Experimentson a variety of datasets demonstrate that our method outperforms existing oneson this task. To further illustrate the value of our formulation, we alsopropose a new depth map refinement method that consistently improve theperformance of state-of-the-art monocular depth estimation methods. Our codeand data are available at http://imagine.enpc.fr/~qiux/P2ORM/.",2D画像の幾何学的オクルージョンに関する概念（つまり、意味論を無視する）を形式化し、ピクセルペアオクルージョン関係を介してオクルージョン境界とオクルージョン方向の両方の新しい統一された定式化を提案します。フォーマーは大規模で正確なオクルージョンデータセットを生成する方法を提供しますが、後者に基づいて、単一の画像からタスクに依存しないピクセルレベルのオクルージョン関係を推定する新しい方法を提案します。さまざまなデータセットのExperimentsonは、このタスクが既存のonesonをこのタスクよりも優れていることを示しています。私たちの定式化の価値をさらに説明するために、最新の単眼深度推定法のパフォーマンスを一貫して改善する新しい深度マップ改良法も提案します。コードとデータは、http：//imagine.enpc.fr/~qiux/P2ORM/で入手できます。
http://arxiv.org/abs/2007.12082v1,"A Study on Evaluation Standard for Automatic Crack Detection Regard the
  Random Fractal",http://arxiv.org/pdf/2007.12082v1,"A reasonable evaluation standard underlies construction of effective deeplearning models. However, we find in experiments that the automatic crackdetectors based on deep learning are obviously underestimated by the widelyused mean Average Precision (mAP) standard. This paper presents a study on theevaluation standard. It is clarified that the random fractal of crack disablesthe mAP standard, because the strict box matching in mAP calculation isunreasonable for the fractal feature. As a solution, a fractal-availableevaluation standard named CovEval is proposed to correct the underestimation incrack detection. In CovEval, a different matching process based on the idea ofcovering box matching is adopted for this issue. In detail, Cover Area rate(CAr) is designed as a covering overlap, and a multi-match strategy is employedto release the one-to-one matching restriction in mAP. Extended Recall (XR),Extended Precision (XP) and Extended F-score (Fext) are defined for scoring thecrack detectors. In experiments using several common frameworks for objectdetection, models get much higher scores in crack detection according toCovEval, which matches better with the visual performance. Moreover, based onfaster R-CNN framework, we present a case study to optimize a crack detectorbased on CovEval standard. Recall (XR) of our best model achieves anindustrial-level at 95.8, which implies that with reasonable standard forevaluation, the methods for object detection are with great potential forautomatic industrial inspection.",合理的な評価基準は、効果的なディープラーニングモデルの構築の基礎となります。ただし、ディープラーニングに基づく自動亀裂検出器は、広く使用されている平均平均精度（mAP）標準では明らかに過小評価されていることが実験でわかりました。本稿では、評価基準に関する研究を紹介する。 mAP計算での厳密なボックスマッチングはフラクタル機能に対して不合理であるため、亀裂のランダムフラクタルはmAP標準を無効にすることが明らかになりました。解決策として、CovEvalというフラクタル利用可能な評価標準が、過小評価の亀裂検出を修正するために提案されています。 CovEvalでは、ボックスマッチングをカバーするという考えに基づく別のマッチングプロセスがこの問題に採用されています。詳細には、カバーエリアレート（CAr）はカバーするオーバーラップとして設計されており、マルチマッチ戦略を使用して、mAPの1対1のマッチング制限を解除します。拡張リコール（XR）、拡張精度（XP）、および拡張Fスコア（Fext）は、亀裂検出器のスコアリング用に定義されています。オブジェクト検出にいくつかの一般的なフレームワークを使用した実験では、モデルは、CovEvalに従ってクラック検出ではるかに高いスコアを取得します。さらに、より高速なR-CNNフレームワークに基づいて、CovEval標準に基づく亀裂検出器を最適化するケーススタディを紹介します。最高のモデルのリコール（XR）は、95.8の産業レベルを達成します。これは、妥当な標準評価により、オブジェクト検出の方法が自動産業検査の大きな可能性を秘めていることを意味します。
http://arxiv.org/abs/2007.12075v1,Representation Sharing for Fast Object Detector Search and Beyond,http://arxiv.org/pdf/2007.12075v1,"Region Proposal Network (RPN) provides strong support for handling the scalevariation of objects in two-stage object detection. For one-stage detectorswhich do not have RPN, it is more demanding to have powerful sub-networkscapable of directly capturing objects of unknown sizes. To enhance suchcapability, we propose an extremely efficient neural architecture searchmethod, named Fast And Diverse (FAD), to better explore the optimalconfiguration of receptive fields and convolution types in the sub-networks forone-stage detectors. FAD consists of a designed search space and an efficientarchitecture search algorithm. The search space contains a rich set of diversetransformations designed specifically for object detection. To cope with thedesigned search space, a novel search algorithm termed Representation Sharing(RepShare) is proposed to effectively identify the best combinations of thedefined transformations. In our experiments, FAD obtains prominent improvementson two types of one-stage detectors with various backbones. In particular, ourFAD detector achieves 46.4 AP on MS-COCO (under single-scale testing),outperforming the state-of-the-art detectors, including the most recentNAS-based detectors, Auto-FPN (searched for 16 GPU-days) and NAS-FCOS (28GPU-days), while significantly reduces the search cost to 0.6 GPU-days. Beyondobject detection, we further demonstrate the generality of FAD on the morechallenging instance segmentation, and expect it to benefit more tasks.",Region Proposal Network（RPN）は、2段階のオブジェクト検出でオブジェクトのスケール変動を処理するための強力なサポートを提供します。 RPNを持たない1ステージ検出器の場合、未知のサイズのオブジェクトを直接キャプチャできる強力なサブネットワークが必要になります。そのような機能を強化するために、Fast And Diverse（FAD）という名前の非常に効率的なニューラルアーキテクチャの検索方法を提案し、サブネットワーク内の1段検出器の受容フィールドとたたみ込みタイプの最適な構成をより詳しく調査します。 FADは、設計された検索スペースと効率的なアーキテクチャー検索アルゴリズムで構成されています。検索スペースには、特にオブジェクト検出のために設計された多様な変換の豊富なセットが含まれています。設計された検索空間に対処するために、表現共有（RepShare）と呼ばれる新しい検索アルゴリズムが提案され、定義された変換の最良の組み合わせを効果的に識別します。私たちの実験では、FADは、さまざまなバックボーンを備えた2種類の1ステージ検出器で顕著な改善を実現しています。特に、当社のFAD検出器は、MS-COCOで46.4 APを達成し（単一スケールのテスト下）、最新のNASベースの検出器であるAuto-FPN（16 GPU日で検索）を含む最新の検出器よりも優れていますNAS-FCOS（28GPU日）、検索コストを0.6 GPU日に大幅に削減します。オブジェクト検出以外にも、より困難なインスタンスセグメンテーションにおけるFADの一般性をさらに実証し、より多くのタスクに利益をもたらすことが期待されます。
http://arxiv.org/abs/2007.12072v1,TSIT: A Simple and Versatile Framework for Image-to-Image Translation,http://arxiv.org/pdf/2007.12072v1,"We introduce a simple and versatile framework for image-to-image translation.We unearth the importance of normalization layers, and provide a carefullydesigned two-stream generative model with newly proposed featuretransformations in a coarse-to-fine fashion. This allows multi-scale semanticstructure information and style representation to be effectively captured andfused by the network, permitting our method to scale to various tasks in bothunsupervised and supervised settings. No additional constraints (e.g., cycleconsistency) are needed, contributing to a very clean and simple method.Multi-modal image synthesis with arbitrary style control is made possible. Asystematic study compares the proposed method with several state-of-the-arttask-specific baselines, verifying its effectiveness in both perceptual qualityand quantitative evaluations.",画像から画像への変換のためのシンプルで用途の広いフレームワークを紹介します。正規化レイヤーの重要性を発掘し、慎重に設計された2ストリーム生成モデルに、新しく提案された特徴変換を粗から細の方法で提供します。これにより、マルチスケールのセマンティック構造情報とスタイル表現をネットワークで効果的にキャプチャして融合できるため、監視されていない設定と監視されている設定の両方でさまざまなタスクにスケーリングできます。追加の制約（たとえば、cycleconsistency）は不要であり、非常にクリーンでシンプルな方法に貢献します。任意のスタイル制御によるマルチモーダル画像合成が可能になります。系統的研究は、提案された方法をいくつかの最先端のタスク固有のベースラインと比較し、知覚品質と定量的評価の両方におけるその有効性を検証します。
http://arxiv.org/abs/2007.12065v1,Polylidar3D -- Fast Polygon Extraction from 3D Data,http://arxiv.org/pdf/2007.12065v1,"Flat surfaces captured by 3D point clouds are often used for localization,mapping, and modeling. Dense point cloud processing has high computation andmemory costs making low-dimensional representations of flat surfaces such aspolygons desirable. We present Polylidar3D, a non-convex polygon extractionalgorithm which takes as input unorganized 3D point clouds (e.g., LiDAR data),organized point clouds (e.g., range images), or user-provided meshes.Non-convex polygons represent flat surfaces in an environment with interiorcutouts representing obstacles or holes. The Polylidar3D front-end transformsinput data into a half-edge triangular mesh. This representation provides acommon level of input data abstraction for subsequent back-end processing. ThePolylidar3D back-end is composed of four core algorithms: mesh smoothing,dominant plane normal estimation, planar segment extraction, and finallypolygon extraction. Polylidar3D is shown to be quite fast, making use of CPUmulti-threading and GPU acceleration when available. We demonstratePolylidar3D's versatility and speed with real-world datasets including aerialLiDAR point clouds for rooftop mapping, autonomous driving LiDAR point cloudsfor road surface detection, and RGBD cameras for indoor floor/wall detection.We also evaluate Polylidar3D on a challenging planar segmentation benchmarkdataset. Results consistently show excellent speed and accuracy.",3D点群によってキャプチャされた平面は、ローカリゼーション、マッピング、モデリングによく使用されます。密な点群処理は、計算やメモリのコストが高く、ポリゴンなどの平面の低次元表現が望ましい。組織化されていない3D点群（例：LiDARデータ）、組織化された点群（例：距離画像）、またはユーザー指定のメッシュを入力として取る非凸ポリゴン抽出アルゴリズムであるPolylidar3Dを提示します。非凸ポリゴンは、障害物や穴を表すinteriorcutoutsのある環境。 Polylidar3Dフロントエンドは、入力データをハーフエッジの三角形メッシュに変換します。この表現は、後続のバックエンド処理に共通レベルの入力データ抽象化を提供します。 The Polylidar3Dバックエンドは、4つのコアアルゴリズムで構成されています：メッシュスムージング、支配的な平面法線推定、平面セグメント抽出、最終的にポリゴン抽出。 Polylidar3Dは非常に高速で、CPUマルチスレッディングとGPUアクセラレーションを利用できる場合はそれを利用しています。屋上マッピング用のaerialLiDAR点群、路面検出用の自動運転LiDAR点群、屋内の床/壁検出用のRGBDカメラなど、現実世界のデータセットを使用してPolylidar3Dの多様性と速度を実証します。結果は一貫して優れた速度と精度を示しています。
http://arxiv.org/abs/2007.12036v1,Implicit Latent Variable Model for Scene-Consistent Motion Forecasting,http://arxiv.org/pdf/2007.12036v1,"In order to plan a safe maneuver an autonomous vehicle must accuratelyperceive its environment, and understand the interactions among trafficparticipants. In this paper, we aim to learn scene-consistent motion forecastsof complex urban traffic directly from sensor data. In particular, we proposeto characterize the joint distribution over future trajectories via an implicitlatent variable model. We model the scene as an interaction graph and employpowerful graph neural networks to learn a distributed latent representation ofthe scene. Coupled with a deterministic decoder, we obtain trajectory samplesthat are consistent across traffic participants, achieving state-of-the-artresults in motion forecasting and interaction understanding. Last but notleast, we demonstrate that our motion forecasts result in safer and morecomfortable motion planning.",安全な操縦を計画するために、自律車両はその環境を正確に認識し、交通参加者間の相互作用を理解する必要があります。このペーパーでは、センサーデータから複雑な都市交通のシーンコンシステントモーション予測を直接学習することを目指しています。特に、潜在的な潜在的変数モデルを介して、将来の軌道上の共同分布を特徴付けることを提案します。シーンを相互作用グラフとしてモデル化し、強力なグラフニューラルネットワークを使用して、シーンの分散潜在表現を学習します。決定論的デコーダーと組み合わせることで、トラフィックの参加者全体で一貫した軌道サンプルを取得し、モーション予測と相互作用の理解において最先端の結果を達成します。最後になりましたが、私たちは私たちのモーション予測がより安全でより快適なモーションプランニングにつながることを示しています。
http://arxiv.org/abs/2007.12034v1,"AttentionNAS: Spatiotemporal Attention Cell Search for Video
  Classification",http://arxiv.org/pdf/2007.12034v1,"Convolutional operations have two limitations: (1) do not explicitly modelwhere to focus as the same filter is applied to all the positions, and (2) areunsuitable for modeling long-range dependencies as they only operate on a smallneighborhood. While both limitations can be alleviated by attention operations,many design choices remain to be determined to use attention, especially whenapplying attention to videos. Towards a principled way of applying attention tovideos, we address the task of spatiotemporal attention cell search. We proposea novel search space for spatiotemporal attention cells, which allows thesearch algorithm to flexibly explore various design choices in the cell. Thediscovered attention cells can be seamlessly inserted into existing backbonenetworks, e.g., I3D or S3D, and improve video classification accuracy by morethan 2% on both Kinetics-600 and MiT datasets. The discovered attention cellsoutperform non-local blocks on both datasets, and demonstrate stronggeneralization across different modalities, backbones, and datasets. Insertingour attention cells into I3D-R50 yields state-of-the-art performance on bothdatasets.",たたみ込み演算には2つの制限があります。（1）同じフィルターがすべての位置に適用されるため、どこに焦点を合わせるかを明示的にモデル化しない、（2）狭い範囲でのみ動作するため、長期依存関係のモデリングには不適切である。どちらの制限もアテンション操作で緩和できますが、特にビデオにアテンションを適用する場合、多くの設計上の選択により、アテンションを使用することが決定されます。注意をビデオに適用する原理的な方法に向けて、私たちは時空間注意セル検索のタスクに取り組みます。セル内のさまざまな設計の選択肢を検索アルゴリズムで柔軟に探索できる、時空間注意セルの新しい検索スペースを提案します。発見されたアテンションセルは、I3DやS3Dなどの既存のバックボーンネットワークにシームレスに挿入でき、Kinetics-600とMiTの両方のデータセットでビデオ分類の精度を2％以上向上させます。発見された注意セルは、両方のデータセットで非ローカルブロックよりも優れており、さまざまなモダリティ、バックボーン、およびデータセットにわたって強い一般化を示しています。 I3D-R50にアテンションセルを挿入すると、両方のデータセットで最先端のパフォーマンスが得られます。
http://arxiv.org/abs/2007.11978v1,"The Devil is in Classification: A Simple Framework for Long-tail
  Instance Segmentation",http://arxiv.org/pdf/2007.11978v1,"Most existing object instance detection and segmentation models only workwell on fairly balanced benchmarks where per-category training sample numbersare comparable, such as COCO. They tend to suffer performance drop on realisticdatasets that are usually long-tailed. This work aims to study and address suchopen challenges. Specifically, we systematically investigate performance dropof the state-of-the-art two-stage instance segmentation model Mask R-CNN on therecent long-tail LVIS dataset, and unveil that a major cause is the inaccurateclassification of object proposals. Based on such an observation, we firstconsider various techniques for improving long-tail classification performancewhich indeed enhance instance segmentation results. We then propose a simplecalibration framework to more effectively alleviate classification head biaswith a bi-level class balanced sampling approach. Without bells and whistles,it significantly boosts the performance of instance segmentation for tailclasses on the recent LVIS dataset and our sampled COCO-LT dataset. Ouranalysis provides useful insights for solving long-tail instance detection andsegmentation problems, and the straightforward \emph{SimCal} method can serveas a simple but strong baseline. With the method we have won the 2019 LVISchallenge. Codes and models are available at\url{https://github.com/twangnh/SimCal}.",既存のほとんどのオブジェクトインスタンス検出およびセグメンテーションモデルは、COCOなど、カテゴリごとのトレーニングサンプル数が比較可能な、かなりバランスのとれたベンチマークでのみ機能します。彼らは、通常は長いものである現実的なデータセットのパフォーマンスが低下する傾向があります。この作業は、そのような未解決の課題を研究して対処することを目的としています。具体的には、最新のロングテールLVISデータセットに対する最新の2ステージインスタンスセグメンテーションモデルマスクR-CNNのパフォーマンス低下を体系的に調査し、主要な原因がオブジェクト提案の不正確な分類であることを明らかにします。このような観察に基づいて、最初に、ロングテール分類のパフォーマンスを向上させるためのさまざまな手法を検討します。これにより、インスタンスのセグメンテーション結果が実際に向上します。次に、2レベルのクラスバランスサンプリングアプローチで分類ヘッドバイアスをより効果的に軽減するためのシンプルなキャリブレーションフレームワークを提案します。ベルとホイッスルがないと、最近のLVISデータセットとサンプルのCOCO-LTデータセットでテールクラスのインスタンスセグメンテーションのパフォーマンスが大幅に向上します。 Ouranalysisは、ロングテールインスタンスの検出とセグメンテーションの問題を解決するための有用な洞察を提供し、単純な\ emph {SimCal}メソッドは、シンプルですが強力なベースラインとして機能します。この方法で、2019 LVISchallengeを獲得しました。コードとモデルは、\ url {https://github.com/twangnh/SimCal}で入手できます。
http://arxiv.org/abs/2007.11965v1,CAD-Deform: Deformable Fitting of CAD Models to 3D Scans,http://arxiv.org/pdf/2007.11965v1,"Shape retrieval and alignment are a promising avenue towards turning 3D scansinto lightweight CAD representations that can be used for content creation suchas mobile or AR/VR gaming scenarios. Unfortunately, CAD model retrieval islimited by the availability of models in standard 3D shape collections (e.g.,ShapeNet). In this work, we address this shortcoming by introducing CAD-Deform,a method which obtains more accurate CAD-to-scan fits by non-rigidly deformingretrieved CAD models. Our key contribution is a new non-rigid deformation modelincorporating smooth transformations and preservation of sharp features, thatsimultaneously achieves very tight fits from CAD models to the 3D scan andmaintains the clean, high-quality surface properties of hand-modeled CADobjects. A series of thorough experiments demonstrate that our method achievessignificantly tighter scan-to-CAD fits, allowing a more accurate digitalreplica of the scanned real-world environment while preserving importantgeometric features present in synthetic CAD environments.",形状の検索と位置合わせは、3Dスキャンを軽量のCAD表現に変え、モバイルやAR / VRゲームシナリオなどのコンテンツ作成に使用できる有望な手段です。残念ながら、CADモデルの取得は、標準の3D形状コレクション（ShapeNetなど）のモデルの可用性によって制限されます。この作業では、CAD-Deformを導入することでこの欠点に対処します。これは、取得されたCADモデルを非厳密に変形することにより、CADとスキャンの適合をより正確に取得する方法です。私たちの主な貢献は、滑らかな変換と鋭い特徴の保持を組み込んだ新しい非剛体変形モデルであり、同時にCADモデルから3Dスキャンへの非常にタイトなフィットを実現し、手でモデリングしたCADオブジェクトのクリーンで高品質な表面特性を維持します。一連の徹底的な実験により、この方法ではスキャンとCADのフィットが大幅に向上し、合成CAD環境に存在する重要な幾何学的機能を維持しながら、スキャンした実世界環境のより正確なデジタル複製が可能になることが示されています。
http://arxiv.org/abs/2007.11946v1,A Solution to Product detection in Densely Packed Scenes,http://arxiv.org/pdf/2007.11946v1,"This work is a solution to densely packed scenes dataset SKU-110k. Our workis modified from cascade R-CNN. To solve the problem, we proposed a random cropstrategy to ensure both the sampling rate and input scale is relativelysufficient as a contrast to the regular random crop. And we adopted some oftrick and optimized the hyper-parameters. To grasp the essential feature of thedensely packed scenes, we analysis the stages of a detector and investigate thebottleneck which limits the performance. As a result, our method obtains 58.7mAP on test set of SKU-110k.",この作業は、密にパックされたシーンデータセットSKU-110kのソリューションです。私たちの仕事はカスケードR-CNNから変更されています。この問題を解決するために、サンプリング率と入力スケールの両方が通常のランダムクロップとは対照的に比較的十分であることを保証するランダムクロップ戦略を提案しました。そして、いくつかのトリックを採用し、ハイパーパラメータを最適化しました。高密度のシーンの本質的な特徴を把握するために、検出器のステージを分析し、パフォーマンスを制限するボトルネックを調査します。その結果、SKU-110kのテストセットで58.7mAPが取得されます。
http://arxiv.org/abs/2007.11924v1,Right for the Right Reason: Making Image Classification Robust,http://arxiv.org/pdf/2007.11924v1,"Convolutional neural networks (CNNs) have achieved astonishing performance onvarious image classification tasks. Although such models classify most imagescorrectly, they do not provide any explanation for their decisions. Recently,there have been attempts to provide such an explanation by determining whichparts of the input image the classifier focuses on most. It turns out that manymodels output the correct classification, but for the wrong reason (e.g., basedon irrelevant parts of the image). In this paper, we propose a new score forautomatically quantifying to which degree the model focuses on the right imageparts. The score is calculated by considering the degree to which the mostdecisive image regions - given by applying an explainer to the CNN model -overlap with the silhouette of the object to be classified. In extensiveexperiments using VGG16, ResNet, and MobileNet as CNNs, Occlusion, LIME, andGrad-Cam/Grad-Cam++ as explanation methods, and Dogs vs. Cats and Caltech 101as data sets, we can show that our metric can indeed be used for making CNNmodels for image classification more robust while keeping their accuracy.",畳み込みニューラルネットワーク（CNN）は、さまざまな画像分類タスクで驚くべきパフォーマンスを実現しています。このようなモデルはほとんどの画像を正しく分類しますが、決定についての説明はありません。最近、分類器が入力画像のどの部分に最も焦点を合わせるかを決定することにより、そのような説明を提供する試みが行われている。多くのモデルは正しい分類を出力しますが、間違った理由（たとえば、画像の無関係な部分に基づく）であることがわかります。この論文では、モデルが適切な画像部分にどの程度焦点を合わせているかを自動的に定量化するための新しいスコアを提案します。スコアは、CNNモデルに説明者を適用することによって与えられる最も決定的な画像領域が、分類されるオブジェクトのシルエットと重複する程度を考慮して計算されます。 VGG16、ResNet、およびMobileNetをCNNとして、Occlusion、LIME、Grad-Cam / Grad-Cam ++を説明メソッドとして、そしてDogs対CatsおよびCaltech 101asのデータセットを使用して広範な実験を行うと、測定基準が実際に使用できることを示すことができます画像分類用のCNNモデルは、精度を維持しながらより堅牢です。
http://arxiv.org/abs/2007.11901v1,Weakly Supervised 3D Object Detection from Lidar Point Cloud,http://arxiv.org/pdf/2007.11901v1,"It is laborious to manually label point cloud data for training high-quality3D object detectors. This work proposes a weakly supervised approach for 3Dobject detection, only requiring a small set of weakly annotated scenes,associated with a few precisely labeled object instances. This is achieved by atwo-stage architecture design. Stage-1 learns to generate cylindrical objectproposals under weak supervision, i.e., only the horizontal centers of objectsare click-annotated on bird's view scenes. Stage-2 learns to refine thecylindrical proposals to get cuboids and confidence scores, using a fewwell-labeled object instances. Using only 500 weakly annotated scenes and 534precisely labeled vehicle instances, our method achieves 85-95% the performanceof current top-leading, fully supervised detectors (which require 3, 712exhaustively and precisely annotated scenes with 15, 654 instances). Moreimportantly, with our elaborately designed network architecture, our trainedmodel can be applied as a 3D object annotator, allowing both automatic andactive working modes. The annotations generated by our model can be used totrain 3D object detectors with over 94% of their original performance (undermanually labeled data). Our experiments also show our model's potential inboosting performance given more training data. Above designs make our approachhighly practical and introduce new opportunities for learning 3D objectdetection with reduced annotation burden.",高品質の3Dオブジェクト検出器をトレーニングするために、手動で点群データにラベルを付けるのは面倒です。この作業は、3Dオブジェクト検出のための弱く監視されたアプローチを提案し、正確にラベル付けされたいくつかのオブジェクトインスタンスに関連付けられた、弱く注釈が付けられたシーンの小さなセットのみを必要とします。これは、2段階のアーキテクチャ設計によって実現されます。 Stage-1は、弱い監視のもとで円柱状のオブジェクトの提案を生成することを学習します。つまり、オブジェクトの水平方向の中心のみが鳥瞰図のシーンでクリックアノテーションされます。ステージ2は、数個の適切にラベル付けされたオブジェクトインスタンスを使用して、円筒形の提案を改良し、直方体と信頼スコアを取得する方法を学習します。わずか500の弱く注釈が付けられたシーンと534の正確にラベル付けされた車両インスタンスを使用して、私たちの方法は、現在の最高の完全監視型検出器のパフォーマンスを85〜95％達成します（15、654のインスタンスで、3、712網羅的かつ正確に注釈が付けられたシーンが必要です）。さらに重要なことに、私たちの精巧に設計されたネットワークアーキテクチャにより、訓練されたモデルは3Dオブジェクトアノテーターとして適用でき、自動とアクティブの両方の作業モードが可能になります。モデルによって生成された注釈を使用して、3Dオブジェクト検出器を元のパフォーマンス（手動でラベル付けされたデータ）の94％以上でトレーニングできます。私たちの実験はまた、より多くのトレーニングデータを与えられたモデルの潜在的な能力向上の可能性を示しています。上記の設計は、私たちのアプローチを非常に実用的なものにし、注釈の負担を減らして3Dオブジェクト検出を学習する新しい機会をもたらします。
http://arxiv.org/abs/2007.11899v1,"Harnessing spatial homogeneity of neuroimaging data: patch individual
  filter layers for CNNs",http://arxiv.org/pdf/2007.11899v1,"Neuroimaging data, e.g. obtained from magnetic resonance imaging (MRI), iscomparably homogeneous due to (1) the uniform structure of the brain and (2)additional efforts to spatially normalize the data to a standard template usinglinear and non-linear transformations. Convolutional neural networks (CNNs), incontrast, have been specifically designed for highly heterogeneous data, suchas natural images, by sliding convolutional filters over different positions inan image. Here, we suggest a new CNN architecture that combines the idea ofhierarchical abstraction in neural networks with a prior on the spatialhomogeneity of neuroimaging data: Whereas early layers are trained globallyusing standard convolutional layers, we introduce for higher, more abstractlayers patch individual filters (PIF). By learning filters in individual imageregions (patches) without sharing weights, PIF layers can learn abstractfeatures faster and with fewer samples. We thoroughly evaluated PIF layers forthree different tasks and data sets, namely sex classification on UK Biobankdata, Alzheimer's disease detection on ADNI data and multiple sclerosisdetection on private hospital data. We demonstrate that CNNs using PIF layersresult in higher accuracies, especially in low sample size settings, and needfewer training epochs for convergence. To the best of our knowledge, this isthe first study which introduces a prior on brain MRI for CNN learning.",神経画像データ、例えば磁気共鳴画像法（MRI）から得られたものは、（1）脳の均一な構造と（2）線形および非線形変換を使用してデータを標準テンプレートに空間的に正規化するための追加の努力により、比較的均質です。畳み込みニューラルネットワーク（CNN）は対照的に、畳み込みフィルターを画像内のさまざまな位置にスライドさせることにより、自然画像などの高度に不均一なデータ用に特別に設計されています。ここでは、ニューラルイメージングデータの空間的均一性に関する事前知識とニューラルネットワークの階層的抽象化の概念を組み合わせた新しいCNNアーキテクチャを提案します。 。重みを共有せずに個々の画像領域（パッチ）でフィルターを学習することにより、PIFレイヤーはより少ないサンプルでより迅速に抽象的な特徴を学習できます。 3つの異なるタスクとデータセット、つまりUK Biobankdataでの性別分類、ADNIデータでのアルツハイマー病の検出、および私立病院データでの多発性硬化症の検出をPIFレイヤーで徹底的に評価しました。 PIFレイヤーを使用するCNNは、特にサンプルサイズの設定が低い場合に精度が高くなり、収束のためのトレーニングエポックが少なくなることを示します。私たちの知る限りでは、これはCNN学習のために脳のMRIに事前情報を導入する最初の研究です。
http://arxiv.org/abs/2007.11888v1,SBAT: Video Captioning with Sparse Boundary-Aware Transformer,http://arxiv.org/pdf/2007.11888v1,"In this paper, we focus on the problem of applying the transformer structureto video captioning effectively. The vanilla transformer is proposed foruni-modal language generation task such as machine translation. However, videocaptioning is a multimodal learning problem, and the video features have muchredundancy between different time steps. Based on these concerns, we propose anovel method called sparse boundary-aware transformer (SBAT) to reduce theredundancy in video representation. SBAT employs boundary-aware poolingoperation for scores from multihead attention and selects diverse features fromdifferent scenarios. Also, SBAT includes a local correlation scheme tocompensate for the local information loss brought by sparse operation. Based onSBAT, we further propose an aligned cross-modal encoding scheme to boost themultimodal interaction. Experimental results on two benchmark datasets showthat SBAT outperforms the state-of-the-art methods under most of the metrics.",この論文では、トランスフォーマー構造をビデオキャプションに効果的に適用する問題に焦点を当てます。バニラ変換器は、機械翻訳などのユニモーダル言語生成タスク用に提案されています。ただし、ビデオキャプションはマルチモーダル学習の問題であり、ビデオ機能は異なるタイムステップ間で多くの冗長性を持っています。これらの懸念に基づいて、スパース境界認識トランスフォーマー（SBAT）と呼ばれるアノベル法を提案し、ビデオ表現の冗長性を削減します。 SBATは、マルチヘッドアテンションからのスコアに境界認識プール操作を採用し、さまざまなシナリオから多様な機能を選択します。また、SBATには、スパース操作によるローカル情報の損失を補償するローカル相関スキームが含まれています。さらに、SBATに基づいて、マルチモーダル相互作用を強化するために、整列されたクロスモーダルエンコーディング方式を提案します。 2つのベンチマークデータセットの実験結果は、SBATがほとんどのメトリックの下で最先端の方法よりも優れていることを示しています。
http://arxiv.org/abs/2007.11876v1,"Real-time CNN-based Segmentation Architecture for Ball Detection in a
  Single View Setup",http://arxiv.org/pdf/2007.11876v1,"This paper considers the task of detecting the ball from a single viewpointin the challenging but common case where the ball interacts frequently withplayers while being poorly contrasted with respect to the background. Wepropose a novel approach by formulating the problem as a segmentation tasksolved by an efficient CNN architecture. To take advantage of the balldynamics, the network is fed with a pair of consecutive images. Our inferencemodel can run in real time without the delay induced by a temporal analysis. Wealso show that test-time data augmentation allows for a significant increasethe detection accuracy. As an additional contribution, we publicly release thedataset on which this work is based.",この論文では、ボールが背景とのコントラストが低く、プレーヤーと頻繁に相互作用する、難しいが一般的なケースで、単一の視点からボールを​​検出するタスクについて考察します。効率的なCNNアーキテクチャによって解決されるセグメンテーションタスクとして問題を定式化することにより、新しいアプローチを提案します。ボールダイナミクスを利用するために、ネットワークには1組の連続した画像が供給されます。私たちの推論モデルは、時間分析によって引き起こされる遅延なしにリアルタイムで実行できます。また、テスト時のデータ拡張により、検出精度が大幅に向上することも示しています。追加の貢献として、この作業の基礎となるデータセットを公開しました。
http://arxiv.org/abs/2007.11866v1,ReLaB: Reliable Label Bootstrapping for Semi-Supervised Learning,http://arxiv.org/pdf/2007.11866v1,"Reducing the amount of labels required to trainconvolutional neural networkswithout performance degradationis key to effectively reduce human annotationeffort. We pro-pose Reliable Label Bootstrapping (ReLaB), anunsupervisedpreprossessing algorithm that paves the way forsemi-supervisedlearning solutions, enabling them to work with muchlowersupervision. Given a dataset with few labeled samples, we firstexploit aself-supervised learning algorithm to learn unsupervisedlatent features andthen apply a label propagation algorithm onthese features and select onlycorrectly labeled samples using alabel noise detection algorithm. This enablesReLaB to createa reliable extended labeled set from the initially fewlabeledsamples that can then be used for semi-supervised learning.We show thatthe selection of the network architecture andthe self-supervised method areimportant to achieve successfullabel propagation and demonstrate that ReLaBsubstantiallyimproves semi-supervised learning in scenarios of very lim-itedsupervision in CIFAR-10, CIFAR-100, and mini-ImageNet. Code:https://github.com/PaulAlbert31/ReLaB.",パフォーマンスを低下させることなく畳み込みニューラルネットワークをトレーニングするために必要なラベルの量を減らすことは、人間の注釈の労力を効果的に減らすための鍵です。私たちは、信頼性の高いラベルブートストラップ（ReLaB）を提案します。これは、半教師付き学習ソリューションの道を開き、はるかに低い監視での作業を可能にする教師なし準備プロセスアルゴリズムです。ラベル付きサンプルが少ないデータセットを前提として、まず教師なし学習機能を活用して教師なし潜在的特徴を学習し、これらの特徴にラベル伝播アルゴリズムを適用して、ラベルノイズ検出アルゴリズムを使用して正しくラベル付けされたサンプルのみを選択します。これにより、ReLaBは、最初は少数のラベル付きサンプルから信頼できる拡張ラベル付きセットを作成できます。これは、半教師あり学習に使用できます。ネットワークアーキテクチャの選択と自己教師ありメソッドが重要なラベル伝播を達成するために重要であることを示し、ReLaBが半教師あり学習を実質的に改善することを示します。 CIFAR-10、CIFAR-100、およびmini-ImageNetの非常に制限された監視のシナリオ。コード：https：//github.com/PaulAlbert31/ReLaB。
http://arxiv.org/abs/2007.11864v1,"Differentiable Hierarchical Graph Grouping for Multi-Person Pose
  Estimation",http://arxiv.org/pdf/2007.11864v1,"Multi-person pose estimation is challenging because it localizes bodykeypoints for multiple persons simultaneously. Previous methods can be dividedinto two streams, i.e. top-down and bottom-up methods. The top-down methodslocalize keypoints after human detection, while the bottom-up methods localizekeypoints directly and then cluster/group them for different persons, which aregenerally more efficient than top-down methods. However, in existing bottom-upmethods, the keypoint grouping is usually solved independently from keypointdetection, making them not end-to-end trainable and have sub-optimalperformance. In this paper, we investigate a new perspective of human partgrouping and reformulate it as a graph clustering task. Especially, we proposea novel differentiable Hierarchical Graph Grouping (HGG) method to learn thegraph grouping in bottom-up multi-person pose estimation task. Moreover, HGG iseasily embedded into main-stream bottom-up methods. It takes human keypointcandidates as graph nodes and clusters keypoints in a multi-layer graph neuralnetwork model. The modules of HGG can be trained end-to-end with the keypointdetection network and is able to supervise the grouping process in ahierarchical manner. To improve the discrimination of the clustering, we add aset of edge discriminators and macro-node discriminators. Extensive experimentson both COCO and OCHuman datasets demonstrate that the proposed method improvesthe performance of bottom-up pose estimation methods.",複数人の姿勢推定は、複数の人のボディキーポイントを同時にローカライズするため、困難です。以前のメソッドは、2つのストリーム、つまりトップダウンメソッドとボトムアップメソッドに分割できます。トップダウン方式は人の検出後にキーポイントをローカライズしますが、ボトムアップ方式は直接キーポイントをローカライズし、さまざまな人のためにそれらをクラスター/グループ化します。これは一般にトップダウン方式よりも効率的です。ただし、既存のボトムアップ方式では、キーポイントのグループ化は通常、キーポイントの検出とは独立して解決されるため、エンドツーエンドのトレーニングが不可能になり、パフォーマンスが最適化されません。このペーパーでは、人間のパーツグループ化の新しい視点を調査し、グラフのクラスタリングタスクとして再定式化します。特に、ボトムアップの複数人ポーズ推定タスクでグラフのグループ化を学習するための、新しい微分可能な階層グラフグループ化（HGG）手法を提案します。さらに、HGGは主流のボトムアップ方式に簡単に組み込まれます。人間のキーポイント候補をグラフノードとして取り、多層グラフニューラルネットワークモデルのキーポイントをクラスター化します。 HGGのモジュールは、キーポイント検出ネットワークを使用してエンドツーエンドでトレーニングでき、階層化された方法でグループ化プロセスを監視できます。クラスタリングの識別を改善するために、一連のエッジ弁別器とマクロノード弁別器を追加します。 COCOとOCHumanの両方のデータセットに対する広範な実験は、提案された方法がボトムアップポーズ推定方法のパフォーマンスを向上させることを示しています。
http://arxiv.org/abs/2007.11858v1,Whole-Body Human Pose Estimation in the Wild,http://arxiv.org/pdf/2007.11858v1,"This paper investigates the task of 2D human whole-body pose estimation,which aims to localize dense landmarks on the entire human body including face,hands, body, and feet. As existing datasets do not have whole-body annotations,previous methods have to assemble different deep models trained independentlyon different datasets of the human face, hand, and body, struggling withdataset biases and large model complexity. To fill in this blank, we introduceCOCO-WholeBody which extends COCO dataset with whole-body annotations. To ourbest knowledge, it is the first benchmark that has manual annotations on theentire human body, including 133 dense landmarks with 68 on the face, 42 onhands and 23 on the body and feet. A single-network model, named ZoomNet, isdevised to take into account the hierarchical structure of the full human bodyto solve the scale variation of different body parts of the same person.ZoomNet is able to significantly outperform existing methods on the proposedCOCO-WholeBody dataset. Extensive experiments show that COCO-WholeBody not onlycan be used to train deep models from scratch for whole-body pose estimationbut also can serve as a powerful pre-training dataset for many different taskssuch as facial landmark detection and hand keypoint estimation. The dataset ispublicly available at https://github.com/jin-s13/COCO-WholeBody.",この論文では、顔、手、体、足を含む人体全体に密集したランドマークを特定することを目的とした、2Dの人体全身姿勢推定のタスクを調査します。既存のデータセットには全身注釈がないため、以前の方法では、人間の顔、手、体のさまざまなデータセットで個別にトレーニングされたさまざまなディープモデルを組み立て、データセットのバイアスと大きなモデルの複雑さに苦労しています。この空白を埋めるために、COCOデータセットを全身注釈で拡張するCOCO-WholeBodyを導入します。私たちの知る限り、これは人の体全体に手動で注釈を付けた最初のベンチマークです。顔に68、手に42、体と足に23の133の密集したランドマークがあります。 ZoomNetという名前の単一ネットワークモデルは、人体全体の階層構造を考慮して、同じ人のさまざまな身体部分のスケール変動を解決するように考案されています。ZoomNetは、提案されたCOCO-WholeBodyデータセットの既存のメソッドを大幅に上回ることができます。大規模な実験により、COCO-WholeBodyは、全身ポーズ推定のためにゼロから深いモデルをトレーニングするために使用できるだけでなく、顔のランドマーク検出や手のキーポイント推定などのさまざまなタスクの強力な事前トレーニングデータセットとしても使用できることを示しています。データセットはhttps://github.com/jin-s13/COCO-WholeBodyで公開されています。
http://arxiv.org/abs/2007.11855v1,Neural Geometric Parser for Single Image Camera Calibration,http://arxiv.org/pdf/2007.11855v1,"We propose a neural geometric parser learning single image camera calibrationfor man-made scenes. Unlike previous neural approaches that rely only onsemantic cues obtained from neural networks, our approach considers bothsemantic and geometric cues, resulting in significant accuracy improvement. Theproposed framework consists of two networks. Using line segments of an image asgeometric cues, the first network estimates the zenith vanishing point andgenerates several candidates consisting of the camera rotation and focallength. The second network evaluates each candidate based on the given imageand the geometric cues, where prior knowledge of man-made scenes is used forthe evaluation. With the supervision of datasets consisting of the horizontalline and focal length of the images, our networks can be trained to estimatethe same camera parameters. Based on the Manhattan world assumption, we canfurther estimate the camera rotation and focal length in a weakly supervisedmanner. The experimental results reveal that the performance of our neuralapproach is significantly higher than that of existing state-of-the-art cameracalibration techniques for single images of indoor and outdoor scenes.",人工シーン用の単一画像カメラのキャリブレーションを学習する神経幾何学的パーサーを提案します。ニューラルネットワークから取得された意味論的キューのみに依存する以前のニューラルアプローチとは異なり、私たちのアプローチは意味論的および幾何学的キューの両方を考慮し、大幅な精度向上をもたらします。提案されているフレームワークは2つのネットワークで構成されています。画像の幾何学的キューのラインセグメントを使用して、最初のネットワークは天頂消失点を推定し、カメラの回転と焦点距離からなるいくつかの候補を生成します。 2番目のネットワークは、与えられた画像と幾何学的キューに基づいて各候補を評価します。人工シーンの事前知識が評価に使用されます。画像の水平線と焦点距離で構成されるデータセットを監視することで、同じカメラパラメータを推定するようにネットワークをトレーニングできます。マンハッタンの世界の仮定に基づいて、弱く監視された方法でカメラの回転と焦点距離をさらに推定できます。実験結果は、私たちの神経アプローチのパフォーマンスが、屋内と屋外のシーンの単一の画像に対する既存の最先端のカメラキャリブレーション技術のパフォーマンスよりも大幅に高いことを示しています。
http://arxiv.org/abs/2007.11840v1,"Regularization of Building Boundaries in Satellite Images using
  Adversarial and Regularized Losses",http://arxiv.org/pdf/2007.11840v1,"In this paper we present a method for building boundary refinement andregularization in satellite images using a fully convolutional neural networktrained with a combination of adversarial and regularized losses. Compared to apure Mask R-CNN model, the overall algorithm can achieve equivalent performancein terms of accuracy and completeness. However, unlike Mask R-CNN that producesirregular footprints, our framework generates regularized and visually pleasingbuilding boundaries which are beneficial in many applications.",この論文では、敵対的損失と正則化損失の組み合わせで訓練された完全畳み込みニューラルネットワークを使用して、衛星画像の境界の細分化と正則化を構築する方法を紹介します。 apure Mask R-CNNモデルと比較すると、全体的なアルゴリズムは、精度と完全性の点で同等のパフォーマンスを実現できます。ただし、不規則なフットプリントを生成するマスクR-CNNとは異なり、私たちのフレームワークは、多くのアプリケーションで有益な、規則的で視覚的に心地よい建物の境界を生成します。
http://arxiv.org/abs/2007.11824v1,Funnel Activation for Visual Recognition,http://arxiv.org/pdf/2007.11824v1,"We present a conceptually simple but effective funnel activation for imagerecognition tasks, called Funnel activation (FReLU), that extends ReLU andPReLU to a 2D activation by adding a negligible overhead of spatial condition.The forms of ReLU and PReLU are y = max(x, 0) and y = max(x, px), respectively,while FReLU is in the form of y = max(x,T(x)), where T(x) is the 2D spatialcondition. Moreover, the spatial condition achieves a pixel-wise modelingcapacity in a simple way, capturing complicated visual layouts with regularconvolutions. We conduct experiments on ImageNet, COCO detection, and semanticsegmentation tasks, showing great improvements and robustness of FReLU in thevisual recognition tasks.",空間条件の無視できるオーバーヘッドを追加することによってReLUおよびPReLUを2Dアクティベーションに拡張する、ファンネルアクティベーション（FReLU）と呼ばれる、画像認識タスクの概念的にシンプルだが効果的なファンネルアクティベーションを提示します。ReLUおよびPReLUの形式は、y = max（x、 0）およびy = max（x、px）、一方、FReLUはy = max（x、T（x））の形式で、T（x）は2D空間条件です。さらに、空間条件はピクセル単位のモデリング能力を簡単な方法で実現し、複雑な視覚的レイアウトを通常の畳み込みでキャプチャします。 ImageNet、COCO検出、およびセマンティクスセグメンテーションタスクの実験を行い、視覚認識タスクにおけるFReLUの大幅な改善と堅牢性を示します。
http://arxiv.org/abs/2007.11823v1,WeightNet: Revisiting the Design Space of Weight Networks,http://arxiv.org/pdf/2007.11823v1,"We present a conceptually simple, flexible and effective framework for weightgenerating networks. Our approach is general that unifies two current distinctand extremely effective SENet and CondConv into the same framework on weightspace. The method, called WeightNet, generalizes the two methods by simplyadding one more grouped fully-connected layer to the attention activationlayer. We use the WeightNet, composed entirely of (grouped) fully-connectedlayers, to directly output the convolutional weight. WeightNet is easy andmemory-conserving to train, on the kernel space instead of the feature space.Because of the flexibility, our method outperforms existing approaches on bothImageNet and COCO detection tasks, achieving better Accuracy-FLOPs andAccuracy-Parameter trade-offs. The framework on the flexible weight space hasthe potential to further improve the performance. Code is available athttps://github.com/megvii-model/WeightNet.",ネットワークを重み付けするための概念的にシンプルで柔軟かつ効果的なフレームワークを提示します。私たちのアプローチは一般的で、2つの現在の非常に効果的なSENetとCondConvを重み空間上の同じフレームワークに統合します。 WeightNetと呼ばれるこのメソッドは、グループ化された完全に接続された1つ以上のレイヤーをアテンションアクティベーションレイヤーに追加するだけで、2つのメソッドを一般化します。完全に（グループ化された）完全に接続されたレイヤーで構成されるWeightNetを使用して、畳み込み重みを直接出力します。 WeightNetは、機能空間ではなくカーネル空間でトレーニングするのが簡単で、メモリを節約します。柔軟性があるため、このメソッドはImageNetとCOCOの両方の検出タスクで既存のアプローチよりも優れており、精度-FLOPと精度-パラメーターのトレードオフが向上しています。フレキシブルウェイトスペースのフレームワークは、パフォーマンスをさらに向上させる可能性があります。コードはhttps://github.com/megvii-model/WeightNetで入手できます。
http://arxiv.org/abs/2007.11814v1,Zero-Shot Recognition through Image-Guided Semantic Classification,http://arxiv.org/pdf/2007.11814v1,"We present a new embedding-based framework for zero-shot learning (ZSL). Mostembedding-based methods aim to learn the correspondence between an imageclassifier (visual representation) and its class prototype (semanticrepresentation) for each class. Motivated by the binary relevance method formulti-label classification, we propose to inversely learn the mapping betweenan image and a semantic classifier. Given an input image, the proposedImage-Guided Semantic Classification (IGSC) method creates a label classifier,being applied to all label embeddings to determine whether a label belongs tothe input image. Therefore, semantic classifiers are image-adaptive and aregenerated during inference. IGSC is conceptually simple and can be realized bya slight enhancement of an existing deep architecture for classification; yetit is effective and outperforms state-of-the-art embedding-based generalizedZSL approaches on standard benchmarks.",ゼロショット学習（ZSL）のための新しい埋め込みベースのフレームワークを紹介します。 Mostembeddingベースのメソッドは、各クラスの画像分類子（視覚的表現）とそのクラスプロトタイプ（意味表現）の間の対応を学習することを目的としています。マルチラベル分類のバイナリ関連性メソッドに動機付けられて、画像と意味的分類子の間のマッピングを逆に学習することを提案します。入力画像が与えられると、proposedImage-Guided Semantic Classification（IGSC）メソッドは、すべてのラベル埋め込みに適用されるラベル分類子を作成し、ラベルが入力画像に属するかどうかを決定します。したがって、意味分類子は画像適応型であり、推論中に生成されます。 IGSCは概念的にシンプルであり、分類のための既存のディープアーキテクチャを少し強化することで実現できます。しかし、それは効果的であり、標準的なベンチマークで、最先端の埋め込みベースのgeneralizedZSLアプローチよりも優れています。
http://arxiv.org/abs/2007.11806v1,"Autonomous Removal of Perspective Distortion based on Detection Results
  of Robotic Elevator Button Corner",http://arxiv.org/pdf/2007.11806v1,"Elevator button recognition is an important function to realize theautonomous operation of elevators. However, challenging image conditions andvarious image distortions make it difficult to accurately recognize buttons. Inthis work, We propose a novel algorithm that can automatically correctperspective distortions of elevator panel images based on button cornerdetection results. The algorithm first leverages DeepLabv3+ model and HoughTransform method to obtain button segmentation results and button cornerdetection results, then utilizes pixel coordinates of standard button cornersas reference features to estimate camera motions for correcting perspectivedistortions. The algorithm is much more robust to outliers and noise on theremoval of perspective distortion than traditional geometric approaches as itonly performs on a single image autonomously. 15 elevator panel images arecaptured from different angles of view as the dataset. The experimental resultsshow that our approach significantly outperforms traditional geometrictechniques in accuracy and robustness. Rectification results of the proposedalgorithm is 77.4% better than the results of traditional geometric algorithmin average.",エレベーターボタンの認識は、エレベーターの自律運転を実現するための重要な機能です。ただし、困難な画像条件やさまざまな画像の歪みにより、ボタンを正確に認識することが難しくなります。この作業では、ボタンのコーナー検出結果に基づいて、エレベータパネル画像の遠近歪みを自動的に補正できる新しいアルゴリズムを提案します。このアルゴリズムは、最初にDeepLabv3 +モデルとHoughTransformメソッドを利用してボタンのセグメンテーション結果とボタンのコーナー検出結果を取得し、次に標準のボタンコーナーのピクセル座標を参照機能として使用して、遠近歪みを補正するためのカメラの動きを推定します。このアルゴリズムは、単一の画像でのみ自律的に実行されるため、従来の幾何学的アプローチよりも遠近歪みの除去における外れ値やノイズに対してはるかに堅牢です。 15のエレベータパネル画像が、データセットとしてさまざまな角度からキャプチャされます。実験結果は、私たちのアプローチが精度と堅牢性において従来の幾何学的手法を大幅に上回っていることを示しています。提案されたアルゴリズムの修正結果は、従来の幾何学的アルゴリズムの結果よりも平均で77.4％優れています。
http://arxiv.org/abs/2007.11803v1,"MuCAN: Multi-Correspondence Aggregation Network for Video
  Super-Resolution",http://arxiv.org/pdf/2007.11803v1,"Video super-resolution (VSR) aims to utilize multiple low-resolution framesto generate a high-resolution prediction for each frame. In this process,inter- and intra-frames are the key sources for exploiting temporal and spatialinformation. However, there are a couple of limitations for existing VSRmethods. First, optical flow is often used to establish temporalcorrespondence. But flow estimation itself is error-prone and affects recoveryresults. Second, similar patterns existing in natural images are rarelyexploited for the VSR task. Motivated by these findings, we propose a temporalmulti-correspondence aggregation strategy to leverage similar patches acrossframes, and a cross-scale nonlocal-correspondence aggregation scheme to exploreself-similarity of images across scales. Based on these two new modules, webuild an effective multi-correspondence aggregation network (MuCAN) for VSR.Our method achieves state-of-the-art results on multiple benchmark datasets.Extensive experiments justify the effectiveness of our method.",ビデオ超解像（VSR）は、複数の低解像度フレームを利用して、各フレームの高解像度予測を生成することを目的としています。このプロセスでは、インターフレームとイントラフレームが時間的および空間的情報を活用するための重要なソースです。ただし、既存のVSRメソッドにはいくつかの制限があります。第1に、時間的対応を確立するためにオプティカルフローがよく使用されます。ただし、フロー推定自体はエラーが発生しやすく、リカバリ結果に影響します。第二に、自然画像に存在する同様のパターンは、VSRタスクではほとんど利用されません。これらの調査結果に動機付けられて、フレーム全体で同様のパッチを活用するための時間的多重対応集約戦略と、スケール全体で画像の自己類似性を探索するためのクロススケール非ローカル対応集約方式を提案します。これら2つの新しいモジュールに基づいて、VSRのための効果的な多重対応集約ネットワーク（MuCAN）を構築します。この手法は、複数のベンチマークデータセットで最先端の結果を達成します。広範な実験により、手法の有効性が正当化されます。
http://arxiv.org/abs/2007.11797v1,End-to-end Learning of Compressible Features,http://arxiv.org/pdf/2007.11797v1,"Pre-trained convolutional neural networks (CNNs) are powerful off-the-shelffeature generators and have been shown to perform very well on a variety oftasks. Unfortunately, the generated features are high dimensional and expensiveto store: potentially hundreds of thousands of floats per example whenprocessing videos. Traditional entropy based lossless compression methods areof little help as they do not yield desired level of compression, while generalpurpose lossy compression methods based on energy compaction (e.g. PCA followedby quantization and entropy coding) are sub-optimal, as they are not tuned totask specific objective. We propose a learned method that jointly optimizes forcompressibility along with the task objective for learning the features. Theplug-in nature of our method makes it straight-forward to integrate with anytarget objective and trade-off against compressibility. We present results onmultiple benchmarks and demonstrate that our method produces features that arean order of magnitude more compressible, while having a regularization effectthat leads to a consistent improvement in accuracy.",事前トレーニング済みの畳み込みニューラルネットワーク（CNN）は、既成の強力なジェネレーターであり、さまざまなタスクで非常にうまく機能することが示されています。残念ながら、生成された機能は高次元であり、保存に費用がかかります。ビデオを処理する場合、例ごとに数十万のフロートが潜在的に存在します。従来のエントロピーベースのロスレス圧縮方法は、目的のレベルの圧縮が得られないためほとんど役に立ちませんが、エネルギー圧縮に基づく汎用のロスのある圧縮方法（PCAに続いて量子化やエントロピーコーディングなど）は、タスク固有の目的に合わせて調整されていないため、最適ではありません。 。特徴を学習するためのタスクの目的と共に圧縮性を共同で最適化する学習方法を提案します。私たちの方法のプラグインの性質により、任意のターゲットの目的と簡単に統合でき、圧縮率とのトレードオフになります。複数のベンチマークの結果を提示し、この方法では、精度が一貫して向上する正則化効果がありながら、圧縮率が1桁高い機能を生成することを示します。
http://arxiv.org/abs/2007.11782v1,Accurate RGB-D Salient Object Detection via Collaborative Learning,http://arxiv.org/pdf/2007.11782v1,"Benefiting from the spatial cues embedded in depth images, recent progress onRGB-D saliency detection shows impressive ability on some challenge scenarios.However, there are still two limitations. One hand is that the pooling andupsampling operations in FCNs might cause blur object boundaries. On the otherhand, using an additional depth-network to extract depth features might lead tohigh computation and storage cost. The reliance on depth inputs during testingalso limits the practical applications of current RGB-D models. In this paper,we propose a novel collaborative learning framework where edge, depth andsaliency are leveraged in a more efficient way, which solves those problemstactfully. The explicitly extracted edge information goes together withsaliency to give more emphasis to the salient regions and object boundaries.Depth and saliency learning is innovatively integrated into the high-levelfeature learning process in a mutual-benefit manner. This strategy enables thenetwork to be free of using extra depth networks and depth inputs to makeinference. To this end, it makes our model more lightweight, faster and moreversatile. Experiment results on seven benchmark datasets show its superiorperformance.",深度画像に埋め込まれた空間的手がかりの恩恵を受けて、RGB-D顕著性検出に関する最近の進歩は、いくつかの課題シナリオで印象的な能力を示していますが、まだ2つの制限があります。一方で、FCNのプーリングおよびアップサンプリング操作により、オブジェクトの境界がぼやける可能性があります。一方、深度機能を抽出するために追加の深度ネットワークを使用すると、計算とストレージのコストが高くなる可能性があります。テスト中の深度入力への依存は、現在のRGB-Dモデルの実用的なアプリケーションも制限します。この論文では、エッジ、深度、顕著性がより効率的に活用され、これらの問題を巧みに解決する新しい協調学習フレームワークを提案します。明示的に抽出されたエッジ情報は顕著性と一緒になり、顕著領域とオブジェクト境界をより強調します。深度と顕著性の学習は、相互にメリットのある方法で高度な機能の学習プロセスに革新的に統合されます。この戦略により、ネットワークは追加の深さネットワークと深さ入力を使用せずに推論することができます。この目的のために、モデルをより軽量、より高速に、より汎用的にします。 7つのベンチマークデータセットの実験結果は、その優れたパフォーマンスを示しています。
http://arxiv.org/abs/2007.11770v1,"Illumination invariant hyperspectral image unmixing based on a digital
  surface model",http://arxiv.org/pdf/2007.11770v1,"Although many spectral unmixing models have been developed to addressspectral variability caused by variable incident illuminations, the mechanismof the spectral variability is still unclear. This paper proposes an unmixingmodel, named illumination invariant spectral unmixing (IISU). IISU makes thefirst attempt to use the radiance hyperspectral data and a LiDAR-deriveddigital surface model (DSM) in order to physically explain variableilluminations and shadows in the unmixing framework. Incident angles, skyfactors, visibility from the sun derived from the LiDAR-derived DSM support theexplicit explanation of endmember variability in the unmixing process fromradiance perspective. The proposed model was efficiently solved by astraightforward optimization procedure. The unmixing results showed that theother state-of-the-art unmixing models did not work well especially in theshaded pixels. On the other hand, the proposed model estimated more accurateabundances and shadow compensated reflectance than the existing models.",さまざまな入射照明によって引き起こされるスペクトル変動に対処するために、多くのスペクトル非混合モデルが開発されてきましたが、スペクトル変動のメカニズムはまだ不明です。この論文では、照明不変スペクトル非混合（IISU）という名前の非混合モデルを提案します。 IISUは、アンミックスフレームワークで可変照明とシャドウを物理的に説明するために、輝度ハイパースペクトルデータとLiDAR派生デジタルサーフェスモデル（DSM）を使用する最初の試みを行います。入射角、スカイファクター、LiDAR派生のDSMから派生した太陽からの可視性は、放射輝度の観点からの非混合プロセスにおける端成分の変動性の明示的な説明をサポートします。提案されたモデルは、直接的な最適化手順によって効率的に解決されました。分離結果は、他の最先端の分離モデルが特にシェーディングされたピクセルでうまく機能しないことを示しました。一方、提案されたモデルは、既存のモデルよりも正確な存在量と影補正反射率を推定しました。
http://arxiv.org/abs/2007.11766v1,Guided Deep Decoder: Unsupervised Image Pair Fusion,http://arxiv.org/pdf/2007.11766v1,"The fusion of input and guidance images that have a tradeoff in theirinformation (e.g., hyperspectral and RGB image fusion or pansharpening) can beinterpreted as one general problem. However, previous studies applied atask-specific handcrafted prior and did not address the problems with a unifiedapproach. To address this limitation, in this study, we propose a guided deepdecoder network as a general prior. The proposed network is composed of anencoder-decoder network that exploits multi-scale features of a guidance imageand a deep decoder network that generates an output image. The two networks areconnected by feature refinement units to embed the multi-scale features of theguidance image into the deep decoder network. The proposed network allows thenetwork parameters to be optimized in an unsupervised way without trainingdata. Our results show that the proposed network can achieve state-of-the-artperformance in various image fusion problems.",情報にトレードオフがある入力画像とガイダンス画像の融合（ハイパースペクトルとRGB画像の融合やパンシャープンなど）は、1つの一般的な問題として解釈できます。ただし、以前の研究では、事前に作成されたタスク固有のタスクを適用し、統一されたアプローチの問題に対処していません。この制限に対処するために、この研究では、一般的な先行技術として、ガイド付きディープデコーダーネットワークを提案します。提案されたネットワークは、ガイダンス画像のマルチスケール機能を利用するエンコーダーデコーダーネットワークと、出力画像を生成するディープデコーダーネットワークで構成されています。 2つのネットワークは、機能改善ユニットによって接続され、ガイダンスイメージのマルチスケール機能をディープデコーダーネットワークに埋め込みます。提案されたネットワークにより、トレーニングデータなしで監視されていない方法でネットワークパラメーターを最適化できます。私たちの結果は、提案されたネットワークがさまざまな画像融合問題で最先端のパフォーマンスを達成できることを示しています。
http://arxiv.org/abs/2007.11762v1,"All at Once: Temporally Adaptive Multi-Frame Interpolation with Advanced
  Motion Modeling",http://arxiv.org/pdf/2007.11762v1,"Recent advances in high refresh rate displays as well as the increasedinterest in high rate of slow motion and frame up-conversion fuel the demandfor efficient and cost-effective multi-frame video interpolation solutions. Tothat regard, inserting multiple frames between consecutive video frames are ofparamount importance for the consumer electronics industry. State-of-the-artmethods are iterative solutions interpolating one frame at the time. Theyintroduce temporal inconsistencies and clearly noticeable visual artifacts.  Departing from the state-of-the-art, this work introduces a true multi-frameinterpolator. It utilizes a pyramidal style network in the temporal domain tocomplete the multi-frame interpolation task in one-shot. A novel flowestimation procedure using a relaxed loss function, and an advanced,cubic-based, motion model is also used to further boost interpolation accuracywhen complex motion segments are encountered. Results on the Adobe240 datasetshow that the proposed method generates visually pleasing, temporallyconsistent frames, outperforms the current best off-the-shelf method by 1.57dbin PSNR with 8 times smaller model and 7.7 times faster. The proposed methodcan be easily extended to interpolate a large number of new frames whileremaining efficient because of the one-shot mechanism.",高リフレッシュレートディスプレイの最近の進歩と、スローモーションの高速化とフレームアップコンバージョンへの関心の高まりにより、効率的でコスト効率の高いマルチフレームビデオ補間ソリューションの需要が高まっています。それに関して、連続するビデオフレームの間に複数のフレームを挿入することは、家電業界にとって最も重要です。最先端の方法は、一度に1フレームを補間する反復ソリューションです。それらは時間的な不一致と明らかに目立つアーティファクトをもたらします。最先端技術から離れて、この作品は真のマルチフレーム補間器を紹介します。時間領域でピラミッド型ネットワークを利用して、マルチフレーム補間タスクをワンショットで完了します。緩和された損失関数と高度な立方体ベースのモーションモデルを使用した新しいフロー推定手順も使用され、複雑なモーションセグメントが検出された場合の補間精度がさらに向上します。 Adobe240データセットの結果は、提案された方法が視覚的に満足のいく、時間的に一貫したフレームを生成し、現在の最良の既製の方法よりもモデルが8倍小さく、7.7倍速い1.57dbin PSNRを上回っていることを示しています。提案された方法は、ワンショットメカニズムにより効率を維持しながら、多数の新しいフレームを補間するように簡単に拡張できます。
http://arxiv.org/abs/2007.11755v1,History Repeats Itself: Human Motion Prediction via Motion Attention,http://arxiv.org/pdf/2007.11755v1,"Human motion prediction aims to forecast future human poses given a pastmotion. Whether based on recurrent or feed-forward neural networks, existingmethods fail to model the observation that human motion tends to repeat itself,even for complex sports actions and cooking activities. Here, we introduce anattention-based feed-forward network that explicitly leverages thisobservation. In particular, instead of modeling frame-wise attention via posesimilarity, we propose to extract motion attention to capture the similaritybetween the current motion context and the historical motion sub-sequences.Aggregating the relevant past motions and processing the result with a graphconvolutional network allows us to effectively exploit motion patterns from thelong-term history to predict the future poses. Our experiments on Human3.6M,AMASS and 3DPW evidence the benefits of our approach for both periodical andnon-periodical actions. Thanks to our attention model, it yieldsstate-of-the-art results on all three datasets. Our code is available athttps://github.com/wei-mao-2019/HisRepItself.",人間の動きの予測は、過去の動きを考慮して将来の人間のポーズを予測することを目的としています。リカレントニューラルネットワークに基づくかフィードフォワードニューラルネットワークに基づくかにかかわらず、既存の方法では、複雑なスポーツアクションやクッキングアクティビティであっても、人間の動きが繰り返す傾向があるという観察をモデル化できません。ここでは、この観測を明示的に活用する注意ベースのフィードフォワードネットワークを紹介します。特に、ポーズ類似性を介してフレーム単位の注意をモデル化する代わりに、現在のモーションコンテキストと過去のモーションサブシーケンス間の類似性をキャプチャするためにモーション注意を抽出することを提案します。関連する過去のモーションを集約し、結果をグラフ畳み込みネットワークで処理することで、長期的な履歴からのモーションパターンを効果的に活用して、将来のポーズを予測します。 Human3.6M、AMASS、および3DPWに関する私たちの実験は、定期的および非定期的なアクションの両方に対する私たちのアプローチの利点を証明しています。アテンションモデルのおかげで、3つのデータセットすべてで最先端の結果が得られます。私たちのコードはhttps://github.com/wei-mao-2019/HisRepItselfで入手できます。
http://arxiv.org/abs/2007.11752v1,PareCO: Pareto-aware Channel Optimization for Slimmable Neural Networks,http://arxiv.org/pdf/2007.11752v1,"Slimmable neural networks provide a flexible trade-off front betweenprediction error and computational cost (such as the number of floating-pointoperations or FLOPs) with the same storage cost as a single model, have beenproposed recently for resource-constrained settings such as mobile devices.However, current slimmable neural networks use a single width-multiplier forall the layers to arrive at sub-networks with different performance profiles,which neglects that different layers affect the network's prediction accuracydifferently and have different FLOP requirements. Hence, developing aprincipled approach for deciding width-multipliers across different layerscould potentially improve the performance of slimmable networks. To allow forheterogeneous width-multipliers across different layers, we formulate theproblem of optimizing slimmable networks from a multi-objective optimizationlens, which leads to a novel algorithm for optimizing both the shared weightsand the width-multipliers for the sub-networks. We perform extensive empiricalanalysis with 14 network and dataset combinations and find that lessover-parameterized networks benefit more from a joint channel and weightoptimization than extremely over-parameterized networks. Quantitatively,improvements up to 1.7\% and 1\% in top-1 accuracy on the ImageNet dataset canbe attained for MobileNetV2 and MobileNetV3, respectively. Our resultshighlight the potential of optimizing the channel counts for different layersjointly with the weights and demonstrate the power of such techniques forslimmable networks.",柔軟なニューラルネットワークは、単一のモデルと同じストレージコストで、予測誤差と計算コスト（浮動小数点演算やFLOPの数など）の間の柔軟なトレードオフフロントを提供し、モバイルデバイスなどのリソースに制約のある設定に対して最近提案されました。ただし、現在のスリム化可能なニューラルネットワークでは、すべてのレイヤーに単一の幅乗数を使用して、さまざまなパフォーマンスプロファイルのサブネットワークに到達します。これにより、さまざまなレイヤーがネットワークの予測精度に異なる影響を及ぼし、FLOP要件が異なります。したがって、さまざまなレイヤーにまたがる幅乗数を決定するための原理的なアプローチを開発することで、スリム化可能なネットワークのパフォーマンスが向上する可能性があります。異なるレイヤー間で異種の幅乗数を可能にするために、多目的最適化レンズからスリム化可能なネットワークを最適化する問題を定式化します。これにより、サブネットワークの共有重みと幅乗数の両方を最適化する新しいアルゴリズムが生まれます。 14のネットワークとデータセットの組み合わせで広範な経験的分析を実行し、パラメーターが少ないネットワークは、パラメーターが非常に多いネットワークよりも、共同チャネルと重み最適化から多くの恩恵を受けることがわかりました。量的には、ImageNetデータセットのトップ1の精度で最大1.7 \％および1 \％の改善が、それぞれMobileNetV2およびMobileNetV3で達成できます。私たちの結果は、重みと一緒に異なる層のチャネル数を最適化する可能性を強調し、スリミング可能なネットワークのためのそのような技術の力を示しています。
http://arxiv.org/abs/2007.11744v1,End-to-End Optimization of Scene Layout,http://arxiv.org/pdf/2007.11744v1,"We propose an end-to-end variational generative model for scene layoutsynthesis conditioned on scene graphs. Unlike unconditional scene layoutgeneration, we use scene graphs as an abstract but general representation toguide the synthesis of diverse scene layouts that satisfy relationshipsincluded in the scene graph. This gives rise to more flexible control over thesynthesis process, allowing various forms of inputs such as scene layoutsextracted from sentences or inferred from a single color image. Using ourconditional layout synthesizer, we can generate various layouts that share thesame structure of the input example. In addition to this conditional generationdesign, we also integrate a differentiable rendering module that enables layoutrefinement using only 2D projections of the scene. Given a depth and asemantics map, the differentiable rendering module enables optimizing over thesynthesized layout to fit the given input in an analysis-by-synthesis fashion.Experiments suggest that our model achieves higher accuracy and diversity inconditional scene synthesis and allows exemplar-based scene generation fromvarious input forms.",シーングラフを条件とするシーンレイアウト合成のエンドツーエンドの変分生成モデルを提案します。無条件のシーンレイアウト生成とは異なり、シーングラフを抽象的なが一般的な表現として使用して、シーングラフに含まれる関係を満たす多様なシーンレイアウトの合成をガイドします。これにより、合成プロセスをより柔軟に制御できるようになり、センテンスから抽出された、または単一のカラー画像から推定されたシーンレイアウトなど、さまざまな形式の入力が可能になります。条件付きレイアウトシンセサイザーを使用して、入力例と同じ構造を共有するさまざまなレイアウトを生成できます。この条件付き生成設計に加えて、シーンの2D投影のみを使用してレイアウトの絞り込みを可能にする微分可能なレンダリングモジュールも統合します。深度と意味論のマップが与えられると、差別化可能なレンダリングモジュールは、合成されたレイアウトを最適化して、与えられた入力を分析ごとの方法でフィットさせることができます。さまざまな入力フォームから。
http://arxiv.org/abs/2007.11731v1,Comprehensive Image Captioning via Scene Graph Decomposition,http://arxiv.org/pdf/2007.11731v1,"We address the challenging problem of image captioning by revisiting therepresentation of image scene graph. At the core of our method lies thedecomposition of a scene graph into a set of sub-graphs, with each sub-graphcapturing a semantic component of the input image. We design a deep model toselect important sub-graphs, and to decode each selected sub-graph into asingle target sentence. By using sub-graphs, our model is able to attend todifferent components of the image. Our method thus accounts for accurate,diverse, grounded and controllable captioning at the same time. We presentextensive experiments to demonstrate the benefits of our comprehensivecaptioning model. Our method establishes new state-of-the-art results incaption diversity, grounding, and controllability, and compares favourably tolatest methods in caption quality. Our project website can be found athttp://pages.cs.wisc.edu/~yiwuzhong/Sub-GC.html.",画像シーングラフの再表示を再検討することにより、画像キャプションの困難な問題に対処します。私たちの方法の中心にあるのは、シーングラフを一連のサブグラフに分解し、各サブグラフが入力画像のセマンティックコンポーネントをキャプチャすることです。重要なサブグラフを選択し、選択した各サブグラフを単一のターゲット文にデコードするディープモデルを設計します。サブグラフを使用することで、モデルは画像のさまざまなコンポーネントに対応できます。したがって、私たちの方法は、正確で、多様で、根拠があり、制御可能なキャプションを同時に説明します。包括的なキャプションモデルの利点を実証するための広範な実験を紹介します。私たちの方法は、キャプションの多様性、グラウンディング、および制御性における新しい最先端の結果を確立し、キャプションの品質において最新の方法と比較して有利です。プロジェクトのウェブサイトはhttp://pages.cs.wisc.edu/~yiwuzhong/Sub-GC.htmlにあります。
http://arxiv.org/abs/2007.11726v1,"A weakly supervised registration-based framework for prostate
  segmentation via the combination of statistical shape model and CNN",http://arxiv.org/pdf/2007.11726v1,"Precise determination of target is an essential procedure in prostateinterventions, such as the prostate biopsy, lesion detection and targetedtherapy. However, the prostate delineation may be tough in some cases due totissue ambiguity or lack of partial anatomical boundary. To address thisproblem, we proposed a weakly supervised registration-based framework for theprecise prostate segmentation, by combining convolutional neural network (CNN)with statistical shape model (SSM). To obtain the prostate region, aninception-based neural network (SSM-Net) was firstly exploited to predict themodel transform, shape control parameters and a fine-tuning vector, for thegeneration of prostate boundary. According to the inferred boundary, anormalized distance map was calculated. Then, a residual U-net (ResU-Net) wasemployed to predict a probability label map from the input images. Finally, theaverage of the distance map and the probability map was regarded as theprostate segmentation. After that, two public dataset PROMISE12 and NCI- ISBI2013 were utilized for the model computation and for the network training andtesting. The validation results demonstrate that the segmentation frameworkusing a SSM with 9500 nodes achieved the best performance, with a dice of 0.904and an average surface distance of 1.88 mm. In addition, we verified the impactof model elasticity augmentation and fine-tuning item on the networksegmentation capability. As a result, both factors have improved thedelineation accuracy, with dice increased by 10% and 7% respectively. Inconclusion, via the combination of two weakly supervised neural networks, oursegmentation method might be an effective and robust approach for prostatesegmentation.",ターゲットの正確な決定は、前立腺生検、病変検出、標的療法などの前立腺インターベンションにおいて不可欠な手順です。ただし、組織のあいまいさや部分的な解剖学的境界の欠如により、場合によっては前立腺の描写は難しいことがあります。この問題に対処するため、畳み込みニューラルネットワーク（CNN）と統計的形状モデル（SSM）を組み合わせることにより、正確に前立腺を分割するための弱く監視された登録ベースのフレームワークを提案しました。前立腺領域を取得するために、最初に、開始ベースのニューラルネットワーク（SSM-Net）を利用して、モデル変換、形状制御パラメーター、および微調整ベクトルを予測し、前立腺境界を生成しました。推定された境界に従って、正規化された距離マップが計算されました。次に、残差Uネット（ResUネット）を使用して、入力画像から確率ラベルマップを予測しました。最後に、距離マップと確率マップの平均を前立腺のセグメンテーションと見なしました。その後、2つのパブリックデータセットPROMISE12とNCI- ISBI2013がモデルの計算とネットワークのトレーニングとテストに使用されました。検証結果は、9500ノードのSSMを使用したセグメンテーションフレームワークが、ダイス0.904、平均表面距離1.88 mmで最高のパフォーマンスを達成したことを示しています。さらに、ネットワークのセグメンテーション機能に対するモデルの弾力性の強化と微調整項目の影響を検証しました。その結果、両方の要素で描写精度が向上し、ダイスがそれぞれ10％と7％増加しました。結論として、2つの弱く監視されたニューラルネットワークの組み合わせを介して、私たちのセグメンテーションメソッドは、前立腺セグメンテーションのための効果的で堅牢なアプローチかもしれません。
http://arxiv.org/abs/2007.11709v1,"Threat of Adversarial Attacks on Face Recognition: A Comprehensive
  Survey",http://arxiv.org/pdf/2007.11709v1,"Face recognition (FR) systems have demonstrated outstanding verificationperformance, suggesting suitability for real-world applications, ranging fromphoto tagging in social media to automated border control (ABC). In an advancedFR system with deep learning-based architecture, however, promoting therecognition efficiency alone is not sufficient and the system should alsowithstand potential kinds of attacks designed to target its proficiency. Recentstudies show that (deep) FR systems exhibit an intriguing vulnerability toimperceptible or perceptible but natural-looking adversarial input images thatdrive the model to incorrect output predictions. In this article, we present acomprehensive survey on adversarial attacks against FR systems and elaborate onthe competence of new countermeasures against them. Further, we propose ataxonomy of existing attack and defense strategies according to differentcriteria. Finally, we compare the presented approaches according to techniques'characteristics.",顔認識（FR）システムは、ソーシャルメディアでの写真のタグ付けから自動境界制御（ABC）まで、実際のアプリケーションへの適合性を示唆する優れた検証パフォーマンスを実証しました。ただし、ディープラーニングベースのアーキテクチャを備えたadvancedFRシステムでは、認識効率を高めるだけでは不十分であり、システムは、その習熟度をターゲットにするように設計された潜在的な種類の攻撃にも耐える必要があります。最近の研究によると、（深い）FRシステムは、モデルを誤った出力予測に駆り立てる、知覚できないかまたは知覚できるが自然に見える敵の入力画像に興味深い脆弱性を示します。この記事では、FRシステムに対する敵対的な攻撃に関する包括的な調査を示し、それらに対する新しい対策の能力について詳しく説明します。さらに、異なる基準に従って既存の攻撃および防御戦略の分類法を提案します。最後に、技術の特性に従って、提示されたアプローチを比較します。
http://arxiv.org/abs/2007.11691v1,"End-to-End Trainable Deep Active Contour Models for Automated Image
  Segmentation: Delineating Buildings in Aerial Imagery",http://arxiv.org/pdf/2007.11691v1,"The automated segmentation of buildings in remote sensing imagery is achallenging task that requires the accurate delineation of multiple buildinginstances over typically large image areas. Manual methods are often laboriousand current deep-learning-based approaches fail to delineate all buildinginstances and do so with adequate accuracy. As a solution, we present TrainableDeep Active Contours (TDACs), an automatic image segmentation framework thatintimately unites Convolutional Neural Networks (CNNs) and Active ContourModels (ACMs). The Eulerian energy functional of the ACM component includesper-pixel parameter maps that are predicted by the backbone CNN, which alsoinitializes the ACM. Importantly, both the ACM and CNN components are fullyimplemented in TensorFlow and the entire TDAC architecture is end-to-endautomatically differentiable and backpropagation trainable without userintervention. TDAC yields fast, accurate, and fully automatic simultaneousdelineation of arbitrarily many buildings in the image. We validate the modelon two publicly available aerial image datasets for building segmentation, andour results demonstrate that TDAC establishes a new state-of-the-artperformance.",リモートセンシング画像での建物の自動セグメンテーションは、通常は大きな画像領域にわたる複数の建物インスタンスの正確な描写を必要とする困難なタスクです。手動による方法は手間がかかることが多く、現在のディープラーニングベースのアプローチでは、すべてのビルディングインスタンスの輪郭を描くことができず、適切な精度で描き出せません。解決策として、畳み込みニューラルネットワーク（CNN）とアクティブコンターモデル（ACM）を密接に統合する自動画像セグメンテーションフレームワークである、TrainableDeepアクティブコンター（TDAC）を紹介します。 ACMコンポーネントのオイラーエネルギー汎関数には、バックボーンCNNによって予測されるピクセルごとのパラメーターマップが含まれ、これもACMを初期化します。重要なのは、ACMコンポーネントとCNNコンポーネントの両方がTensorFlowに完全に実装されており、TDACアーキテクチャ全体がエンドツーエンドで自動的に区別可能で、ユーザーの介入なしにバックプロパゲーションのトレーニングが可能であることです。 TDACは、画像内の任意の数の建物を、高速、正確、かつ完全に自動で同時に描写します。セグメンテーションを構築するための2つの公開空中画像データセットのモデルを検証し、TDACが新しい最先端のパフォーマンスを確立していることを示しています。
http://arxiv.org/abs/2007.11690v1,Integrating Image Captioning with Rule-based Entity Masking,http://arxiv.org/pdf/2007.11690v1,"Given an image, generating its natural language description (i.e., caption)is a well studied problem. Approaches proposed to address this problem usuallyrely on image features that are difficult to interpret. Particularly, theseimage features are subdivided into global and local features, where globalfeatures are extracted from the global representation of the image, while localfeatures are extracted from the objects detected locally in an image. Although,local features extract rich visual information from the image, existing modelsgenerate captions in a blackbox manner and humans have difficulty interpretingwhich local objects the caption is aimed to represent. Hence in this paper, wepropose a novel framework for the image captioning with an explicit object(e.g., knowledge graph entity) selection process while still maintaining itsend-to-end training ability. The model first explicitly selects which localentities to include in the caption according to a human-interpretable mask,then generate proper captions by attending to selected entities. Experimentsconducted on the MSCOCO dataset demonstrate that our method achieves goodperformance in terms of the caption quality and diversity with a moreinterpretable generating process than previous counterparts.",画像を考えると、その自然言語の説明（つまり、キャプション）を生成することはよく研究された問題です。この問題に対処するために提案されたアプローチは、通常、解釈が難しい画像の特徴に依存しています。特に、これらの画像特徴は、グローバル特徴とローカル特徴に細分され、グローバル特徴は画像のグローバル表現から抽出され、ローカル特徴は画像でローカルに検出されたオブジェクトから抽出されます。ローカル機能は画像から豊富な視覚情報を抽出しますが、既存のモデルはブラックボックスの方法でキャプションを生成し、人間はキャプションが表すことを目的とするローカルオブジェクトを解釈することが困難です。したがって、このペーパーでは、エンドツーエンドのトレーニング能力を維持しながら、明示的なオブジェクト（ナレッジグラフエンティティなど）の選択プロセスを使用した画像キャプションの新しいフレームワークを提案します。モデルは最初に、人間が解釈できるマスクに従ってキャプションに含めるローカリティを明示的に選択し、次に、選択されたエンティティに注意を払って適切なキャプションを生成します。 MSCOCOデータセットで実施された実験は、我々の方法が、以前の対応物よりも解釈可能な生成プロセスにより、キャプションの品質と多様性の点で優れたパフォーマンスを達成することを示しています。
http://arxiv.org/abs/2007.11689v1,Multi-modality imaging with structure-promoting regularisers,http://arxiv.org/pdf/2007.11689v1,"Imaging with multiple modalities or multiple channels is becomingincreasingly important for our modern society. A key tool for understanding andearly diagnosis of cancer and dementia is PET-MR, a combined positron emissiontomography and magnetic resonance imaging scanner which can simultaneouslyacquire functional and anatomical data. Similarly in remote sensing, whilehyperspectral sensors may allow to characterise and distinguish materials,digital cameras offer high spatial resolution to delineate objects. In both ofthese examples, the imaging modalities can be considered individually orjointly. In this chapter we discuss mathematical approaches which allow tocombine information from several imaging modalities so that multi-modalityimaging can be more than just the sum of its components.",複数のモダリティまたは複数のチャネルを使用したイメージングは​​、現代社会にとってますます重要になっています。癌と認知症の早期診断を理解するための重要なツールは、機能と解剖学的データを同時に取得できる陽電子放射断層撮影と磁気共鳴画像スキャナーの組み合わせであるPET-MRです。同様に、リモートセンシングでは、ハイパースペクトルセンサーが材料の特性を明らかにして区別できる場合がありますが、デジタルカメラはオブジェクトの輪郭を描くために高い空間解像度を提供します。これらの例の両方で、画像モダリティは個別にまたは共同で検討できます。この章では、複数のモダリティイメージングがそのコンポーネントの合計以上になるように、いくつかのイメージングモダリティからの情報を組み合わせることができる数学的なアプローチについて説明します。
http://arxiv.org/abs/2007.11679v1,Cloud Transformers,http://arxiv.org/pdf/2007.11679v1,"We present a new versatile building block for deep point cloud processingarchitectures. This building block combines the ideas of self-attention layersfrom the transformer architecture with the efficiency of standard convolutionallayers in two and three-dimensional dense grids. The new block operates viamultiple parallel heads, whereas each head projects feature representations ofindividual points into a low-dimensional space, treats the first two or threedimensions as spatial coordinates and then uses dense convolution to propagateinformation across points. The results of the processing of individual headsare then combined together resulting in the update of point features. Using thenew block, we build architectures for point cloud segmentation as well as forimage-based point cloud reconstruction. We show that despite the dissimilaritybetween these tasks, the resulting architectures achieve state-of-the-artperformance for both of them demonstrating the versatility of the new block.",ディープポイントクラウド処理アーキテクチャ用の新しい用途の広いビルディングブロックを紹介します。このビルディングブロックは、トランスアーキテクチャの自己注意レイヤーのアイデアと、2次元および3次元の高密度グリッドにおける標準の畳み込みレイヤーの効率を組み合わせています。新しいブロックは複数の並列ヘッドを介して動作しますが、各ヘッドは個々のポイントのフィーチャ表現を低次元空間に投影し、最初の2次元または3次元を空間座標として扱い、密な畳み込みを使用してポイント間で情報を伝達します。次に、個々のヘッドの処理結果が結合され、ポイントフィーチャが更新されます。新しいブロックを使用して、点群セグメンテーションおよび画像ベースの点群再構成のためのアーキテクチャを構築します。これらのタスクの相違点にも関わらず、結果として得られるアーキテクチャーは両方とも最先端のパフォーマンスを実現し、新しいブロックの多様性を実証します。
http://arxiv.org/abs/2007.11678v1,Contact and Human Dynamics from Monocular Video,http://arxiv.org/pdf/2007.11678v1,"Existing deep models predict 2D and 3D kinematic poses from video that areapproximately accurate, but contain visible errors that violate physicalconstraints, such as feet penetrating the ground and bodies leaning at extremeangles. In this paper, we present a physics-based method for inferring 3D humanmotion from video sequences that takes initial 2D and 3D pose estimates asinput. We first estimate ground contact timings with a novel prediction networkwhich is trained without hand-labeled data. A physics-based trajectoryoptimization then solves for a physically-plausible motion, based on theinputs. We show this process produces motions that are significantly morerealistic than those from purely kinematic methods, substantially improvingquantitative measures of both kinematic and dynamic plausibility. Wedemonstrate our method on character animation and pose estimation tasks ondynamic motions of dancing and sports with complex contact patterns.",既存のディープモデルは、ビデオから2Dおよび3Dの運動学的ポーズをほぼ正確に予測しますが、地面を貫通する足や極端な角度で傾いたボディなどの物理的制約に違反する目に見えるエラーを含みます。このペーパーでは、入力として初期2Dおよび3Dポーズ推定値を使用するビデオシーケンスから3D人間の動きを推測するための物理学ベースの方法を紹介します。最初に、手でラベル付けされたデータなしでトレーニングされた新しい予測ネットワークを使用して、接地のタイミングを推定します。物理学ベースの軌道最適化は、入力に基づいて、物理的に妥当なモーションを解きます。このプロセスは、純粋な運動学的方法からの動きよりもはるかに現実的な動きを生成し、運動学的および動的な妥当性の両方の定量的測定を大幅に改善することを示します。複雑な接触パターンを持つダンスやスポーツのダイナミックモーションに関するキャラクターアニメーションとポーズ推定タスクに関する手法を実証します。
http://arxiv.org/abs/2007.11668v1,Analogical Reasoning for Visually Grounded Language Acquisition,http://arxiv.org/pdf/2007.11668v1,"Children acquire language subconsciously by observing the surrounding worldand listening to descriptions. They can discover the meaning of words evenwithout explicit language knowledge, and generalize to novel compositionseffortlessly. In this paper, we bring this ability to AI, by studying the taskof Visually grounded Language Acquisition (VLA). We propose a multimodaltransformer model augmented with a novel mechanism for analogical reasoning,which approximates novel compositions by learning semantic mapping andreasoning operations from previously seen compositions. Our proposed method,Analogical Reasoning Transformer Networks (ARTNet), is trained on rawmultimedia data (video frames and transcripts), and after observing a set ofcompositions such as ""washing apple"" or ""cutting carrot"", it can generalize andrecognize new compositions in new video frames, such as ""washing carrot"" or""cutting apple"". To this end, ARTNet refers to relevant instances in thetraining data and uses their visual features and captions to establishanalogies with the query image. Then it chooses the suitable verb and noun tocreate a new composition that describes the new image best. Extensiveexperiments on an instructional video dataset demonstrate that the proposedmethod achieves significantly better generalization capability and recognitionaccuracy compared to state-of-the-art transformer models.",子どもたちは、周囲の世界を観察し、説明を聞くことで無意識のうちに言語を習得します。彼らは明確な言語知識がなくても単語の意味を発見し、簡単に新しい構成に一般化することができます。このホワイトペーパーでは、視覚に基づいた言語習得（VLA）のタスクを研究することにより、この能力をAIにもたらします。以前に見られた構成から意味マッピングと推論操作を学習することにより、新規構成を近似する類推の新規メカニズムで強化されたマルチモーダル変換モデルを提案します。私たちが提案する方法、Analogical Reasoning Transformer Networks（ARTNet）は、rawmultimediaデータ（ビデオフレームとトランスクリプト）でトレーニングされ、「リンゴを洗う」または「ニンジンを切る」などの一連の構成を観察した後、新しい構成を一般化して新しいものに認識できます。 「ニンジンを洗う」または「リンゴを切る」などのビデオフレーム。このため、ARTNetはトレーニングデータの関連インスタンスを参照し、それらの視覚的機能とキャプションを使用して、クエリ画像との類似性を確立します。次に、適切な動詞と名詞を選択して、新しいイメージを最もよく表す新しい構成を作成します。教育ビデオデータセットの広範な実験は、提案された方法が最新のトランスフォーマーモデルと比較して大幅に優れた汎化能力と認識精度を達成することを示しています。
http://arxiv.org/abs/2007.11653v1,"Darwin's Neural Network: AI-based Strategies for Rapid and Scalable Cell
  and Coronavirus Screening",http://arxiv.org/pdf/2007.11653v1,"Recent advances in the interdisciplinary scientific field of machineperception, computer vision, and biomedical engineering underpin a collectionof machine learning algorithms with a remarkable ability to decipher thecontents of microscope and nanoscope images. Machine learning algorithms aretransforming the interpretation and analysis of microscope and nanoscopeimaging data through use in conjunction with biological imaging modalities.These advances are enabling researchers to carry out real-time experiments thatwere previously thought to be computationally impossible. Here we adapt thetheory of survival of the fittest in the field of computer vision and machineperception to introduce a new framework of multi-class instance segmentationdeep learning, Darwin's Neural Network (DNN), to carry out morphometricanalysis and classification of COVID19 and MERS-CoV collected in vivo and ofmultiple mammalian cell types in vitro.",機械知覚、コンピュータービジョン、および生物医学工学の学際的な科学分野における最近の進歩は、顕微鏡およびナノスコープの画像の内容を解読する驚くべき能力を持つ機械学習アルゴリズムのコレクションを支えています。機械学習アルゴリズムは、生物学的画像モダリティと組み合わせて使用​​することにより、顕微鏡およびナノスコープ画像データの解釈と分析を変換します。これらの進歩により、研究者は、以前は計算不可能と考えられていたリアルタイム実験を実行できるようになりました。ここでは、コンピュータービジョンとマシン知覚の分野で適者生存の理論を採用して、マルチクラスインスタンスセグメンテーションディープラーニングの新しいフレームワークであるダーウィンのニューラルネットワーク（DNN）を導入し、形態計測分析とCOVID19およびMERS-CoV収集の分類を実行しますin vivoおよびin vitroで複数の哺乳類細胞型。
http://arxiv.org/abs/2007.11641v1,"Attention based Multiple Instance Learning for Classification of Blood
  Cell Disorders",http://arxiv.org/pdf/2007.11641v1,"Red blood cells are highly deformable and present in various shapes. In bloodcell disorders, only a subset of all cells is morphologically altered andrelevant for the diagnosis. However, manually labeling of all cells islaborious, complicated and introduces inter-expert variability. We propose anattention based multiple instance learning method to classify blood samples ofpatients suffering from blood cell disorders. Cells are detected using an R-CNNarchitecture. With the features extracted for each cell, a multiple instancelearning method classifies patient samples into one out of four blood celldisorders. The attention mechanism provides a measure of the contribution ofeach cell to the overall classification and significantly improves thenetwork's classification accuracy as well as its interpretability for themedical expert.",赤血球は非常に変形しやすく、さまざまな形で存在します。血球障害では、すべての細胞のサブセットのみが形態学的に変化し、診断に関連しています。ただし、すべてのセルを手動でラベル付けするのは面倒で複雑であり、専門家間のばらつきが生じます。血球障害を患っている患者の血液サンプルを分類するために注意ベースの複数インスタンス学習法を提案します。細胞は、R-CNNarchitectureを使用して検出されます。細胞ごとに抽出された特徴を使用して、複数インスタンス学習メソッドは、患者のサンプルを4つの血球障害のうち1つに分類します。アテンションメカニズムは、各セルの分類全体への寄与の測定を提供し、ネットワークの分類精度と医療専門家のためのその解釈可能性を大幅に改善します。
http://arxiv.org/abs/2007.11634v1,Subjective and Objective Quality Assessment of High Frame Rate Videos,http://arxiv.org/pdf/2007.11634v1,"High frame rate (HFR) videos are becoming increasingly common with thetremendous popularity of live, high-action streaming content such as sports.Although HFR contents are generally of very high quality, high bandwidthrequirements make them challenging to deliver efficiently, while simultaneouslymaintaining their quality. To optimize trade-offs between bandwidthrequirements and video quality, in terms of frame rate adaptation, it isimperative to understand the intricate relationship between frame rate andperceptual video quality. Towards advancing progression in this direction wedesigned a new subjective resource, called the LIVE-YouTube-HFR (LIVE-YT-HFR)dataset, which is comprised of 480 videos having 6 different frame rates,obtained from 16 diverse contents. In order to understand the combined effectsof compression and frame rate adjustment, we also processed videos at 5compression levels at each frame rate. To obtain subjective labels on thevideos, we conducted a human study yielding 19,000 human quality ratingsobtained from a pool of 85 human subjects. We also conducted a holisticevaluation of existing state-of-the-art Full and No-Reference video qualityalgorithms, and statistically benchmarked their performance on the newdatabase. The LIVE-YT-HFR database has been made available online for publicuse and evaluation purposes, with hopes that it will help advance research inthis exciting video technology direction. It may be obtained at\url{https://live.ece.utexas.edu/research/LIVE_YT_HFR/LIVE_YT_HFR/index.html}","高フレームレート（HFR）ビデオは、スポーツなどのライブでアクションの多いストリーミングコンテンツの絶大な人気により、ますます一般的になっています。HFRコンテンツは一般に非常に高品質ですが、高帯域幅要件により、効率的に配信すると同時に、品質を維持することが難しくなっています。帯域幅要件とビデオ品質の間のトレードオフを最適化するには、フレームレート適応の観点から、フレームレートと知覚ビデオ品質の複雑な関係を理解することが不可欠です。この方向に進歩を進めるために、LIVE-YouTube-HFR（LIVE-YT-HFR）データセットと呼ばれる新しい主観的なリソースを設計しました。これは、16の異なるコンテンツから取得した6つの異なるフレームレートを持つ480本のビデオで構成されています。圧縮とフレームレート調整の組み合わせ効果を理解するために、各フレームレートで5つの圧縮レベルでビデオを処理しました。ビデオの主観的なラベルを取得するために、85人の被験者のプールから得られた19,000人の人間の品質評価をもたらす人間の研究を実施しました。また、既存の最先端の完全および非参照ビデオ品質アルゴリズムの総合評価を実施し、新しいデータベースでのパフォーマンスを統計的にベンチマークしました。 LIVE-YT-HFRデータベースは、このエキサイティングなビデオテクノロジーの方向性に関する研究の進歩に役立つことを期待して、パブリックユースおよび評価目的でオンラインで利用可能になっています。 \ url {https://live.ece.utexas.edu/research/LIVE_YT_HFR/LIVE_YT_HFR/index.html}で入手できます。"
http://arxiv.org/abs/2007.11622v1,Tiny Transfer Learning: Towards Memory-Efficient On-Device Learning,http://arxiv.org/pdf/2007.11622v1,"We present Tiny-Transfer-Learning (TinyTL), an efficient on-device learningmethod to adapt pre-trained models to newly collected data on edge devices.Different from conventional transfer learning methods that fine-tune the fullnetwork or the last layer, TinyTL freezes the weights of the feature extractorwhile only learning the biases, thus doesn't require storing the intermediateactivations, which is the major memory bottleneck for on-device learning. Tomaintain the adaptation capacity without updating the weights, TinyTLintroduces memory-efficient lite residual modules to refine the featureextractor by learning small residual feature maps in the middle. Besides,instead of using the same feature extractor, TinyTL adapts the architecture ofthe feature extractor to fit different target datasets while fixing theweights: TinyTL pre-trains a large super-net that contains many weight-sharedsub-nets that can individually operate; different target dataset selects thesub-net that best match the dataset. This backpropagation-free discrete sub-netselection incurs no memory overhead. Extensive experiments show that TinyTL canreduce the training memory cost by order of magnitude (up to 13.3x) withoutsacrificing accuracy compared to fine-tuning the full network.",Tiny-Transfer-Learning（TinyTL）を紹介します。これは、事前にトレーニングされたモデルをエッジデバイスで新しく収集されたデータに適応させるための効率的なオンデバイス学習方法です。ネットワーク全体または最後のレイヤーを微調整する従来の転送学習方法とは異なり、TinyTLはフリーズしますバイアスのみを学習している間、特徴抽出の重みは中間活性化を保存する必要がありません。これは、デバイス上の学習の主要なメモリボトルネックです。重みを更新せずに適応能力を維持するために、TinyTLはメモリ効率の良いlite残差モジュールを導入して、途中で小さな残差特徴マップを学習することにより、featureextractorを改良します。さらに、同じ特徴抽出器を使用する代わりに、TinyTLは特徴抽出器のアーキテクチャを適合させて、重みを修正しながら異なるターゲットデータセットに適合させます。TinyTLは、個別に動作できる多くの重み共有サブネットを含む大きなスーパーネットを事前トレーニングします。別のターゲットデータセットは、データセットに最も一致するサブネットを選択します。この逆伝播のない個別のサブネット選択では、メモリのオーバーヘッドは発生しません。広範な実験により、TinyTLは、ネットワーク全体を微調整する場合と比較して、精度を犠牲にすることなく、トレーニングメモリのコストを1桁（最大13.3倍）削減できることが示されています。
http://arxiv.org/abs/2007.11610v1,"SIZER: A Dataset and Model for Parsing 3D Clothing and Learning Size
  Sensitive 3D Clothing",http://arxiv.org/pdf/2007.11610v1,"While models of 3D clothing learned from real data exist, no method canpredict clothing deformation as a function of garment size. In this paper, weintroduce SizerNet to predict 3D clothing conditioned on human body shape andgarment size parameters, and ParserNet to infer garment meshes and shape underclothing with personal details in a single pass from an input mesh. SizerNetallows to estimate and visualize the dressing effect of a garment in varioussizes, and ParserNet allows to edit clothing of an input mesh directly,removing the need for scan segmentation, which is a challenging problem initself. To learn these models, we introduce the SIZER dataset of clothing sizevariation which includes $100$ different subjects wearing casual clothing itemsin various sizes, totaling to approximately 2000 scans. This dataset includesthe scans, registrations to the SMPL model, scans segmented in clothing parts,garment category and size labels. Our experiments show better parsing accuracyand size prediction than baseline methods trained on SIZER. The code, model anddataset will be released for research purposes.",実際のデータから学習した3D服のモデルは存在しますが、服のサイズの関数として服の変形を予測する方法はありません。この論文では、SizerNetを導入して、人体の形状と衣服のサイズパラメータに基づいて条件付けされた3Dの衣服を予測し、ParserNetを使用して、衣服のメッシュと個人の詳細を含む下着を入力メッシュからの単一パスで推測します。 SizerNetは、さまざまなサイズの衣服のドレッシング効果を推定および視覚化でき、ParserNetは、入力メッシュの衣服を直接編集して、スキャンのセグメンテーションの必要性を取り除き、それ自体が難しい問題です。これらのモデルを学習するために、さまざまなサイズのカジュアルな衣料品を着用し、合計で約2000スキャンの$ 100 $の異なる被験者を含む、衣服のサイズ変動のSIZERデータセットを紹介します。このデータセットには、スキャン、SMPLモデルへの登録、衣服のパーツ、衣服のカテゴリ、サイズラベルに分割されたスキャンが含まれます。私たちの実験は、SIZERでトレーニングされたベースラインメソッドよりも優れた解析精度とサイズ予測を示しています。コード、モデル、データセットは研究目的でリリースされます。
http://arxiv.org/abs/2007.11576v1,Deep Variational Instance Segmentation,http://arxiv.org/pdf/2007.11576v1,"Instance Segmentation, which seeks to obtain both class and instance labelsfor each pixel in the input image, is a challenging task in computer vision.State-of-the-art algorithms often employ two separate stages, the first onegenerating object proposals and the second one recognizing and refining theboundaries. Further, proposals are usually based on detectors such as fasterR-CNN which search for boxes in the entire image exhaustively. In this paper,we propose a novel algorithm that directly utilizes a fully convolutionalnetwork (FCN) to predict instance labels. Specifically, we propose avariational relaxation of instance segmentation as minimizing an optimizationfunctional for a piecewise-constant segmentation problem, which can be used totrain an FCN end-to-end. It extends the classical Mumford-Shah variationalsegmentation problem to be able to handle permutation-invariant labels in theground truth of instance segmentation. Experiments on PASCAL VOC 2012, SemanticBoundaries dataset(SBD), and the MSCOCO 2017 dataset show that the proposedapproach efficiently tackle the instance segmentation task. The source code andtrained models will be released with the paper.",入力画像の各ピクセルのクラスラベルとインスタンスラベルの両方を取得しようとするインスタンスセグメンテーションは、コンピュータービジョンでは困難な作業です。最新のアルゴリズムでは、多くの場合、2つの別々のステージを使用します。境界を認識し、洗練する。さらに、提案は通常、画像全体のボックスを網羅的に検索するfasterR-CNNなどの検出器に基づいています。この論文では、完全な畳み込みネットワーク（FCN）を直接利用してインスタンスラベルを予測する新しいアルゴリズムを提案します。具体的には、FCNのエンドツーエンドのトレーニングに使用できる区分的に一定のセグメンテーション問題の最適化関数を最小化することとして、インスタンスセグメンテーションの変動緩和を提案します。これは、インスタンスのセグメンテーションのグラウンドトゥルースで順列不変のラベルを処理できるように、古典的なマンフォードシャーバリエーションセグメンテーション問題を拡張します。 PASCAL VOC 2012、SemanticBoundariesデータセット（SBD）、およびMSCOCO 2017データセットに関する実験は、提案されたアプローチがインスタンスのセグメンテーションタスクに効率的に取り組むことを示しています。ソースコードとトレーニング済みモデルは、ペーパーとともにリリースされます。
http://arxiv.org/abs/2007.11571v1,Neural Sparse Voxel Fields,http://arxiv.org/pdf/2007.11571v1,"Photo-realistic free-viewpoint rendering of real-world scenes using classicalcomputer graphics techniques is challenging, because it requires the difficultstep of capturing detailed appearance and geometry models. Recent studies havedemonstrated promising results by learning scene representations thatimplicitly encode both geometry and appearance without 3D supervision. However,existing approaches in practice often show blurry renderings caused by thelimited network capacity or the difficulty in finding accurate intersections ofcamera rays with the scene geometry. Synthesizing high-resolution imagery fromthese representations often requires time-consuming optical ray marching. Inthis work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scenerepresentation for fast and high-quality free-viewpoint rendering. NSVF definesa set of voxel-bounded implicit fields organized in a sparse voxel octree tomodel local properties in each cell. We progressively learn the underlyingvoxel structures with a diffentiable ray-marching operation from only a set ofposed RGB images. With the sparse voxel octree structure, rendering novel viewscan be accelerated by skipping the voxels containing no relevant scene content.Our method is over 10 times faster than the state-of-the-art (namely, NeRF) atinference time while achieving higher quality results. Furthermore, byutilizing an explicit sparse voxel representation, our method can easily beapplied to scene editing and scene composition. We also demonstrate severalchallenging tasks, including multi-scene learning, free-viewpoint rendering ofa moving human, and large-scale scene rendering.",古典的なコンピューターグラフィックステクニックを使用した現実世界のシーンの写実的な自由視点レンダリングは、詳細な外観とジオメトリモデルをキャプチャするという難しいステップを必要とするため、困難です。最近の研究では、3Dの監視なしでジオメトリと外観の両方を暗黙的にエンコードするシーン表現を学習することにより、有望な結果が示されています。ただし、実際の既存のアプローチでは、ネットワーク容量が限られていることや、カメラレイとシーンジオメトリの正確な交差点を見つけることが難しいことが原因でレンダリングがぼやけることがよくあります。これらの表現から高解像度画像を合成するには、多くの場合、時間のかかる光線の行進が必要です。この作品では、高速かつ高品質の自由視点レンダリング用の新しい神経シーン表現であるニューラルスパースボクセルフィールド（NSVF）を紹介します。 NSVFは、スパースボクセル八分木に編成されたボクセル境界の暗黙フィールドのセットを定義して、各セルのローカルプロパティをモデル化します。私たちは、配置されたRGB画像のセットのみから、微分可能な光線マーチング操作で基礎となるボクセル構造を段階的に学習します。スパースボクセルオクツリー構造では、関連するシーンコンテンツを含まないボクセルをスキップすることで、新しいビュースキャンのレンダリングを高速化できます。この手法は、最新の（つまり、NeRF）推論時間よりも10倍以上高速でありながら、より高い品質の結果を実現します。 。さらに、明示的なスパースボクセル表現を利用することで、この方法をシーン編集やシーン構成に簡単に適用できます。また、マルチシーン学習、動いている人間の自由視点レンダリング、大規模なシーンレンダリングなど、いくつかの困難なタスクを示します。
http://arxiv.org/abs/2007.11548v1,Attend and Segment: Attention Guided Active Semantic Segmentation,http://arxiv.org/pdf/2007.11548v1,"In a dynamic environment, an agent with a limited field of view/resourcecannot fully observe the scene before attempting to parse it. The deployment ofcommon semantic segmentation architectures is not feasible in such settings. Inthis paper we propose a method to gradually segment a scene given a sequence ofpartial observations. The main idea is to refine an agent's understanding ofthe environment by attending the areas it is most uncertain about. Our methodincludes a self-supervised attention mechanism and a specialized architectureto maintain and exploit spatial memory maps for filling-in the unseen areas inthe environment. The agent can select and attend an area while relying on thecues coming from the visited areas to hallucinate the other parts. We reach amean pixel-wise accuracy of 78.1%, 80.9% and 76.5% on CityScapes, CamVid, andKitti datasets by processing only 18% of the image pixels (10 retina-likeglimpses). We perform an ablation study on the number of glimpses, input imagesize and effectiveness of retina-like glimpses. We compare our method toseveral baselines and show that the optimal results are achieved by havingaccess to a very low resolution view of the scene at the first timestep.",動的環境では、限られた視野/リソースを持つエージェントは、解析を試みる前にシーンを完全に観察することはできません。一般的なセマンティックセグメンテーションアーキテクチャの展開は、このような設定では実現できません。この論文では、一連の部分的な観察を前提として、シーンを段階的に分割する方法を提案します。主なアイデアは、最も不確かな領域に参加することで、エージェントの環境に対する理解を深めることです。私たちの方法には、環境の目に見えない領域を埋めるための空間メモリマップを維持および活用するための、自己監視された注意メカニズムと特殊なアーキテクチャが含まれています。エージェントは他の部分を幻覚させるために訪問されたエリアから来るキューに依存しながら、エリアを選択して参加することができます。 CityScapes、CamVid、Kittiデータセットでは、画像ピクセルの18％のみ（10網膜のような視線）を処理することで、平均的なピクセル単位の精度78.1％、80.9％、76.5％に達しています。垣間見る数、入力画像サイズ、網膜のような垣間見る効果に関するアブレーション研究を行います。この方法をいくつかのベースラインと比較し、最初のタイムステップでシーンの非常に低解像度のビューにアクセスすることで、最適な結果が得られることを示します。
http://arxiv.org/abs/2007.11514v1,"Endo-Sim2Real: Consistency learning-based domain adaptation for
  instrument segmentation",http://arxiv.org/pdf/2007.11514v1,"Surgical tool segmentation in endoscopic videos is an important component ofcomputer assisted interventions systems. Recent success of image-basedsolutions using fully-supervised deep learning approaches can be attributed tothe collection of big labeled datasets. However, the annotation of a bigdataset of real videos can be prohibitively expensive and time consuming.Computer simulations could alleviate the manual labeling problem, however,models trained on simulated data do not generalize to real data. This workproposes a consistency-based framework for joint learning of simulated and real(unlabeled) endoscopic data to bridge this performance generalization issue.Empirical results on two data sets (15 videos of the Cholec80 and EndoVis'15dataset) highlight the effectiveness of the proposed \emph{Endo-Sim2Real}method for instrument segmentation. We compare the segmentation of the proposedapproach with state-of-the-art solutions and show that our method improvessegmentation both in terms of quality and quantity.",内視鏡ビデオでの手術器具のセグメンテーションは、コンピュータ支援介入システムの重要なコンポーネントです。完全に監視されたディープラーニングアプローチを使用した画像ベースのソリューションの最近の成功は、大きなラベル付きデータセットのコレクションに起因する可能性があります。ただし、実際のビデオのビッグデータセットの注釈は、法外に高価で時間がかかる可能性があります。コンピューターのシミュレーションは、手動のラベル付けの問題を軽減できますが、シミュレーションデータでトレーニングされたモデルは、実際のデータに一般化されません。この作業は、シミュレーションされた実数（ラベル付けされていない）内視鏡データの共同学習のための一貫性ベースのフレームワークを提案し、このパフォーマンスの一般化の問題を解決します.2つのデータセット（Cholec80とEndoVisの15データセットの15ビデオ）の実証結果は、提案された\器具のセグメンテーションのためのemph {Endo-Sim2Real}メソッド。私たちは提案されたアプローチのセグメンテーションを最先端のソリューションと比較し、私たちの方法が質と量の両方の面でセグメンテーションを改善することを示します。
http://arxiv.org/abs/2007.11498v2,CrossTransformers: spatially-aware few-shot transfer,http://arxiv.org/pdf/2007.11498v2,"Given new tasks with very little data--such as new classes in aclassification problem or a domain shift in the input--performance of modernvision systems degrades remarkably quickly. In this work, we illustrate how theneural network representations which underpin modern vision systems are subjectto supervision collapse, whereby they lose any information that is notnecessary for performing the training task, including information that may benecessary for transfer to new tasks or domains. We then propose two methods tomitigate this problem. First, we employ self-supervised learning to encouragegeneral-purpose features that transfer better. Second, we propose a novelTransformer based neural network architecture called CrossTransformers, whichcan take a small number of labeled images and an unlabeled query, find coarsespatial correspondence between the query and the labeled images, and then inferclass membership by computing distances between spatially-correspondingfeatures. The result is a classifier that is more robust to task and domainshift, which we demonstrate via state-of-the-art performance on Meta-Dataset, arecent dataset for evaluating transfer from ImageNet to many other visiondatasets.",分類問題の新しいクラスや入力のドメインシフトなど、データがほとんどない新しいタスクを考えると、modernvisionシステムのパフォーマンスは著しく急速に低下します。この作業では、新しいビジョンやドメインへの転送に必要な情報を含め、トレーニングタスクの実行に不要な情報が失われるため、現代のビジョンシステムを支えるニューラルネットワーク表現がどのように監督されなくなるかを示します。次に、この問題を軽減する2つの方法を提案します。まず、自己管理学習を採用して、より優れた転送を行う汎用目的の機能を奨励します。次に、CrossTransformersと呼ばれるノベルトランスフォーマーベースのニューラルネットワークアーキテクチャを提案します。これは、少数のラベル付き画像とラベルなしクエリを取得し、クエリとラベル付き画像の間の粗空間対応を見つけ、空間的に対応する機能間の距離を計算することによってクラスメンバーシップを推定します。結果は、タスクとドメインシフトに対してより堅牢な分類子です。これは、メタデータセット、ImageNetから他の多くのビジョンデータセットへの転送を評価するための最新のデータセットで最先端のパフォーマンスを介して示しています。
http://arxiv.org/abs/2007.11469v1,"Deep Models and Shortwave Infrared Information to Detect Face
  Presentation Attacks",http://arxiv.org/pdf/2007.11469v1,"This paper addresses the problem of face presentation attack detection usingdifferent image modalities. In particular, the usage of short wave infrared(SWIR) imaging is considered. Face presentation attack detection is performedusing recent models based on Convolutional Neural Networks using only carefullyselected SWIR image differences as input. Conducted experiments show superiorperformance over similar models acting on either color images or on acombination of different modalities (visible, NIR, thermal and depth), as wellas on a SVM-based classifier acting on SWIR image differences. Experiments havebeen carried on a new public and freely available database, containing a widevariety of attacks. Video sequences have been recorded thanks to severalsensors resulting in 14 different streams in the visible, NIR, SWIR and thermalspectra, as well as depth data. The best proposed approach is able to almostperfectly detect all impersonation attacks while ensuring low bonafideclassification errors. On the other hand, obtained results show thatobfuscation attacks are more difficult to detect. We hope that the proposeddatabase will foster research on this challenging problem. Finally, all thecode and instructions to reproduce presented experiments is made available tothe research community.",このペーパーでは、さまざまな画像モダリティを使用した顔提示攻撃検出の問題に対処します。特に、短波赤外（SWIR）イメージングの使用が検討されています。顔提示攻撃の検出は、入力として慎重に選択されたSWIR画像の違いのみを使用して、畳み込みニューラルネットワークに基づく最近のモデルを使用して実行されます。実施された実験は、カラー画像または異なるモダリティ（可視、NIR、熱および深度）の組み合わせ、ならびにSWIR画像の差異に作用するSVMベースの分類器に作用する類似モデルよりも優れたパフォーマンスを示します。実験は、さまざまな攻撃を含む、無料で利用できる新しい公開データベースで行われました。いくつかのセンサーのおかげでビデオシーケンスが記録され、可視、NIR、SWIR、熱スペクトル、および深度データに14の異なるストリームが生成されます。最善の提案されたアプローチは、すべてのなりすまし攻撃をほぼ完全に検出すると同時に、適切な分類エラーを低く抑えることです。一方、得られた結果は、難読化攻撃の検出がより困難であることを示しています。提案されたデータベースがこの困難な問題に関する研究を促進することを願っています。最後に、提示された実験を再現するためのすべてのコードと指示は、研究コミュニティに提供されます。
http://arxiv.org/abs/2007.11465v1,Wasserstein Routed Capsule Networks,http://arxiv.org/pdf/2007.11465v1,"Capsule networks offer interesting properties and provide an alternative totoday's deep neural network architectures. However, recent approaches havefailed to consistently achieve competitive results across different imagedatasets. We propose a new parameter efficient capsule architecture, that isable to tackle complex tasks by using neural networks trained with anapproximate Wasserstein objective to dynamically select capsules throughout theentire architecture. This approach focuses on implementing a robust routingscheme, which can deliver improved results using little overhead. We performseveral ablation studies verifying the proposed concepts and show that ournetwork is able to substantially outperform other capsule approaches by over1.2 % on CIFAR-10, using fewer parameters.",カプセルネットワークは興味深い特性を提供し、今日のディープニューラルネットワークアーキテクチャに代わるものを提供します。ただし、最近のアプローチでは、さまざまな画像データセットにわたって競争力のある結果を一貫して達成できませんでした。新しいパラメーター効率の良いカプセルアーキテクチャを提案します。これは、近似Wasserstein目標でトレーニングされたニューラルネットワークを使用して、アーキテクチャ全体でカプセルを動的に選択することにより、複雑なタスクに取り組むことができます。このアプローチは、ほとんどオーバーヘッドを使用せずに改善された結果を提供できる、堅牢なルーティングスキームの実装に重点を置いています。提案された概念を検証するいくつかのアブレーション研究を実行し、当社のネットワークが、より少ないパラメーターを使用して、CIFAR-10で他のカプセルアプローチよりも実質的に1.2％以上優れていることを示しています。
http://arxiv.org/abs/2007.11462v1,"FedOCR: Communication-Efficient Federated Learning for Scene Text
  Recognition",http://arxiv.org/pdf/2007.11462v1,"While scene text recognition techniques have been widely used in commercialapplications, data privacy has rarely been taken into account by this researchcommunity. Most existing algorithms have assumed a set of shared or centralizedtraining data. However, in practice, data may be distributed on different localdevices that can not be centralized to share due to the privacy restrictions.In this paper, we study how to make use of decentralized datasets for traininga robust scene text recognizer while keeping them stay on local devices. To thebest of our knowledge, we propose the first framework leveraging federatedlearning for scene text recognition, which is trained with decentralizeddatasets collaboratively. Hence we name it FedOCR. To make FedCOR fairlysuitable to be deployed on end devices, we make two improvements includingusing lightweight models and hashing techniques. We argue that both are crucialfor FedOCR in terms of the communication efficiency of federated learning. Thesimulations on decentralized datasets show that the proposed FedOCR achievescompetitive results to the models that are trained with centralized data, withfewer communication costs and higher-level privacy-preserving.",シーンテキスト認識技術は商用アプリケーションで広く使用されていますが、この研究コミュニティではデータプライバシーはほとんど考慮されていません。ほとんどの既存のアルゴリズムは、共有または一元化されたトレーニングデータのセットを想定しています。ただし、実際には、プライバシー制限のために集中化して共有できないさまざまなローカルデバイスにデータを配布する場合があります。このホワイトペーパーでは、分散されたデータセットを使用して、ローカルにとどまりながら堅牢なシーンテキストレコグナイザーをトレーニングする方法を検討します。デバイス。私たちの知る限りでは、シーンテキストの認識にフェデレーテッドラーニングを活用する最初のフレームワークを提案します。これは、分散型データセットで協調的にトレーニングされます。したがって、FedOCRという名前を付けます。 FedCORをエンドデバイスに展開するのにかなり適したものにするために、軽量モデルとハッシュ技術の使用を含む2つの改善を行います。どちらもFedOCRにとって、統合学習の通信効率の点で非常に重要であると主張します。分散型データセットのシミュレーションは、提案されたFedOCRが、集中型データでトレーニングされたモデルに対して競争力のある結果を実現し、通信コストとプライバシー保護をより低くすることを示しています。
http://arxiv.org/abs/2007.11460v1,"Video-ception Network: Towards Multi-Scale Efficient Asymmetric
  Spatial-Temporal Interactions",http://arxiv.org/pdf/2007.11460v1,"Previous video modeling methods leverage the cubic 3D convolution filters orits decomposed variants to exploit the motion cues for precise actionrecognition, which tend to be performed on the video features along thetemporal and spatial axes symmetrically. This brings the hypothesis implicitlythat the actions are recognized from the cubic voxel level and neglects theessential spatial-temporal shape diversity across different actions. In thispaper, we propose a novel video representing method that fuses the featuresspatially and temporally in an asymmetric way to model action atomics spanningmulti-scale spatial-temporal scales. To permit the feature fusion procedureefficiently and effectively, we also design the optimized feature interactionlayer, which covers most feature fusion techniques as special case of it, e.g.,channel shuffling and channel concatenating. We instantiate our method as a\textit{plug-and-play} block, termed Multi-Scale Efficient AsymmetricSpatial-Temporal Block. Our method can easily adapt the traditional 2D CNNs tothe video understanding tasks such as action recognition. We verify our methodon several most recent large-scale video datasets requiring strong temporalreasoning or appearance discriminating, e.g., Something-to-Something v1,Kinetics and Diving48, demonstrate the new state-of-the-art results withoutbells and whistles.",以前のビデオモデリング手法は、立方3D畳み込みフィルターまたはその分解されたバリアントを利用して、モーションキューを利用して正確なアクション認識を行います。これは、アクションが立方体のボクセルレベルから認識され、さまざまなアクションにわたる本質的な時空間形状の多様性を無視するという仮説を暗黙のうちにもたらします。この論文では、特徴を時空間的に非対称的に融合させて、マルチスケールの時空間スケールにまたがるアクションアトミックをモデル化する方法を表す新しいビデオを提案します。機能の融合手順を効率的かつ効果的に許可するために、最適化された機能の相互作用レイヤーも設計します。これにより、ほとんどの機能の融合技術が特別なケースとしてカバーされます。たとえば、チャネルのシャッフルやチャネルの連結などです。メソッドをa \ textit {plug-and-play}ブロックとしてインスタンス化し、Multi-Scale Efficient AsymmetricSpatial-Temporal Blockと呼びます。私たちの方法は、従来の2D CNNをアクション認識などのビデオ理解タスクに簡単に適合させることができます。私たちは、Something-to-Something v1、Kinetics、Diving48などの強力な時間的推論または外観の区別を必要とする最新のいくつかの大規模なビデオデータセットの手法を検証し、ベルやホイッスルなしで新しい最先端の結果を示します。
http://arxiv.org/abs/2007.11457v1,"Learning One Class Representations for Face Presentation Attack
  Detection using Multi-channel Convolutional Neural Networks",http://arxiv.org/pdf/2007.11457v1,"Face recognition has evolved as a widely used biometric modality. However,its vulnerability against presentation attacks poses a significant securitythreat. Though presentation attack detection (PAD) methods try to address thisissue, they often fail in generalizing to unseen attacks. In this work, wepropose a new framework for PAD using a one-class classifier, where therepresentation used is learned with a Multi-Channel Convolutional NeuralNetwork (MCCNN). A novel loss function is introduced, which forces the networkto learn a compact embedding for bonafide class while being far from therepresentation of attacks. A one-class Gaussian Mixture Model is used on top ofthese embeddings for the PAD task. The proposed framework introduces a novelapproach to learn a robust PAD system from bonafide and available (known)attack classes. This is particularly important as collecting bonafide data andsimpler attacks are much easier than collecting a wide variety of expensiveattacks. The proposed system is evaluated on the publicly available WMCAmulti-channel face PAD database, which contains a wide variety of 2D and 3Dattacks. Further, we have performed experiments with MLFP and SiW-M datasetsusing RGB channels only. Superior performance in unseen attack protocols showsthe effectiveness of the proposed approach. Software, data, and protocols toreproduce the results are made available publicly.",顔認識は、広く使用されている生体認証モダリティとして進化してきました。ただし、プレゼンテーション攻撃に対するその脆弱性は、重大なセキュリティ上の脅威をもたらします。プレゼンテーション攻撃検出（PAD）メソッドはこの問題に対処しようとしますが、目に見えない攻撃への一般化に失敗することがよくあります。この作業では、1クラスの分類器を使用してPADの新しいフレームワークを提案します。使用される表現は、マルチチャネルたたみ込みニューラルネットワーク（MCCNN）で学習されます。斬新な損失関数が導入されました。これにより、ネットワークは、攻撃の表現から遠く離れている間、真正なクラスのコンパクトな埋め込みを学習します。 1クラスのガウス混合モデルは、PADタスクのこれらの埋め込みに加えて使用されます。提案されたフレームワークは、真正なものと利用可能な（既知の）攻撃クラスから堅牢なPADシステムを学ぶための斬新なアプローチを紹介します。真正なデータと単純な攻撃を収集することは、さまざまな高価な攻撃を収集するよりもはるかに簡単であるため、これは特に重要です。提案されたシステムは、多種多様な2Dおよび3D攻撃を含む、公に利用可能なWMCAマルチチャンネルフェイスPADデータベースで評価されます。さらに、RGBチャネルのみを使用して、MLFPおよびSiW-Mデータセットで実験を行いました。目に見えない攻撃プロトコルでの優れたパフォーマンスは、提案されたアプローチの有効性を示しています。結果を再現するためのソフトウェア、データ、プロトコルは公開されています。
http://arxiv.org/abs/2007.11432v1,"Combining Implicit Function Learning and Parametric Models for 3D Human
  Reconstruction",http://arxiv.org/pdf/2007.11432v1,"Implicit functions represented as deep learning approximations are powerfulfor reconstructing 3D surfaces. However, they can only produce static surfacesthat are not controllable, which provides limited ability to modify theresulting model by editing its pose or shape parameters. Nevertheless, suchfeatures are essential in building flexible models for both computer graphicsand computer vision. In this work, we present methodology that combinesdetail-rich implicit functions and parametric representations in order toreconstruct 3D models of people that remain controllable and accurate even inthe presence of clothing. Given sparse 3D point clouds sampled on the surfaceof a dressed person, we use an Implicit Part Network (IP-Net)to jointly predictthe outer 3D surface of the dressed person, the and inner body surface, and thesemantic correspondences to a parametric body model. We subsequently usecorrespondences to fit the body model to our inner surface and then non-rigidlydeform it (under a parametric body + displacement model) to the outer surfacein order to capture garment, face and hair detail. In quantitative andqualitative experiments with both full body data and hand scans we show thatthe proposed methodology generalizes, and is effective even given incompletepoint clouds collected from single-view depth images. Our models and code canbe downloaded from http://virtualhumans.mpi-inf.mpg.de/ipnet.",深層学習近似として表される陰関数は、3Dサーフェスを再構築するのに強力です。ただし、制御できない静的なサーフェスしか生成できないため、ポーズまたはシェイプパラメータを編集して結果のモデルを変更する機能が制限されます。それにもかかわらず、そのような機能は、コンピュータグラフィックスとコンピュータビジョンの両方の柔軟なモデルを構築する上で不可欠です。この作業では、衣服が存在していても制御可能で正確なままである人々の3Dモデルを再構築するために、詳細な陰関数とパラメトリック表現を組み合わせる方法論を提示します。服を着た人の表面でサンプリングされた疎な3D点群が与えられた場合、私たちはインプリシットパーツネットワーク（IP-Net）を使用して、服を着た人の外側の3D表面、身体の内側と内側の表面、およびパラメトリックボディモデルへのこれらの対応を予測します。続いて、対応関係を使用してボディモデルを内面にフィットさせ、それを（パラメトリックボディ+変位モデルの下で）非剛体的に外面に変形して、衣服、顔、髪のディテールをキャプチャします。全身データとハンドスキャンの両方を使用した定量的および定性的実験では、提案された方法が一般化し、単一ビューの深度画像から収集された不完全な点群であっても効果的であることを示しています。モデルとコードは、http：//virtualhumans.mpi-inf.mpg.de/ipnetからダウンロードできます。
http://arxiv.org/abs/2007.11430v1,"Learning Disentangled Feature Representation for Hybrid-distorted Image
  Restoration",http://arxiv.org/pdf/2007.11430v1,"Hybrid-distorted image restoration (HD-IR) is dedicated to restore realdistorted image that is degraded by multiple distortions. Existing HD-IRapproaches usually ignore the inherent interference among hybrid distortionswhich compromises the restoration performance. To decompose such interference,we introduce the concept of Disentangled Feature Learning to achieve thefeature-level divide-and-conquer of hybrid distortions. Specifically, wepropose the feature disentanglement module (FDM) to distribute featurerepresentations of different distortions into different channels by revisinggain-control-based normalization. We also propose a feature aggregation module(FAM) with channel-wise attention to adaptively filter out the distortionrepresentations and aggregate useful content information from differentchannels for the construction of raw image. The effectiveness of the proposedscheme is verified by visualizing the correlation matrix of features andchannel responses of different distortions. Extensive experimental results alsoprove superior performance of our approach compared with the latest HD-IRschemes.",ハイブリッド歪み画像復元（HD-IR）は、複数の歪みによって劣化したリアルな歪み画像の復元に特化しています。既存のHD-IRアプローチは通常、復元性能を損なうハイブリッド歪み間の固有の干渉を無視します。そのような干渉を分解するために、ハイブリッド歪みの機能レベルの分割統治を達成するために、Disentangled Feature Learningの概念を導入します。具体的には、ゲイン制御ベースの正規化を修正することにより、さまざまな歪みのfeaturerepresentationsをさまざまなチャネルに分散するために、機能の絡み合い解消モジュール（FDM）を提案します。また、歪みの表現を適応的にフィルタリングし、生の画像を構築するためにさまざまなチャネルから有用なコンテンツ情報を集約するために、チャネルごとに注意を払った機能集約モジュール（FAM）を提案します。提案されたスキームの有効性は、特徴の相関行列とさまざまな歪みのチャネル応答を視覚化することによって検証されます。広範な実験結果はまた、最新のHD-IRスキームと比較して、アプローチの優れたパフォーマンスを証明します。
http://arxiv.org/abs/2007.11392v1,Feature based Sequential Classifier with Attention Mechanism,http://arxiv.org/pdf/2007.11392v1,"Cervical cancer is one of the deadliest cancers affecting women globally.Cervical intraepithelial neoplasia (CIN) assessment using histopathologicalexamination of cervical biopsy slides is subject to interobserver variability.Automated processing of digitized histopathology slides has the potential formore accurate classification for CIN grades from normal to increasing grades ofpre-malignancy: CIN1, CIN2 and CIN3. Cervix disease is generally understood toprogress from the bottom (basement membrane) to the top of the epithelium. Tomodel this relationship of disease severity to spatial distribution ofabnormalities, we propose a network pipeline, DeepCIN, to analyzehigh-resolution epithelium images (manually extracted from whole-slide images)hierarchically by focusing on localized vertical regions and fusing this localinformation for determining Normal/CIN classification. The pipeline containstwo classifier networks: 1) a cross-sectional, vertical segment-level sequencegenerator (two-stage encoder model) is trained using weak supervision togenerate feature sequences from the vertical segments to preserve thebottom-to-top feature relationships in the epithelium image data; 2) anattention-based fusion network image-level classifier predicting the final CINgrade by merging vertical segment sequences. The model produces the CINclassification results and also determines the vertical segment contributionsto CIN grade prediction. Experiments show that DeepCIN achievespathologist-level CIN classification accuracy.",子宮頸がんは、世界的に女性に影響を与える最も致命的ながんの1つです。子宮頸部生検スライドの組織病理学的検査を使用した子宮頸部上皮内腫瘍（CIN）の評価は、観察者間のばらつきの影響を受けます。前悪性腫瘍のグレード：CIN1、CIN2、CIN3。子宮頸部疾患は一般に、上皮の底部（基底膜）から上部へと進行すると理解されています。異常の空間分布に対する疾患の重症度のこの関係をモデル化するために、ローカライズされた垂直領域に焦点を当て、このローカル情報を融合して正常/ CINを決定することにより、高解像度の上皮画像（スライド全体の画像から手動で抽出）を階層的に分析するネットワークパイプライン、DeepCINを提案します分類。パイプラインには2つの分類子ネットワークが含まれています。1）断面の垂直セグメントレベルのシーケンスジェネレーター（2ステージエンコーダーモデル）は、弱い監視を使用してトレーニングされ、垂直セグメントから特徴シーケンスを生成して、上皮画像の上下の特徴の関係を維持します。データ; 2）垂直セグメントシーケンスをマージすることによって最終的なCINgradeを予測する注意力ベースのフュージョンネットワーク画像レベル分類子。モデルはCIN分類結果を生成し、CINグレード予測に対する垂直セグメントの寄与も決定します。実験は、DeepCINが病理学者レベルのCIN分類精度を達成することを示しています。
http://arxiv.org/abs/2007.11365v1,"Depthwise Spatio-Temporal STFT Convolutional Neural Networks for Human
  Action Recognition",http://arxiv.org/pdf/2007.11365v1,"Conventional 3D convolutional neural networks (CNNs) are computationallyexpensive, memory intensive, prone to overfitting, and most importantly, thereis a need to improve their feature learning capabilities. To address theseissues, we propose spatio-temporal short term Fourier transform (STFT) blocks,a new class of convolutional blocks that can serve as an alternative to the 3Dconvolutional layer and its variants in 3D CNNs. An STFT block consists ofnon-trainable convolution layers that capture spatially and/or temporally localFourier information using a STFT kernel at multiple low frequency points,followed by a set of trainable linear weights for learning channelcorrelations. The STFT blocks significantly reduce the space-time complexity in3D CNNs. In general, they use 3.5 to 4.5 times less parameters and 1.5 to 1.8times less computational costs when compared to the state-of-the-art methods.Furthermore, their feature learning capabilities are significantly better thanthe conventional 3D convolutional layer and its variants. Our extensiveevaluation on seven action recognition datasets, including Something-somethingv1 and v2, Jester, Diving-48, Kinetics-400, UCF 101, and HMDB 51, demonstratethat STFT blocks based 3D CNNs achieve on par or even better performancecompared to the state-of-the-art methods.",従来の3D畳み込みニューラルネットワーク（CNN）は、計算コストが高く、メモリを大量に消費し、過剰適合しがちであり、最も重要なこととして、機能学習能力を向上させる必要があります。これらの問題に対処するために、3D畳み込みレイヤーと3D CNNのバリアントの代替として機能する畳み込みブロックの新しいクラスである時空間短期フーリエ変換（STFT）ブロックを提案します。 STFTブロックは、複数の低周波数ポイントでSTFTカーネルを使用して空間的または時間的にlocalFourier情報をキャプチャする非トレーニング可能なたたみ込み層で構成され、その後にチャネル相関を学習するためのトレーニング可能な線形重みのセットが続きます。 STFTブロックは、3D CNNの時空の複雑さを大幅に軽減します。一般的に、最先端の方法と比較すると、パラメーターの使用量が3.5から4.5倍少なく、計算コストが1.5から1.8倍少なくなります。さらに、それらの機能学習機能は、従来の3D畳み込みレイヤーとそのバリアントよりもはるかに優れています。 Something-somethingv1とv2、Jester、Diving-48、Kinetics-400、UCF 101、およびHMDB 51を含む7つのアクション認識データセットに対する私たちの広範な評価は、STFTブロックベースの3D CNNが現状と比較して同等以上のパフォーマンスを達成することを示しています-最先端の方法。
http://arxiv.org/abs/2007.11361v1,Human-Centered Unsupervised Segmentation Fusion,http://arxiv.org/pdf/2007.11361v1,"Segmentation is generally an ill-posed problem since it results in multiplesolutions and is, therefore, hard to define ground truth data to evaluatealgorithms. The problem can be naively surpassed by using only one annotatorper image, but such acquisition doesn't represent the cognitive perception ofan image by the majority of people. Nowadays, it is not difficult to obtainmultiple segmentations with crowdsourcing, so the only problem that stays ishow to get one ground truth segmentation per image. There already existnumerous algorithmic solutions, but most methods are supervised or don'tconsider confidence per human segmentation. In this paper, we introduce a newsegmentation fusion model that is based on K-Modes clustering. Results obtainedfrom publicly available datasets with human ground truth segmentations clearlyshow that our model outperforms the state-of-the-art on human segmentations.",セグメンテーションは複数のソリューションをもたらし、アルゴリズムを評価するためのグラウンドトゥルースデータを定義することが難しいため、一般に不適切な問題です。アノテーターごとの画像を1つだけ使用することで、問題を簡単に克服できますが、そのような取得は、大多数の人々による画像の認知的知覚を表すものではありません。現在、クラウドソーシングで複数のセグメンテーションを取得することは難しくありません。そのため、残る唯一の問題は、画像ごとに1つのグラウンドトゥルースセグメンテーションを取得することです。すでに数多くのアルゴリズムソリューションが存在しますが、ほとんどの方法は監視されているか、人間のセグメンテーションごとの信頼性を考慮していません。このホワイトペーパーでは、Kモードのクラスタリングに基づくニュースセグメント化融合モデルを紹介します。人間のグラウンドトゥルースセグメンテーションを使用して公開されているデータセットから得られた結果は、私たちのモデルが人間のセグメンテーションの最先端技術よりも優れていることを明確に示しています。
http://arxiv.org/abs/2007.11355v1,"Leveraging Undiagnosed Data for Glaucoma Classification with
  Teacher-Student Learning",http://arxiv.org/pdf/2007.11355v1,"Recently, deep learning has been adopted to the glaucoma classification taskwith performance comparable to that of human experts. However, a well traineddeep learning model demands a large quantity of properly labeled data, which isrelatively expensive since the accurate labeling of glaucoma requires years ofspecialist training. In order to alleviate this problem, we propose a glaucomaclassification framework which takes advantage of not only the properly labeledimages, but also undiagnosed images without glaucoma labels. To be morespecific, the proposed framework is adapted from the teacher-student-learningparadigm. The teacher model encodes the wrapped information of undiagnosedimages to a latent feature space, meanwhile the student model learns from theteacher through knowledge transfer to improve the glaucoma classification. Forthe model training procedure, we propose a novel training strategy thatsimulates the real-world teaching practice named as 'Learning To Teach withKnowledge Transfer (L2T-KT)', and establish a 'Quiz Pool' as the teacher'soptimization target. Experiments show that the proposed framework is able toutilize the undiagnosed data effectively to improve the glaucoma predictionperformance.",最近、深層学習が緑内障分類タスクに採用され、人間の専門家と同等のパフォーマンスが得られました。ただし、十分にトレーニングされたディープラーニングモデルでは、適切にラベル付けされた大量のデータが必要です。これは、緑内障の正確なラベル付けに何年もの専門的なトレーニングが必要になるため、比較的費用がかかります。この問題を軽減するために、適切にラベル付けされた画像だけでなく、緑内障のラベルがない診断されていない画像も利用する緑内障分類フレームワークを提案します。より具体的には、提案されたフレームワークは、教師、生徒、学習のパラダイムから採用されています。教師モデルは診断されていない画像のラップされた情報を潜在的な特徴空間にエンコードし、一方、学生モデルは緑内障の分類を改善するために知識の伝達を通じて教師から学習します。モデルのトレーニング手順については、「知識移転による学習（L2T-KT）」という名前の実際の教育実践をシミュレートする新しいトレーニング戦略を提案し、教師の最適化ターゲットとして「クイズプール」を確立します。実験は、提案されたフレームワークが緑内障の予測性能を改善するために診断されていないデータを効果的に利用できることを示しています。
http://arxiv.org/abs/2007.11349v1,Learning Directional Feature Maps for Cardiac MRI Segmentation,http://arxiv.org/pdf/2007.11349v1,"Cardiac MRI segmentation plays a crucial role in clinical diagnosis forevaluating personalized cardiac performance parameters. Due to the indistinctboundaries and heterogeneous intensity distributions in the cardiac MRI, mostexisting methods still suffer from two aspects of challenges: inter-classindistinction and intra-class inconsistency. To tackle these two problems, wepropose a novel method to exploit the directional feature maps, which cansimultaneously strengthen the differences between classes and the similaritieswithin classes. Specifically, we perform cardiac segmentation and learn adirection field pointing away from the nearest cardiac tissue boundary to eachpixel via a direction field (DF) module. Based on the learned direction field,we then propose a feature rectification and fusion (FRF) module to improve theoriginal segmentation features, and obtain the final segmentation. The proposedmodules are simple yet effective and can be flexibly added to any existingsegmentation network without excessively increasing time and space complexity.We evaluate the proposed method on the 2017 MICCAI Automated Cardiac DiagnosisChallenge (ACDC) dataset and a large-scale self-collected dataset, showing goodsegmentation performance and robust generalization ability of the proposedmethod.",心臓MRIセグメンテーションは、個別の心臓性能パラメーターを評価するための臨床診断において重要な役割を果たします。心臓MRIの境界が不明確で強度分布が不均一であるため、既存の方法では、クラス間の不一致とクラス内の不一致という2つの課題に直面しています。これら2つの問題に取り組むために、方向性特徴マップを活用する新しい方法を提案します。これにより、クラス間の違いとクラス内の類似性を同時に強化できます。具体的には、心臓のセグメンテーションを実行し、方向フィールド（DF）モジュールを介して、最も近い心臓組織の境界から各ピクセルへと向いている方向フィールドを学習します。学習した方向フィールドに基づいて、元のセグメンテーション機能を改善し、最終的なセグメンテーションを取得するための機能修正および融合（FRF）モジュールを提案します。提案されたモジュールはシンプルでありながら効果的であり、時間と空間の複雑さを過度に増加させることなく、既存のセグメンテーションネットワークに柔軟に追加できます.2017 MICCAI Automated Cardiac DiagnosisChallenge（ACDC）データセットと大規模な自己収集データセットで提案された方法を評価し、提案された方法の商品分類性能と堅牢な一般化能力。
http://arxiv.org/abs/2007.11344v1,DEAL: Deep Evidential Active Learning for Image Classification,http://arxiv.org/pdf/2007.11344v1,"Convolutional Neural Networks (CNNs) have proven to be state-of-the-artmodels for supervised computer vision tasks, such as image classification.However, large labeled data sets are generally needed for the training andvalidation of such models. In many domains, unlabeled data is available butlabeling is expensive, for instance when specific expert knowledge is required.Active Learning (AL) is one approach to mitigate the problem of limited labeleddata. Through selecting the most informative and representative data instancesfor labeling, AL can contribute to more efficient learning of the model. RecentAL methods for CNNs propose different solutions for the selection of instancesto be labeled. However, they do not perform consistently well and are oftencomputationally expensive. In this paper, we propose a novel AL algorithm thatefficiently learns from unlabeled data by capturing high predictionuncertainty. By replacing the softmax standard output of a CNN with theparameters of a Dirichlet density, the model learns to identify data instancesthat contribute efficiently to improving model performance during training. Wedemonstrate in several experiments with publicly available data that our methodconsistently outperforms other state-of-the-art AL approaches. It can be easilyimplemented and does not require extensive computational resources fortraining. Additionally, we are able to show the benefits of the approach on areal-world medical use case in the field of automated detection of visualsignals for pneumonia on chest radiographs.",畳み込みニューラルネットワーク（CNN）は、画像分類などの監視されたコンピュータービジョンタスク用の最先端のモデルであることが証明されていますが、通常、このようなモデルのトレーニングと検証には、大きなラベル付きデータセットが必要です。多くのドメインでは、ラベル付けされていないデータを使用できますが、特定の専門知識が必要な場合など、ラベル付けはコストがかかります。アクティブラーニング（AL）は、ラベル付きデータが限られているという問題を軽減する1つのアプローチです。ラベル付けに最も有益で代表的なデータインスタンスを選択することにより、ALはモデルのより効率的な学習に貢献できます。 CNNの最近の方法では、ラベル付けするインスタンスを選択するためのさまざまなソリューションを提案しています。ただし、それらは一貫して適切に実行されず、多くの場合、計算コストが高くなります。この論文では、高い予測不確実性を捕捉することにより、ラベルなしデータから効率的に学習する新しいALアルゴリズムを提案します。 CNNのソフトマックス標準出力をディリクレ密度のパラメーターで置き換えることにより、モデルは、トレーニング中のモデルパフォーマンスの改善に効果的に寄与するデータインスタンスを特定することを学習します。私たちの方法が他の最先端のALのアプローチを一貫して上回っていることを公に入手可能なデータでいくつかの実験で実証します。これは簡単に実装でき、トレーニングのために膨大な計算リソースを必要としません。さらに、胸部X線写真で肺炎の視覚信号を自動検出する分野で、領域別の医療ユースケースに対するアプローチの利点を示すことができます。
http://arxiv.org/abs/2007.11341v1,Unsupervised Shape and Pose Disentanglement for 3D Meshes,http://arxiv.org/pdf/2007.11341v1,"Parametric models of humans, faces, hands and animals have been widely usedfor a range of tasks such as image-based reconstruction, shape correspondenceestimation, and animation. Their key strength is the ability to factor surfacevariations into shape and pose dependent components. Learning such modelsrequires lots of expert knowledge and hand-defined object-specific constraints,making the learning approach unscalable to novel objects. In this paper, wepresent a simple yet effective approach to learn disentangled shape and poserepresentations in an unsupervised setting. We use a combination ofself-consistency and cross-consistency constraints to learn pose and shapespace from registered meshes. We additionally incorporate as-rigid-as-possibledeformation(ARAP) into the training loop to avoid degenerate solutions. Wedemonstrate the usefulness of learned representations through a number of tasksincluding pose transfer and shape retrieval. The experiments on datasets of 3Dhumans, faces, hands and animals demonstrate the generality of our approach.Code is made available athttps://virtualhumans.mpi-inf.mpg.de/unsup_shape_pose/.",人間、顔、手、動物のパラメトリックモデルは、画像ベースの再構築、形状対応推定、アニメーションなどのさまざまなタスクで広く使用されています。それらの重要な強みは、表面変動を形状に因数分解し、依存するコンポーネントにポーズをとることができることです。そのようなモデルを学習するには、多くの専門知識と手動で定義したオブジェクト固有の制約が必要であり、学習アプローチを新規オブジェクトに拡張できません。このホワイトペーパーでは、監視されていない環境で絡み合っていない形状とポーズ表現を学習するための、シンプルで効果的なアプローチを紹介します。登録されたメッシュからポーズとシェイプスペースを学習するために、自己無矛盾性と相互無矛盾性制約の組み合わせを使用します。さらに、縮退したソリューションを回避するために、トレーニングループにas-rigid-as-possibledeformation（ARAP）を組み込みます。ポーズの転送や形状の取得などのいくつかのタスクを通じて、学習した表現の有用性を実証します。 3D人間、顔、手、動物のデータセットの実験は、私たちのアプローチの一般性を示しています。コードは、https：//virtualhumans.mpi-inf.mpg.de/unsup_shape_pose/で入手できます。
http://arxiv.org/abs/2007.11330v1,Multi-Task Curriculum Framework for Open-Set Semi-Supervised Learning,http://arxiv.org/pdf/2007.11330v1,"Semi-supervised learning (SSL) has been proposed to leverage unlabeled datafor training powerful models when only limited labeled data is available. Whileexisting SSL methods assume that samples in the labeled and unlabeled datashare the classes of their samples, we address a more complex novel scenarionamed open-set SSL, where out-of-distribution (OOD) samples are contained inunlabeled data. Instead of training an OOD detector and SSL separately, wepropose a multi-task curriculum learning framework. First, to detect the OODsamples in unlabeled data, we estimate the probability of the sample belongingto OOD. We use a joint optimization framework, which updates the networkparameters and the OOD score alternately. Simultaneously, to achieve highperformance on the classification of in-distribution (ID) data, we select IDsamples in unlabeled data having small OOD scores, and use these data withlabeled data for training the deep neural networks to classify ID samples in asemi-supervised manner. We conduct several experiments, and our method achievesstate-of-the-art results by successfully eliminating the effect of OOD samples.",半教師付き学習（SSL）は、限られたラベル付きデータしか利用できない場合に、ラベルなしデータを活用して強力なモデルをトレーニングするために提案されています。既存のSSLメソッドは、ラベル付きデータとラベルなしデータのサンプルがサンプルのクラスを共有することを前提としていますが、オープンセットSSLという名前のより複雑な小説シナリオに対処します。ここでは、配布外（OOD）サンプルがラベルなしデータに含まれています。 OOD検出器とSSLを個別にトレーニングする代わりに、マルチタスクカリキュラム学習フレームワークを提案します。まず、ラベルのないデータでOODsamplesを検出するために、OODに属するサンプルの確率を推定します。ネットワークパラメータとOODスコアを交互に更新する共同最適化フレームワークを使用します。同時に、分布内（ID）データの分類で高性能を達成するために、OODスコアが小さいラベルなしデータのIDサンプルを選択し、これらのデータをラベル付きデータとともに使用して、ディープニューラルネットワークをトレーニングし、半教師ありの方法でIDサンプルを分類します。私たちはいくつかの実験を行い、私たちの方法はOODサンプルの影響をうまく排除することで最先端の結果を達成しています。
http://arxiv.org/abs/2007.11328v1,"Watchlist Risk Assessment using Multiparametric Cost and Relative
  Entropy",http://arxiv.org/pdf/2007.11328v1,"This paper addresses the facial biometric-enabled watchlist technology inwhich risk detectors are mandatory mechanisms for early detection of threats,as well as for avoiding offense to innocent travelers. We propose amultiparametric cost assessment and relative entropy measures as riskdetectors. We experimentally demonstrate the effects of mis-identification andimpersonation under various watchlist screening scenarios and constraints. Thekey contributions of this paper are the novel techniques for design andanalysis of the biometric-enabled watchlist and the supporting infrastructure,as well as measuring the impersonation impact on e-border performance.",このペーパーでは、顔の生体認証が有効なウォッチリストテクノロジーについて説明します。このテクノロジーでは、リスク検出機能が脅威を早期に検出し、罪のない旅行者の攻撃を回避するために必須のメカニズムです。リスク検出器として、マルチパラメトリックコスト評価と相対エントロピー測定を提案します。さまざまなウォッチリストスクリーニングシナリオと制約の下での誤認となりすましの影響を実験的に示します。このホワイトペーパーの主要な貢献は、生体認証が有効なウォッチリストとサポートインフラストラクチャの設計と分析、および電子境界のパフォーマンスに対する偽装の影響を測定するための新しい手法です。
http://arxiv.org/abs/2007.11983v1,CNN+RNN Depth and Skeleton based Dynamic Hand Gesture Recognition,http://arxiv.org/pdf/2007.11983v1,"Human activity and gesture recognition is an important component of rapidlygrowing domain of ambient intelligence, in particular in assisting living andsmart homes. In this paper, we propose to combine the power of two deeplearning techniques, the convolutional neural networks (CNN) and the recurrentneural networks (RNN), for automated hand gesture recognition using both depthand skeleton data. Each of these types of data can be used separately to trainneural networks to recognize hand gestures. While RNN were reported previouslyto perform well in recognition of sequences of movement for each skeleton jointgiven the skeleton information only, this study aims at utilizing depth dataand apply CNN to extract important spatial information from the depth images.Together, the tandem CNN+RNN is capable of recognizing a sequence of gesturesmore accurately. As well, various types of fusion are studied to combine boththe skeleton and depth information in order to extract temporal-spatialinformation. An overall accuracy of 85.46% is achieved on the dynamic handgesture-14/28 dataset.",人間の活動とジェスチャー認識は、特に生活やスマートホームの支援において、急速に成長しているアンビエントインテリジェンスのドメインの重要なコンポーネントです。この論文では、深度データとスケルトンデータの両方を使用して自動化された手ジェスチャー認識のために、畳み込みニューラルネットワーク（CNN）とリカレントニューラルネットワーク（RNN）の2つのディープラーニング技術の力を組み合わせることが提案されています。これらの各タイプのデータを個別に使用して、ニューラルネットワークをトレーニングし、手振りを認識させることができます。 RNNは以前に、骨格情報のみを与えられた各骨格の動きのシーケンスの認識でうまく機能することが報告されていましたが、この研究は深度データを利用し、CNNを適用して深度画像から重要な空間情報を抽出することを目的としています。まとめて、タンデムCNN + RNNはジェスチャーのシーケンスをより正確に認識する方法。同様に、時間的空間情報を抽出するために、スケルトン情報と深度情報の両方を組み合わせるために、さまざまなタイプの融合が研究されています。動的ハンドジェスチャー-14/28データセットでは、85.46％の全体的な精度が達成されます。
http://arxiv.org/abs/2007.11986v1,Dog Identification using Soft Biometrics and Neural Networks,http://arxiv.org/pdf/2007.11986v1,"This paper addresses the problem of biometric identification of animals,specifically dogs. We apply advanced machine learning models such as deepneural network on the photographs of pets in order to determine the petidentity. In this paper, we explore the possibility of using different types of""soft"" biometrics, such as breed, height, or gender, in fusion with ""hard""biometrics such as photographs of the pet's face. We apply the principle oftransfer learning on different Convolutional Neural Networks, in order tocreate a network designed specifically for breed classification. The proposednetwork is able to achieve an accuracy of 90.80% and 91.29% whendifferentiating between the two dog breeds, for two different datasets. Withoutthe use of ""soft"" biometrics, the identification rate of dogs is 78.09% but byusing a decision network to incorporate ""soft"" biometrics, the identificationrate can achieve an accuracy of 84.94%.",この論文は、動物、特に犬の生体認証の問題に対処します。ペットの写真にディープニューラルネットワークなどの高度な機械学習モデルを適用して、信頼性を判断します。このペーパーでは、品種、身長、性別などのさまざまな種類の「ソフト」バイオメトリクスを、ペットの顔の写真などの「ハード」バイオメトリクスと組み合わせて使用​​する可能性を探ります。品種分類用に特別に設計されたネットワークを作成するために、さまざまな畳み込みニューラルネットワークに転移学習の原理を適用します。提案されたネットワークは、2つの異なるデータセットに対して、2つの犬種を区別するときに90.80％と91.29％の精度を達成できます。 「ソフト」バイオメトリクスを使用しない場合、犬の識別率は78.09％ですが、決定ネットワークを使用して「ソフト」バイオメトリクスを組み込むことにより、識別率は84.94％の精度を達成できます。
http://arxiv.org/abs/2007.11323v1,Risk Assessment in the Face-based Watchlist Screening in e-Border,http://arxiv.org/pdf/2007.11323v1,"This paper concerns with facial-based watchlist technology as a component ofautomated border control machines deployed in e-borders. The key task of thewatchlist technology is to mitigate effects of mis-identification andimpersonation. To address this problem, we developed a novel cost-based modelof traveler risk assessment and proved its efficiency via intensive experimentsusing large-scale facial databases. The results of this study are applicable toany biometric modality to be used in watchlist technology.",このペーパーは、e-bordersに展開された自動化された境界制御マシンのコンポーネントとしての顔面ベースのウォッチリスト技術に関係しています。ウォッチリスト技術の主要なタスクは、誤認やなりすましの影響を軽減することです。この問題に対処するために、旅行者のリスク評価の新しいコストベースのモデルを開発し、大規模な顔のデータベースを使用した集中的な実験によってその効率を証明しました。この研究の結果は、ウォッチリスト技術で使用されるあらゆる生体認証モダリティに適用できます。
http://arxiv.org/abs/2007.11987v1,Multi-Metric Evaluation of Thermal-to-Visual Face Recognition,http://arxiv.org/pdf/2007.11987v1,"In this paper, we aim to address the problem of heterogeneous orcross-spectral face recognition using machine learning to synthesize visualspectrum face from infrared images. The synthesis of visual-band face imagesallows for more optimal extraction of facial features to be used for faceidentification and/or verification. We explore the ability to use GenerativeAdversarial Networks (GANs) for face image synthesis, and examine theperformance of these images using pre-trained Convolutional Neural Networks(CNNs). The features extracted using CNNs are applied in face identificationand verification. We explore the performance in terms of acceptance rate whenusing various similarity measures for face verification.",この論文では、赤外線画像から視覚スペクトル顔を合成するために機械学習を使用して、異種またはクロススペクトル顔認識の問題に対処することを目指しています。視覚帯の顔画像の合成により、顔の識別および/または検証に使用される顔の特徴のより最適な抽出が可能になります。顔画像合成にGenerativeAdversarial Networks（GAN）を使用する機能を探索し、事前トレーニング済みのConvolutional Neural Networks（CNN）を使用してこれらの画像のパフォーマンスを調べます。 CNNを使用して抽出された特徴は、顔の識別と検証に適用されます。顔の検証にさまざまな類似性測定を使用する場合、受け入れ率の観点からパフォーマンスを調査します。
http://arxiv.org/abs/2007.11319v1,"Real-Time Instrument Segmentation in Robotic Surgery using Auxiliary
  Supervised Deep Adversarial Learning",http://arxiv.org/pdf/2007.11319v1,"Robot-assisted surgery is an emerging technology which has undergone rapidgrowth with the development of robotics and imaging systems. Innovations invision, haptics and accurate movements of robot arms have enabled surgeons toperform precise minimally invasive surgeries. Real-time semantic segmentationof the robotic instruments and tissues is a crucial step in robot-assistedsurgery. Accurate and efficient segmentation of the surgical scene not onlyaids in the identification and tracking of instruments but also providedcontextual information about the different tissues and instruments beingoperated with. For this purpose, we have developed a light-weight cascadedconvolutional neural network (CNN) to segment the surgical instruments fromhigh-resolution videos obtained from a commercial robotic system. We propose amulti-resolution feature fusion module (MFF) to fuse the feature maps ofdifferent dimensions and channels from the auxiliary and main branch. We alsointroduce a novel way of combining auxiliary loss and adversarial loss toregularize the segmentation model. Auxiliary loss helps the model to learnlow-resolution features, and adversarial loss improves the segmentationprediction by learning higher order structural information. The model alsoconsists of a light-weight spatial pyramid pooling (SPP) unit to aggregate richcontextual information in the intermediate stage. We show that our modelsurpasses existing algorithms for pixel-wise segmentation of surgicalinstruments in both prediction accuracy and segmentation time ofhigh-resolution videos.",ロボット支援手術は、ロボット工学と画像処理システムの開発により急速な成長を遂げてきた新しい技術です。革新的なビジョン、ハプティックス、ロボットアームの正確な動きにより、外科医は正確な低侵襲手術を行うことができます。ロボット機器と組織のリアルタイムのセマンティックセグメンテーションは、ロボット支援手術における重要なステップです。手術シーンの正確で効率的なセグメンテーションは、器具の識別と追跡を支援するだけでなく、操作されているさまざまな組織と器具に関するコンテキスト情報も提供します。この目的のために、商用ロボットシステムから取得した高解像度ビデオから手術器具をセグメント化するために、軽量のカスケード型畳み込みニューラルネットワーク（CNN）を開発しました。補助および主分岐からの異なる次元およびチャネルの機能マップを融合するために、マルチ解像度機能融合モジュール（MFF）を提案します。また、セグメンテーションモデルを正規化するために、補助損失と敵対的損失を組み合わせる新しい方法も紹介します。補助損失は、モデルが低解像度の特徴を学習するのに役立ち、敵対的損失は、より高次の構造情報を学習することにより、セグメンテーション予測を改善します。モデルは、中間段階でリッチコンテキスト情報を集約するための軽量の空間ピラミッドプーリング（SPP）ユニットでも構成されています。高解像度ビデオの予測精度とセグメンテーション時間の両方で、外科用器具のピクセル単位のセグメンテーションのためのモデルが既存のアルゴリズムを上回ることを示しています。
http://arxiv.org/abs/2007.11318v1,Multi-Spectral Facial Biometrics in Access Control,http://arxiv.org/pdf/2007.11318v1,"This study demonstrates how facial biometrics, acquired using multi-spectralsensors, such as RGB, depth, and infrared, assist the data accumulation in theprocess of authorizing users of automated and semi-automated access systems.This data serves the purposes of person authentication, as well as facialtemperature estimation. We utilize depth data taken using an inexpensive RGB-Dsensor to find the head pose of a subject. This allows the selection of videoframes containing a frontal-view head pose for face recognition and facetemperature reading. Usage of the frontal-view frames improves the efficiencyof face recognition while the corresponding synchronized IR video frames allowfor more efficient temperature estimation for facial regions of interest. Inaddition, this study reports emerging applications of biometrics in biomedicaland health care solutions. Including surveys of recent pilot projects,involving new sensors of biometric data and new applications of humanphysiological and behavioral biometrics. It also shows the new and promisinghorizons of using biometrics in natural and contactless control interfaces forsurgical control, rehabilitation and accessibility.",この研究は、RGB、深度、赤外線などのマルチスペクトルセンサーを使用して取得した顔のバイオメトリクスが、自動および半自動アクセスシステムのユーザーを承認するプロセスでデータの蓄積をどのように支援するかを示しています。このデータは、個人認証の目的に役立ちます。同様に顔面温度推定。安価なRGB-Dsensorを使用して取得した深度データを利用して、被験者の頭のポーズを見つけます。これにより、顔認識と顔温度の読み取りのために正面の頭のポーズを含むビデオフレームを選択できます。正面ビューフレームを使用すると、顔認識の効率が向上しますが、対応する同期されたIRビデオフレームにより、関心のある顔領域のより効率的な温度推定が可能になります。さらに、この研究では、バイオメディカルおよびヘルスケアソリューションにおけるバイオメトリクスの新たなアプリケーションについて報告しています。最近のパイロットプロジェクトの調査を含み、生体認証データの新しいセンサーと、人体生理学および行動生体認証の新しいアプリケーションを含みます。また、外科的制御、リハビリテーション、アクセシビリティのための自然で非接触の制御インターフェースでバイオメトリクスを使用することの新しい有望な展望も示しています。
http://arxiv.org/abs/2007.11301v1,DeepSVG: A Hierarchical Generative Network for Vector Graphics Animation,http://arxiv.org/pdf/2007.11301v1,"Scalable Vector Graphics (SVG) are ubiquitous in modern 2D interfaces due totheir ability to scale to different resolutions. However, despite the successof deep learning-based models applied to rasterized images, the problem ofvector graphics representation learning and generation remains largelyunexplored. In this work, we propose a novel hierarchical generative network,called DeepSVG, for complex SVG icons generation and interpolation. Ourarchitecture effectively disentangles high-level shapes from the low-levelcommands that encode the shape itself. The network directly predicts a set ofshapes in a non-autoregressive fashion. We introduce the task of complex SVGicons generation by releasing a new large-scale dataset along with anopen-source library for SVG manipulation. We demonstrate that our networklearns to accurately reconstruct diverse vector graphics, and can serve as apowerful animation tool by performing interpolations and other latent spaceoperations. Our code is available at https://github.com/alexandre01/deepsvg.",スケーラブルベクターグラフィックス（SVG）は、さまざまな解像度にスケーリングできるため、最新の2Dインターフェイスに広く使用されています。しかし、ラスター化された画像に適用されたディープラーニングベースのモデルの成功にもかかわらず、ベクターグラフィックス表現の学習と生成の問題は、大部分は未踏のままです。この作業では、複雑なSVGアイコンの生成と補間のために、DeepSVGと呼ばれる新しい階層型生成ネットワークを提案します。私たちのアーキテクチャは、形状自体をエンコードする低レベルのコマンドから高レベルの形状を効果的に解きます。ネットワークは、非自己回帰的に一連の形状を直接予測します。 SVG操作用のオープンソースライブラリとともに新しい大規模データセットをリリースすることにより、複雑なSVGicons生成のタスクを紹介します。私たちのネットワークが多様なベクトルグラフィックスを正確に再構築することを学習し、補間やその他の潜在的な宇宙操作を実行することで強力なアニメーションツールとして機能できることを示します。コードはhttps://github.com/alexandre01/deepsvgで入手できます。
http://arxiv.org/abs/2007.11784v1,"Deep Learning Based Segmentation of Various Brain Lesions for
  Radiosurgery",http://arxiv.org/pdf/2007.11784v1,"Semantic segmentation of medical images with deep learning models is rapidlydeveloped. In this study, we benchmarked state-of-the-art deep learningsegmentation algorithms on our clinical stereotactic radiosurgery dataset,demonstrating the strengths and weaknesses of these algorithms in a fairlypractical scenario. In particular, we compared the model performances withrespect to their sampling method, model architecture, and the choice of lossfunctions, identifying the suitable settings for their applications andshedding light on the possible improvements.",深層学習モデルを使用した医用画像のセマンティックセグメンテーションが急速に開発されています。この研究では、臨床定位放射線治療データセットに最先端のディープラーニングセグメンテーションアルゴリズムをベンチマークし、かなり実用的なシナリオでこれらのアルゴリズムの長所と短所を示しました。特に、サンプリング方法、モデルアーキテクチャ、および損失関数の選択に関してモデルのパフォーマンスを比較し、アプリケーションに適した設定を特定し、考えられる改善点を明らかにしました。
http://arxiv.org/abs/2007.11259v2,Adversarial Training Reduces Information and Improves Transferability,http://arxiv.org/pdf/2007.11259v2,"Recent results show that features of adversarially trained networks forclassification, in addition to being robust, enable desirable properties suchas invertibility. The latter property may seem counter-intuitive as it iswidely accepted by the community that classification models should only capturethe minimal information (features) required for the task. Motivated by thisdiscrepancy, we investigate the dual relationship between Adversarial Trainingand Information Theory. We show that the Adversarial Training can improvelinear transferability to new tasks, from which arises a new trade-off betweentransferability of representations and accuracy on the source task. We validateour results employing robust networks trained on CIFAR-10, CIFAR-100 andImageNet on several datasets. Moreover, we show that Adversarial Trainingreduces Fisher information of representations about the input and of theweights about the task, and we provide a theoretical argument which explainsthe invertibility of deterministic networks without violating the principle ofminimality. Finally, we leverage our theoretical insights to remarkably improvethe quality of reconstructed images through inversion.",最近の結果は、堅牢であることに加えて、分類のために敵対的に訓練されたネットワークの機能が、可逆性などの望ましい特性を可能にすることを示しています。後者のプロパティは、分類モデルがタスクに必要な最小限の情報（機能）のみをキャプチャする必要があるとコミュニティによって広く受け入れられているため、直観に反するように見えるかもしれません。この不一致が動機となって、敵対的訓練と情報理論の間の二重の関係を調査します。敵対的トレーニングは、新しいタスクへの線形転送可能性を向上させることができることを示します。これにより、表現の転送可能性とソースタスクの精度との間に新しいトレードオフが生じます。いくつかのデータセットでCIFAR-10、CIFAR-100およびImageNetでトレーニングされた堅牢なネットワークを使用して、結果を検証します。さらに、我々は、敵対訓練が入力に関する表現とタスクに関する重みのフィッシャー情報を減らすことを示し、最小性の原則に違反することなく決定論的ネットワークの可逆性を説明する理論的議論を提供します。最後に、理論的洞察を活用して、反転によって再構成された画像の品質を著しく改善します。
http://arxiv.org/abs/2007.11257v1,Deep-VFX: Deep Action Recognition Driven VFX for Short Video,http://arxiv.org/pdf/2007.11257v1,"Human motion is a key function to communicate information. In theapplication, short-form mobile video is so popular all over the world such asTik Tok. The users would like to add more VFX so as to pursue creativity andpersonlity. Many special effects are added on the short video platform. Thesegives the users more possibility to show off these personality. The common andtraditional way is to create the template of VFX. However, in order tosynthesis the perfect, the users have to tedious attempt to grasp the timingand rhythm of new templates. It is not easy-to-use especially for the mobileapp. This paper aims to change the VFX synthesis by motion driven instead ofthe traditional template matching. We propose the AI method to improve this VFXsynthesis. In detail, in order to add the special effect on the human body. Theskeleton extraction is essential in this system. We also propose a novel formof LSTM to find out the user's intention by action recognition. The experimentshows that our system enables to generate VFX for short video more easier andefficient.",人間の動きは情報を伝達するための重要な機能です。アプリケーションでは、Tik Tokなどの短編モバイルビデオが世界中で非常に人気があります。ユーザーは、創造性と個性を追求するために、さらにVFXを追加したいと考えています。多くの特殊効果が短いビデオプラットフォームに追加されています。これらはユーザーにこれらの個性を誇示するより多くの可能性を与えます。一般的で伝統的な方法は、VFXのテンプレートを作成することです。しかし、完璧なものを合成するために、ユーザーは新しいテンプレートのタイミングとリズムを把握するための面倒な試みをしなければなりません。特にmobileappでは使いやすくありません。このペーパーは、従来のテンプレートマッチングではなく、モーションドリブンによってVFX合成を変更することを目的としています。このVFXsynthesisを改善するためのAI手法を提案します。具体的には、人体に特殊効果を加えるため。このシステムでは、骨格の抽出が不可欠です。また、行動認識によりユーザーの意図を知るためのLSTMの新しい形を提案します。実験は、私たちのシステムが短いビデオのVFXをより簡単かつ効率的に生成できることを示しています。
http://arxiv.org/abs/2007.11984v1,Unsupervised Deep Representation Learning for Real-Time Tracking,http://arxiv.org/pdf/2007.11984v1,"The advancement of visual tracking has continuously been brought by deeplearning models. Typically, supervised learning is employed to train thesemodels with expensive labeled data. In order to reduce the workload of manualannotations and learn to track arbitrary objects, we propose an unsupervisedlearning method for visual tracking. The motivation of our unsupervisedlearning is that a robust tracker should be effective in bidirectionaltracking. Specifically, the tracker is able to forward localize a target objectin successive frames and backtrace to its initial position in the first frame.Based on such a motivation, in the training process, we measure the consistencybetween forward and backward trajectories to learn a robust tracker fromscratch merely using unlabeled videos. We build our framework on a Siamesecorrelation filter network, and propose a multi-frame validation scheme and acost-sensitive loss to facilitate unsupervised learning. Without bells andwhistles, the proposed unsupervised tracker achieves the baseline accuracy asclassic fully supervised trackers while achieving a real-time speed.Furthermore, our unsupervised framework exhibits a potential in leveraging moreunlabeled or weakly labeled data to further improve the tracking accuracy.",視覚追跡の進歩は、ディープラーニングモデルによって継続的にもたらされています。通常、教師あり学習は、これらのモデルを高価なラベル付きデータでトレーニングするために使用されます。手動注釈の作業負荷を軽減し、任意のオブジェクトを追跡することを学ぶために、視覚追跡のための教師なし学習方法を提案します。私たちの教師なし学習の動機は、ロバストトラッカーが双方向追跡に効果的であるということです。具体的には、トラッカーは連続するフレームでターゲットオブジェクトを前方にローカライズし、最初のフレームの最初の位置にバックトレースできます。そのような動機に基づいて、トレーニングプロセスでは、順方向と逆方向の軌道間の一貫性を測定して、堅牢なトラッカーを最初から学習します単にラベルのない動画を使用する。 Siamesecorrelationフィルターネットワーク上にフレームワークを構築し、教師なし学習を容易にするために、マルチフレーム検証スキームとコストに敏感な損失を提案します。提案された監視なしトラッカーは、標準的な完全監視付きトラッカーとしてベースラインの精度を実現しながら、リアルタイムの速度を実現します。さらに、私たちの監視なしフレームワークは、より正確なラベル付けされていないデータまたは弱いラベル付けされたデータを活用して、追跡精度をさらに向上させる可能性を示します。
http://arxiv.org/abs/2007.11256v1,"Improving Monocular Depth Estimation by Leveraging Structural Awareness
  and Complementary Datasets",http://arxiv.org/pdf/2007.11256v1,"Monocular depth estimation plays a crucial role in 3D recognition andunderstanding. One key limitation of existing approaches lies in their lack ofstructural information exploitation, which leads to inaccurate spatial layout,discontinuous surface, and ambiguous boundaries. In this paper, we tackle thisproblem in three aspects. First, to exploit the spatial relationship of visualfeatures, we propose a structure-aware neural network with spatial attentionblocks. These blocks guide the network attention to global structures or localdetails across different feature layers. Second, we introduce a global focalrelative loss for uniform point pairs to enhance spatial constraint in theprediction, and explicitly increase the penalty on errors in depth-wisediscontinuous regions, which helps preserve the sharpness of estimationresults. Finally, based on analysis of failure cases for prior methods, wecollect a new Hard Case (HC) Depth dataset of challenging scenes, such asspecial lighting conditions, dynamic objects, and tilted camera angles. The newdataset is leveraged by an informed learning curriculum that mixes trainingexamples incrementally to handle diverse data distributions. Experimentalresults show that our method outperforms state-of-the-art approaches by a largemargin in terms of both prediction accuracy on NYUDv2 dataset andgeneralization performance on unseen datasets.",単眼深度推定は、3Dの認識と理解に重要な役割を果たします。既存のアプローチの主な制限の1つは、構造情報を利用できないことです。これにより、空間レイアウトが不正確になり、表面が不連続になり、境界が曖昧になります。このペーパーでは、この問題に3つの側面で取り組みます。まず、視覚的特徴の空間的関係を活用するために、空間的注意ブロックを備えた構造認識ニューラルネットワークを提案します。これらのブロックは、さまざまなフィーチャレイヤー全体のグローバル構造またはローカル詳細へのネットワークの注意を導きます。第2に、予測の空間制約を強化するために均一なポイントペアのグローバルな焦点相対損失を導入し、深度方向の不連続領域のエラーに対するペナルティを明示的に増加させます。これにより、推定結果の鮮明さを維持できます。最後に、以前の方法の失敗事例の分析に基づいて、特別な照明条件、動的オブジェクト、傾斜したカメラの角度など、困難なシーンの新しいハードケース（HC）深度データセットを収集します。新しいデータセットは、さまざまなデータの分散を処理するためにトレーニング例を段階的に混合する、情報に基づく学習カリキュラムによって活用されます。実験結果は、NYUDv2データセットの予測精度と目に見えないデータセットの一般化パフォーマンスの両方の点で、この手法が最先端のアプローチよりも大きなマージンで優れていることを示しています。
http://arxiv.org/abs/2007.11255v1,"DeepCLR: Correspondence-Less Architecture for Deep End-to-End Point
  Cloud Registration",http://arxiv.org/pdf/2007.11255v1,"This work addresses the problem of point cloud registration using deep neuralnetworks. We propose an approach to predict the alignment between two pointclouds with overlapping data content, but displaced origins. Such point cloudsoriginate, for example, from consecutive measurements of a LiDAR mounted on amoving platform. The main difficulty in deep registration of raw point cloudsis the fusion of template and source point cloud. Our proposed architectureapplies flow embedding to tackle this problem, which generates features thatdescribe the motion of each template point. These features are then used topredict the alignment in an end-to-end fashion without extracting explicitpoint correspondences between both input clouds. We rely on the KITTI odometryand ModelNet40 datasets for evaluating our method on various pointdistributions. Our approach achieves state-of-the-art accuracy and the lowestrun-time of the compared methods.",この作業は、ディープニューラルネットワークを使用した点群登録の問題に対処します。データコンテンツが重複しているが原点がずれている2つの点群間の位置合わせを予測する方法を提案します。このような点群は、たとえば、移動プラットフォームに取り付けられたLiDARの連続測定から発生します。生の点群の深い登録における主な困難は、テンプレートとソースの点群の融合です。私たちの提案するアーキテクチャは、この問題に取り組むためにフローの埋め込みを適用します。これにより、各テンプレートポイントの動きを説明する機能が生成されます。次に、これらの機能を使用して、両方の入力クラウド間の明示的な点の対応を抽出することなく、エンドツーエンドの方法でアライメントを予測します。さまざまなポイント分布でのメソッドの評価は、KITTIオドメトリとModelNet40データセットに依存しています。私たちのアプローチは、最先端の精度と比較されたメソッドの最短実行時間を実現します。
http://arxiv.org/abs/2007.11246v1,"Fragments-Expert: A Graphical User Interface MATLAB Toolbox for
  Classification of File Fragments",http://arxiv.org/pdf/2007.11246v1,"The classification of file fragments of various file formats is an essentialtask in various applications such as firewalls, intrusion detection systems,anti-viruses, web content filtering, and digital forensics. However, thecommunity lacks a suitable software tool that can integrate major methods forfeature extraction from file fragments and classification among various fileformats. In this paper, we present Fragments-Expert that is a graphical userinterface MATLAB toolbox for the classification of file fragments. It providesusers with 22 categories of features extracted from file fragments. Thesefeatures can be employed by 7 categories of machine learning algorithms for thetask of classification among various file formats.",さまざまなファイル形式のファイルフラグメントの分類は、ファイアウォール、侵入検知システム、アンチウイルス、Webコンテンツフィルタリング、デジタルフォレンジックなどのさまざまなアプリケーションで不可欠なタスクです。ただし、コミュニティには、ファイルフラグメントからの特徴抽出やさまざまなファイル形式間の分類のための主要な方法を統合できる適切なソフトウェアツールがありません。このペーパーでは、ファイルフラグメントを分類するためのグラフィカルユーザーインターフェイスMATLABツールボックスであるFragments-Expertを紹介します。ファイルフラグメントから抽出された22のカテゴリの機能をユーザーに提供します。これらの機能は、7つのカテゴリの機械学習アルゴリズムで、さまざまなファイル形式間の分類タスクに使用できます。
http://arxiv.org/abs/2007.11245v1,Learnable Descent Algorithm for Nonsmooth Nonconvex Image Reconstruction,http://arxiv.org/pdf/2007.11245v1,"We propose a general learning based framework for solving nonsmooth andnonconvex image reconstruction problems. We model the regularization functionas the composition of the $l_{2,1}$ norm and a smooth but nonconvex featuremapping parametrized as a deep convolutional neural network. We develop aprovably convergent descent-type algorithm to solve the nonsmooth nonconvexminimization problem by leveraging the Nesterov's smoothing technique and theidea of residual learning, and learn the network parameters such that theoutputs of the algorithm match the references in training data. Our method isversatile as one can employ various modern network structures into theregularization, and the resulting network inherits the guaranteed convergenceof the algorithm. We also show that the proposed network is parameter-efficientand its performance compares favorably to the state-of-the-art methods in avariety of image reconstruction problems in practice.","非平滑および非凸画像再構成問題を解決するための一般的な学習ベースのフレームワークを提案します。正規化関数は、$ l_ {2,1} $ノルムの構成と、深い畳み込みニューラルネットワークとしてパラメーター化された滑らかだが非凸の機能マッピングとしてモデル化します。 Nesterovの平滑化手法と残差学習のアイデアを活用して非平滑非凸最小化問題を解決するために、適度に収束する降下型アルゴリズムを開発し、アルゴリズムの出力がトレーニングデータの参照と一致するようにネットワークパラメーターを学習します。私たちの方法は、さまざまな最新のネットワーク構造を正規化に使用できるため、汎用性が高く、結果として得られるネットワークは、アルゴリズムの保証された収束を継承します。また、提案されたネットワークはパラメーター効率がよく、そのパフォーマンスは、実際のさまざまな画像再構成問題において、最先端の方法に匹敵することも示しています。"
http://arxiv.org/abs/2007.11240v1,Edge-aware Graph Representation Learning and Reasoning for Face Parsing,http://arxiv.org/pdf/2007.11240v1,"Face parsing infers a pixel-wise label to each facial component, which hasdrawn much attention recently. Previous methods have shown their efficiency inface parsing, which however overlook the correlation among different faceregions. The correlation is a critical clue about the facial appearance, pose,expression etc., and should be taken into account for face parsing. To thisend, we propose to model and reason the region-wise relations by learning graphrepresentations, and leverage the edge information between regions foroptimized abstraction. Specifically, we encode a facial image onto a globalgraph representation where a collection of pixels (""regions"") with similarfeatures are projected to each vertex. Our model learns and reasons overrelations between the regions by propagating information across vertices on thegraph. Furthermore, we incorporate the edge information to aggregate thepixel-wise features onto vertices, which emphasizes on the features aroundedges for fine segmentation along edges. The finally learned graphrepresentation is projected back to pixel grids for parsing. Experimentsdemonstrate that our model outperforms state-of-the-art methods on the widelyused Helen dataset, and also exhibits the superior performance on thelarge-scale CelebAMask-HQ and LaPa dataset. The code is available athttps://github.com/tegusi/EAGRNet.",顔の解析は、最近多くの注目を集めている各顔のコンポーネントにピクセル単位のラベルを推測します。以前の方法は、面解析での効率を示していましたが、異なる顔領域間の相関関係を見落としていました。相関関係は、顔の外観、ポーズ、表情などに関する重要な手がかりであり、顔の解析では考慮に入れる必要があります。この目的のために、グラフ表現を学習することによって領域ごとの関係をモデル化し、推論し、最適化された抽象化のために領域間のエッジ情報を活用することを提案します。具体的には、似たような特徴を持つピクセルのコレクション（「領域」）が各頂点に投影されるグローバルグラフ表現に顔の画像をエンコードします。私たちのモデルは、グラフ上の頂点間で情報を伝播することにより、領域間の相互関係を学習し、理由を示します。さらに、エッジに関する情報を組み込んで、ピクセル単位の特徴を頂点に集約します。これにより、エッジに沿った細かいセグメンテーションのためにエッジ周辺の特徴が強調されます。最終的に学習されたグラフ表現は、解析のためにピクセルグリッドに投影されます。実験は、私たちのモデルが広く使用されているヘレンデータセットで最先端の方法よりも優れていること、および大規模なCelebAMask-HQおよびLaPaデータセットで優れたパフォーマンスを示していることを示しています。コードはhttps://github.com/tegusi/EAGRNetで入手できます。
http://arxiv.org/abs/2007.11222v1,"Greenhouse Segmentation on High-Resolution Optical Satellite Imagery
  using Deep Learning Techniques",http://arxiv.org/pdf/2007.11222v1,"Greenhouse segmentation has pivotal importance for climate-smart agriculturalland-use planning. Deep learning-based approaches provide state-of-the-artperformance in natural image segmentation. However, semantic segmentation onhigh-resolution optical satellite imagery is a challenging task because of thecomplex environment. In this paper, a sound methodology is proposed forpixel-wise classification on images acquired by the Azersky (SPOT-7) opticalsatellite. In particular, customized variations of U-Net-like architectures areemployed to identify greenhouses. Two models are proposed which uniquelyincorporate dilated convolutions and skip connections, and the results arecompared to that of the baseline U-Net model. The dataset used consists ofpan-sharpened orthorectified Azersky images (red, green, blue,and near infraredchannels) with 1.5-meter resolution and annotation masks, collected from 15regions in Azerbaijan where the greenhouses are densely congested. The imagescover the cumulative area of 1008 $km^2$ and annotation masks contain 47559polygons in total. The $F_1, Kappa, AUC$, and $IOU$ scores are used forperformance evaluation. It is observed that the use of the deconvolutionallayers alone throughout the expansive path does not yield satisfactory results;therefore, they are either replaced or coupled with bilinear interpolation. Allmodels benefit from the hard example mining (HEM) strategy. It is also reportedthat the best accuracy of $93.29\%$ ($F_1\,score$) is recorded when theweighted binary cross-entropy loss is coupled with the dice loss. Experimentalresults showed that both of the proposed models outperformed the baseline U-Netarchitecture such that the best model proposed scored $4.48\%$ higher incomparison to the baseline architecture.",温室のセグメンテーションは、気候に配慮した農地利用計画にとって極めて重要です。ディープラーニングベースのアプローチは、自然な画像セグメンテーションにおいて最先端のパフォーマンスを提供します。ただし、高解像度の光学衛星画像でのセマンティックセグメンテーションは、複雑な環境のため、やりがいのある作業です。この論文では、Azersky（SPOT-7）光学衛星によって取得された画像のピクセルごとの分類のための適切な方法が提案されています。特に、温室を特定するために、U-Netのようなアーキテクチャのカスタマイズされたバリエーションが採用されています。拡張された畳み込みを一意に組み込んで接続をスキップする2つのモデルが提案され、結果はベースラインU-Netモデルの結果と比較されます。使用されるデータセットは、温室が密集しているアゼルバイジャンの15の地域から収集された、1.5メートルの解像度と注釈マスクを備えた、パンシャープンのオルソ補正されたアゼルスキー画像（赤、緑、青、近赤外線チャネル）で構成されています。画像は1008 $ km ^ 2 $の累積領域をカバーし、注釈マスクには合計47559のポリゴンが含まれます。 $ F_1、Kappa、AUC $、および$ IOU $スコアは、パフォーマンス評価に使用されます。展開パス全体でデコンボリューショナルレイヤーを単独で使用すると満足な結果が得られないことが観察されています。そのため、これらは置き換えられるか、バイリニア補間と組み合わせられます。すべてのモデルは、ハードサンプルマイニング（HEM）戦略の恩恵を受けます。また、加重バイナリクロスエントロピー損失がダイス損失と組み合わされたときに、$ 93.29 \％$（$ F_1 \、score $）の最高精度が記録されることも報告されています。実験結果は、提案されたモデルの両方がベースラインU-Netarchitectureよりも優れていることを示しており、提案された最良のモデルはベースラインアーキテクチャと比較して$ 4.48 \％$高く評価されました。
http://arxiv.org/abs/2007.11186v1,Instance-aware Self-supervised Learning for Nuclei Segmentation,http://arxiv.org/pdf/2007.11186v1,"Due to the wide existence and large morphological variances of nuclei,accurate nuclei instance segmentation is still one of the most challengingtasks in computational pathology. The annotating of nuclei instances, requiringexperienced pathologists to manually draw the contours, is extremely laboriousand expensive, which often results in the deficiency of annotated data. Thedeep learning based segmentation approaches, which highly rely on the quantityof training data, are difficult to fully demonstrate their capacity in thisarea. In this paper, we propose a novel self-supervised learning framework todeeply exploit the capacity of widely-used convolutional neural networks (CNNs)on the nuclei instance segmentation task. The proposed approach involves twosub-tasks (i.e., scale-wise triplet learning and count ranking), which enableneural networks to implicitly leverage the prior-knowledge of nuclei size andquantity, and accordingly mine the instance-aware feature representations fromthe raw data. Experimental results on the publicly available MoNuSeg datasetshow that the proposed self-supervised learning approach can remarkably boostthe segmentation accuracy of nuclei instance---a new state-of-the-art averageAggregated Jaccard Index (AJI) of 70.63%, is achieved by our self-supervisedResUNet-101. To our best knowledge, this is the first work focusing on theself-supervised learning for instance segmentation.",核が広く存在し、形態学的分散が大きいため、正確な核インスタンスのセグメンテーションは、依然として計算病理学において最も困難なタスクの1つです。核のインスタンスの注釈付けは、経験豊富な病理学者が手動で輪郭を描く必要があり、非常に面倒で費用がかかるため、注釈付きデータが不足することがよくあります。トレーニングデータの量に大きく依存するディープラーニングベースのセグメンテーションアプローチは、この領域でその能力を完全に実証することは困難です。この論文では、核インスタンスセグメンテーションタスクで広く使用されている畳み込みニューラルネットワーク（CNN）の能力を深く活用するための、新しい自己監視学習フレームワークを提案します。提案されたアプローチには2つのサブタスク（つまり、スケールワイズトリプレット学習とカウントランキング）が含まれます。これにより、ニューラルネットワークは、核のサイズと量の事前知識を暗黙的に活用し、生データからインスタンスを認識する機能表現をマイニングできます。公開されているMoNuSegデータセットの実験結果は、提案された自己監視学習アプローチが核インスタンスのセグメンテーション精度を著しく向上させることができることを示しています。 self-supervisedResUNet-101。私たちの知る限りでは、これはインスタンスのセグメンテーションのための自己管理学習に焦点を当てた最初の作品です。
http://arxiv.org/abs/2007.11180v1,"MI^2GAN: Generative Adversarial Network for Medical Image Domain
  Adaptation using Mutual Information Constraint",http://arxiv.org/pdf/2007.11180v1,"Domain shift between medical images from multicentres is still an openquestion for the community, which degrades the generalization performance ofdeep learning models. Generative adversarial network (GAN), which synthesizeplausible images, is one of the potential solutions to address the problem.However, the existing GAN-based approaches are prone to fail at preservingimage-objects in image-to-image (I2I) translation, which reduces theirpracticality on domain adaptation tasks. In this paper, we propose a novel GAN(namely MI$^2$GAN) to maintain image-contents during cross-domain I2Itranslation. Particularly, we disentangle the content features from domaininformation for both the source and translated images, and then maximize themutual information between the disentangled content features to preserve theimage-objects. The proposed MI$^2$GAN is evaluated on two tasks---polypsegmentation using colonoscopic images and the segmentation of optic disc andcup in fundus images. The experimental results demonstrate that the proposedMI$^2$GAN can not only generate elegant translated images, but alsosignificantly improve the generalization performance of widely used deeplearning networks (e.g., U-Net).",マルチセンターからの医用画像間のドメインシフトは、コミュニティにとって依然として未解決の問題であり、ディープラーニングモデルの一般化パフォーマンスを低下させます。もっともらしい画像を合成する生成的敵対的ネットワーク（GAN）は、問題に対処するための潜在的な解決策の1つですが、既存のGANベースのアプローチは、画像から画像（I2I）への変換で画像オブジェクトを保存することに失敗する傾向があります。ドメイン適応タスクの実用性を低下させます。この論文では、クロスドメインI2I変換中に画像コンテンツを維持するための新しいGAN（つまりMI $ ^ 2 $ GAN）を提案します。特に、ソース画像と翻訳された画像の両方のドメイン情報からコンテンツの特徴のもつれを解き、もつれたもつれたコンテンツの特徴間の相互情報を最大化して、画像オブジェクトを保存します。提案されたMI $ ^ 2 $ GANは、大腸内視鏡画像を使用したポリセグメント化と、眼底画像の視神経乳頭とカップのセグメンテーションの2つのタスクで評価されます。実験結果は、提案されたMI $ ^ 2 $ GANがエレガントな翻訳画像を生成できるだけでなく、広く使用されているディープラーニングネットワーク（U-Netなど）の一般化パフォーマンスを大幅に改善できることを示しています。
http://arxiv.org/abs/2007.12066v1,"A Computation-Efficient CNN System for High-Quality Brain Tumor
  Segmentation",http://arxiv.org/pdf/2007.12066v1,"In this paper, a Convolutional Neural Network (CNN) system is proposed forbrain tumor segmentation. The system consists of three parts, a pre-processingblock to reduce the data volume, an application-specific CNN(ASCNN) to segmenttumor areas precisely, and a refinement block to detect/remove false positivepixels. The CNN, designed specifically for the task, has 7 convolution layers,16 channels per layer, requiring only 11716 parameters. The convolutionscombined with max-pooling in the first half of the CNN are performed tolocalize tumor areas. Two convolution modes, namely depthwise convolution andstandard convolution, are performed in parallel in the first 2 layers toextract elementary features efficiently. For a fine classification ofpixel-wise precision in the second half of the CNN, the feature maps aremodulated by adding the weighted local features generated in the first half ofthe CNN. The performance of the proposed system has been evaluated by an onlineplatform with dataset BRATS2018. Requiring a very low computation volume, theproposed system delivers a high segmentation quality indicated by its averageDice scores of 0.75, 0.88 and 0.76 for enhancing tumor, whole tumor and tumorcore, respectively, and the median Dice scores of 0.85, 0.92, and 0.86. Theconsistency in system performance has also been measured, demonstrating thatthe system is able to reproduce almost the same output to the same input afterretraining. The simple structure of the proposed system facilitates itsimplementation in computation restricted environment, and a wide range ofapplications can thus be expected.",この論文では、畳み込みニューラルネットワーク（CNN）システムが脳腫瘍セグメンテーションを提案します。このシステムは、データ量を減らすための前処理ブロック、腫瘍領域を正確にセグメント化するためのアプリケーション固有のCNN（ASCNN）、および偽陽性ピクセルを検出/削除するためのリファインメントブロックの3つの部分で構成されています。タスク用に特別に設計されたCNNには、7つの畳み込み層があり、層あたり16チャネルで、11716パラメーターのみが必要です。 CNNの前半で最大プーリングと組み合わせた畳み込みを実行して、腫瘍領域を特定します。 2つの畳み込みモード、つまり深さ方向の畳み込みと標準の畳み込みが最初の2つのレイヤーで並列に実行され、基本的な特徴を効率的に抽出します。 CNNの後半でピクセル単位の精度を細かく分類するには、CNNの前半で生成された重み付けされたローカルフィーチャを追加することにより、フィーチャマップを変調します。提案されたシステムのパフォーマンスは、データセットBRATS2018を備えたオンラインプラットフォームによって評価されています。非常に低い計算量を必要とする提案されたシステムは、腫瘍、腫瘍全体、腫瘍コアをそれぞれ強化するためのaverageDiceスコア0.75、0.88、0.76、および中央ダイススコア0.85、0.92、0.86によって示される高いセグメンテーション品質を提供します。システムパフォーマンスの一貫性も測定されており、再トレーニング後にシステムがほぼ同じ出力を同じ入力に再現できることを示しています。提案されたシステムの単純な構造は、計算が制限された環境でのその実装を容易にし、したがって、広範なアプリケーションが期待され得る。
